website,codecontent,api
"https://github.com/saltstack/salt/blob/d7338a0e795d3c9cb65dfa211f7dbec740dbb543/salt/modules/mod_random.py
","
import base64
import random

import salt.utils.data
import salt.utils.pycrypto
from salt.exceptions import SaltInvocationError

__virtualname__ = \""random\""


def __virtual__():
    return __virtualname__


def hash(value, algorithm=\""sha512\""):
    return salt.utils.data.hash(value, algorithm=algorithm)


def str_encode(value, encoder=\""base64\""):
    if isinstance(value, str):
        value = value.encode(__salt_system_encoding__)
    if encoder == \""base64\"":
        try:
            out = base64.b64encode(value)
            out = out.decode(__salt_system_encoding__)
        except TypeError:
            raise SaltInvocationError(\""Value must be an encode-able string\"")
    else:
        try:
            out = value.encode(encoder)
        except LookupError:
            raise SaltInvocationError(\""You must specify a valid encoder\"")
        except AttributeError:
            raise SaltInvocationError(\""Value must be an encode-able string\"")
    return out


def get_str(
    length=20,
    chars=None,
    lowercase=True,
    uppercase=True,
    digits=True,
    punctuation=True,
    whitespace=False,
    printable=False,
):
    return salt.utils.pycrypto.secure_password(
        length=length,
        chars=chars,
        lowercase=lowercase,
        uppercase=uppercase,
        digits=digits,
        punctuation=punctuation,
        whitespace=whitespace,
        printable=printable,
    )


def shadow_hash(crypt_salt=None, password=None, algorithm=\""sha512\""):
    return salt.utils.pycrypto.gen_hash(crypt_salt, password, algorithm)


def rand_int(start=1, end=10, seed=None):
    if seed is not None:
        random.seed(seed)
    return random.randint(start, end)


def seed(range=10, hash=None):
    if hash is None:
        hash = __grains__[\""id\""]

    random.seed(hash)
    return random.randrange(range)


def sample(value, size, seed=None):
    return salt.utils.data.sample(value, size, seed=seed)


def shuffle(value, seed=None):
    return salt.utils.data.shuffle(value, seed=seed)
","['__virtual__', 'hash', 'str_encode', 'isinstance', 'encode', 'b64encode', 'decode', 'SaltInvocationError', 'get_str', 'secure_password', 'shadow_hash', 'gen_hash', 'rand_int', 'seed', 'randint', 'randrange', 'sample', 'shuffle']"
"https://github.com/django/django/blob/594873befbbec13a2d9a048a361757dd3cf178da/django/contrib/auth/hashers.py
","import base64
import binascii
import functools
import hashlib
import importlib
import math
import warnings

from django.conf import settings
from django.core.exceptions import ImproperlyConfigured
from django.core.signals import setting_changed
from django.dispatch import receiver
from django.utils.crypto import (
    RANDOM_STRING_CHARS,
    constant_time_compare,
    get_random_string,
    pbkdf2,
)
from django.utils.module_loading import import_string
from django.utils.translation import gettext_noop as _

UNUSABLE_PASSWORD_SUFFIX_LENGTH = (
)


def is_password_usable(encoded):
    return encoded is None or not encoded.startswith(UNUSABLE_PASSWORD_PREFIX)


def verify_password(password, encoded, preferred=\""default\""):
    if password is None or not is_password_usable(encoded):
        return False, False

    preferred = get_hasher(preferred)
    try:
        hasher = identify_hasher(encoded)
    except ValueError:
        return False, False

    hasher_changed = hasher.algorithm != preferred.algorithm
    must_update = hasher_changed or preferred.must_update(encoded)
    is_correct = hasher.verify(password, encoded)

    if not is_correct and not hasher_changed and must_update:
        hasher.harden_runtime(password, encoded)

    return is_correct, must_update


def check_password(password, encoded, setter=None, preferred=\""default\""):
    is_correct, must_update = verify_password(password, encoded, preferred=preferred)
    if setter and is_correct and must_update:
        setter(password)
    return is_correct


async def acheck_password(password, encoded, setter=None, preferred=\""default\""):
    is_correct, must_update = verify_password(password, encoded, preferred=preferred)
    if setter and is_correct and must_update:
        await setter(password)
    return is_correct


def make_password(password, salt=None, hasher=\""default\""):
    if password is None:
        return UNUSABLE_PASSWORD_PREFIX + get_random_string(
            UNUSABLE_PASSWORD_SUFFIX_LENGTH
        )
    if not isinstance(password, (bytes, str)):
        raise TypeError(
            \""Password must be a string or bytes, got %s.\"" % type(password).__qualname__
        )
    hasher = get_hasher(hasher)
    salt = salt or hasher.salt()
    return hasher.encode(password, salt)


@functools.lru_cache
def get_hashers():
    hashers = []
    for hasher_path in settings.PASSWORD_HASHERS:
        hasher_cls = import_string(hasher_path)
        hasher = hasher_cls()
        if not getattr(hasher, \""algorithm\""):
            raise ImproperlyConfigured(
                \""hasher doesn't specify an algorithm name: %s\"" % hasher_path
            )
        hashers.append(hasher)
    return hashers


@functools.lru_cache
def get_hashers_by_algorithm():
    return {hasher.algorithm: hasher for hasher in get_hashers()}


@receiver(setting_changed)
def reset_hashers(*, setting, **kwargs):
    if setting == \""PASSWORD_HASHERS\"":
        get_hashers.cache_clear()
        get_hashers_by_algorithm.cache_clear()


def get_hasher(algorithm=\""default\""):
    if hasattr(algorithm, \""algorithm\""):
        return algorithm

    elif algorithm == \""default\"":
        return get_hashers()[0]

    else:
        hashers = get_hashers_by_algorithm()
        try:
            return hashers[algorithm]
        except KeyError:
            raise ValueError(
                \""Unknown password hashing algorithm '%s'. \""
                \""Did you specify it in the PASSWORD_HASHERS \""
                \""setting?\"" % algorithm
            )


def identify_hasher(encoded):
    if (len(encoded) == 32 and \""$\"" not in encoded) or (
        len(encoded) == 37 and encoded.startswith(\""md5$$\"")
    ):
        algorithm = \""unsalted_md5\""
    elif len(encoded) == 46 and encoded.startswith(\""sha1$$\""):
        algorithm = \""unsalted_sha1\""
    else:
        algorithm = encoded.split(\""$\"", 1)[0]
    return get_hasher(algorithm)


def mask_hash(hash, show=6, char=\""*\""):
    masked = hash[:show]
    masked += char * len(hash[show:])
    return masked


def must_update_salt(salt, expected_entropy):
    return len(salt) * math.log2(len(RANDOM_STRING_CHARS)) < expected_entropy


class BasePasswordHasher:

    algorithm = None
    library = None
    salt_entropy = 128

    def _load_library(self):
        if self.library is not None:
            if isinstance(self.library, (tuple, list)):
                name, mod_path = self.library
            else:
                mod_path = self.library
            try:
                module = importlib.import_module(mod_path)
            except ImportError as e:
                raise ValueError(
                    \""Couldn't load %r algorithm library: %s\""
                    % (self.__class__.__name__, e)
                )
            return module
        raise ValueError(
            \""Hasher %r doesn't specify a library attribute\"" % self.__class__.__name__
        )

    def salt(self):
        char_count = math.ceil(self.salt_entropy / math.log2(len(RANDOM_STRING_CHARS)))
        return get_random_string(char_count, allowed_chars=RANDOM_STRING_CHARS)

    def verify(self, password, encoded):
        raise NotImplementedError(
            \""subclasses of BasePasswordHasher must provide a verify() method\""
        )

    def _check_encode_args(self, password, salt):
        if password is None:
            raise TypeError(\""password must be provided.\"")
        if not salt or \""$\"" in salt:
            raise ValueError(\""salt must be provided and cannot contain $.\"")

    def encode(self, password, salt):
        raise NotImplementedError(
            \""subclasses of BasePasswordHasher must provide an encode() method\""
        )

    def decode(self, encoded):
        raise NotImplementedError(
            \""subclasses of BasePasswordHasher must provide a decode() method.\""
        )

    def safe_summary(self, encoded):
        raise NotImplementedError(
            \""subclasses of BasePasswordHasher must provide a safe_summary() method\""
        )

    def must_update(self, encoded):
        return False

    def harden_runtime(self, password, encoded):
        warnings.warn(
            \""subclasses of BasePasswordHasher should provide a harden_runtime() method\""
        )


class PBKDF2PasswordHasher(BasePasswordHasher):

    algorithm = \""pbkdf2_sha256\""
    iterations = 870000
    digest = hashlib.sha256

    def encode(self, password, salt, iterations=None):
        self._check_encode_args(password, salt)
        iterations = iterations or self.iterations
        hash = pbkdf2(password, salt, iterations, digest=self.digest)
        hash = base64.b64encode(hash).decode(\""ascii\"").strip()
        return \""%s$%d$%s$%s\"" % (self.algorithm, iterations, salt, hash)

    def decode(self, encoded):
        algorithm, iterations, salt, hash = encoded.split(\""$\"", 3)
        assert algorithm == self.algorithm
        return {
            \""algorithm\"": algorithm,
            \""hash\"": hash,
            \""iterations\"": int(iterations),
            \""salt\"": salt,
        }

    def verify(self, password, encoded):
        decoded = self.decode(encoded)
        encoded_2 = self.encode(password, decoded[\""salt\""], decoded[\""iterations\""])
        return constant_time_compare(encoded, encoded_2)

    def safe_summary(self, encoded):
        decoded = self.decode(encoded)
        return {
            _(\""algorithm\""): decoded[\""algorithm\""],
            _(\""iterations\""): decoded[\""iterations\""],
            _(\""salt\""): mask_hash(decoded[\""salt\""]),
            _(\""hash\""): mask_hash(decoded[\""hash\""]),
        }

    def must_update(self, encoded):
        decoded = self.decode(encoded)
        update_salt = must_update_salt(decoded[\""salt\""], self.salt_entropy)
        return (decoded[\""iterations\""] != self.iterations) or update_salt

    def harden_runtime(self, password, encoded):
        decoded = self.decode(encoded)
        extra_iterations = self.iterations - decoded[\""iterations\""]
        if extra_iterations > 0:
            self.encode(password, decoded[\""salt\""], extra_iterations)


class PBKDF2SHA1PasswordHasher(PBKDF2PasswordHasher):

    algorithm = \""pbkdf2_sha1\""
    digest = hashlib.sha1


class Argon2PasswordHasher(BasePasswordHasher):

    algorithm = \""argon2\""
    library = \""argon2\""

    time_cost = 2
    memory_cost = 102400
    parallelism = 8

    def encode(self, password, salt):
        argon2 = self._load_library()
        params = self.params()
        data = argon2.low_level.hash_secret(
            password.encode(),
            salt.encode(),
            time_cost=params.time_cost,
            memory_cost=params.memory_cost,
            parallelism=params.parallelism,
            hash_len=params.hash_len,
            type=params.type,
        )
        return self.algorithm + data.decode(\""ascii\"")

    def decode(self, encoded):
        argon2 = self._load_library()
        algorithm, rest = encoded.split(\""$\"", 1)
        assert algorithm == self.algorithm
        params = argon2.extract_parameters(\""$\"" + rest)
        variety, *_, b64salt, hash = rest.split(\""$\"")
        b64salt += \""=\"" * (-len(b64salt) % 4)
        salt = base64.b64decode(b64salt).decode(\""latin1\"")
        return {
            \""algorithm\"": algorithm,
            \""hash\"": hash,
            \""memory_cost\"": params.memory_cost,
            \""parallelism\"": params.parallelism,
            \""salt\"": salt,
            \""time_cost\"": params.time_cost,
            \""variety\"": variety,
            \""version\"": params.version,
            \""params\"": params,
        }

    def verify(self, password, encoded):
        argon2 = self._load_library()
        algorithm, rest = encoded.split(\""$\"", 1)
        assert algorithm == self.algorithm
        try:
            return argon2.PasswordHasher().verify(\""$\"" + rest, password)
        except argon2.exceptions.VerificationError:
            return False

    def safe_summary(self, encoded):
        decoded = self.decode(encoded)
        return {
            _(\""algorithm\""): decoded[\""algorithm\""],
            _(\""variety\""): decoded[\""variety\""],
            _(\""version\""): decoded[\""version\""],
            _(\""memory cost\""): decoded[\""memory_cost\""],
            _(\""time cost\""): decoded[\""time_cost\""],
            _(\""parallelism\""): decoded[\""parallelism\""],
            _(\""salt\""): mask_hash(decoded[\""salt\""]),
            _(\""hash\""): mask_hash(decoded[\""hash\""]),
        }

    def must_update(self, encoded):
        decoded = self.decode(encoded)
        current_params = decoded[\""params\""]
        new_params = self.params()
        new_params.salt_len = current_params.salt_len
        update_salt = must_update_salt(decoded[\""salt\""], self.salt_entropy)
        return (current_params != new_params) or update_salt

    def harden_runtime(self, password, encoded):
        pass

    def params(self):
        argon2 = self._load_library()
        return argon2.Parameters(
            type=argon2.low_level.Type.ID,
            version=argon2.low_level.ARGON2_VERSION,
            salt_len=argon2.DEFAULT_RANDOM_SALT_LENGTH,
            hash_len=argon2.DEFAULT_HASH_LENGTH,
            time_cost=self.time_cost,
            memory_cost=self.memory_cost,
            parallelism=self.parallelism,
        )


class BCryptSHA256PasswordHasher(BasePasswordHasher):

    algorithm = \""bcrypt_sha256\""
    digest = hashlib.sha256
    library = (\""bcrypt\"", \""bcrypt\"")
    rounds = 12

    def salt(self):
        bcrypt = self._load_library()
        return bcrypt.gensalt(self.rounds)

    def encode(self, password, salt):
        bcrypt = self._load_library()
        password = password.encode()
        if self.digest is not None:
            password = binascii.hexlify(self.digest(password).digest())

        data = bcrypt.hashpw(password, salt)
        return \""%s$%s\"" % (self.algorithm, data.decode(\""ascii\""))

    def decode(self, encoded):
        algorithm, empty, algostr, work_factor, data = encoded.split(\""$\"", 4)
        assert algorithm == self.algorithm
        return {
            \""algorithm\"": algorithm,
            \""algostr\"": algostr,
            \""checksum\"": data[22:],
            \""salt\"": data[:22],
            \""work_factor\"": int(work_factor),
        }

    def verify(self, password, encoded):
        algorithm, data = encoded.split(\""$\"", 1)
        assert algorithm == self.algorithm
        encoded_2 = self.encode(password, data.encode(\""ascii\""))
        return constant_time_compare(encoded, encoded_2)

    def safe_summary(self, encoded):
        decoded = self.decode(encoded)
        return {
            _(\""algorithm\""): decoded[\""algorithm\""],
            _(\""work factor\""): decoded[\""work_factor\""],
            _(\""salt\""): mask_hash(decoded[\""salt\""]),
            _(\""checksum\""): mask_hash(decoded[\""checksum\""]),
        }

    def must_update(self, encoded):
        decoded = self.decode(encoded)
        return decoded[\""work_factor\""] != self.rounds

    def harden_runtime(self, password, encoded):
        _, data = encoded.split(\""$\"", 1)
        rounds = data.split(\""$\"")[2]
        diff = 2 ** (self.rounds - int(rounds)) - 1
        while diff > 0:
            self.encode(password, salt.encode(\""ascii\""))
            diff -= 1


class BCryptPasswordHasher(BCryptSHA256PasswordHasher):

    algorithm = \""bcrypt\""
    digest = None


class ScryptPasswordHasher(BasePasswordHasher):

    algorithm = \""scrypt\""
    block_size = 8
    maxmem = 0
    parallelism = 1
    work_factor = 2**14

    def encode(self, password, salt, n=None, r=None, p=None):
        self._check_encode_args(password, salt)
        n = n or self.work_factor
        r = r or self.block_size
        p = p or self.parallelism
        hash_ = hashlib.scrypt(
            password.encode(),
            salt=salt.encode(),
            n=n,
            r=r,
            p=p,
            maxmem=self.maxmem,
            dklen=64,
        )
        hash_ = base64.b64encode(hash_).decode(\""ascii\"").strip()
        return \""%s$%d$%s$%d$%d$%s\"" % (self.algorithm, n, salt, r, p, hash_)

    def decode(self, encoded):
        algorithm, work_factor, salt, block_size, parallelism, hash_ = encoded.split(
            \""$\"", 6
        )
        assert algorithm == self.algorithm
        return {
            \""algorithm\"": algorithm,
            \""work_factor\"": int(work_factor),
            \""salt\"": salt,
            \""block_size\"": int(block_size),
            \""parallelism\"": int(parallelism),
            \""hash\"": hash_,
        }

    def verify(self, password, encoded):
        decoded = self.decode(encoded)
        encoded_2 = self.encode(
            password,
            decoded[\""salt\""],
            decoded[\""work_factor\""],
            decoded[\""block_size\""],
            decoded[\""parallelism\""],
        )
        return constant_time_compare(encoded, encoded_2)

    def safe_summary(self, encoded):
        decoded = self.decode(encoded)
        return {
            _(\""algorithm\""): decoded[\""algorithm\""],
            _(\""work factor\""): decoded[\""work_factor\""],
            _(\""block size\""): decoded[\""block_size\""],
            _(\""parallelism\""): decoded[\""parallelism\""],
            _(\""salt\""): mask_hash(decoded[\""salt\""]),
            _(\""hash\""): mask_hash(decoded[\""hash\""]),
        }

    def must_update(self, encoded):
        decoded = self.decode(encoded)
        return (
            decoded[\""work_factor\""] != self.work_factor
            or decoded[\""block_size\""] != self.block_size
            or decoded[\""parallelism\""] != self.parallelism
        )

    def harden_runtime(self, password, encoded):
        pass


class MD5PasswordHasher(BasePasswordHasher):

    algorithm = \""md5\""

    def encode(self, password, salt):
        self._check_encode_args(password, salt)
        hash = hashlib.md5((salt + password).encode()).hexdigest()
        return \""%s$%s$%s\"" % (self.algorithm, salt, hash)

    def decode(self, encoded):
        algorithm, salt, hash = encoded.split(\""$\"", 2)
        assert algorithm == self.algorithm
        return {
            \""algorithm\"": algorithm,
            \""hash\"": hash,
            \""salt\"": salt,
        }

    def verify(self, password, encoded):
        decoded = self.decode(encoded)
        encoded_2 = self.encode(password, decoded[\""salt\""])
        return constant_time_compare(encoded, encoded_2)

    def safe_summary(self, encoded):
        decoded = self.decode(encoded)
        return {
            _(\""algorithm\""): decoded[\""algorithm\""],
            _(\""salt\""): mask_hash(decoded[\""salt\""], show=2),
            _(\""hash\""): mask_hash(decoded[\""hash\""]),
        }

    def must_update(self, encoded):
        decoded = self.decode(encoded)
        return must_update_salt(decoded[\""salt\""], self.salt_entropy)

    def harden_runtime(self, password, encoded):
        pass
","['is_password_usable', 'startswith', 'verify_password', 'get_hasher', 'identify_hasher', 'must_update', 'verify', 'harden_runtime', 'check_password', 'setter', 'acheck_password', 'make_password', 'get_random_string', 'isinstance', 'TypeError', 'type', 'salt', 'encode', 'get_hashers', 'import_string', 'hasher_cls', 'getattr', 'ImproperlyConfigured', 'append', 'get_hashers_by_algorithm', 'receiver', 'reset_hashers', 'cache_clear', 'hasattr', 'ValueError', 'len', 'split', 'mask_hash', 'must_update_salt', 'log2', '_load_library', 'import_module', 'ceil', 'NotImplementedError', '_check_encode_args', 'decode', 'safe_summary', 'warn', 'PBKDF2PasswordHasher', 'pbkdf2', 'b64encode', 'strip', 'int', 'constant_time_compare', '_', 'PBKDF2SHA1PasswordHasher', 'Argon2PasswordHasher', 'params', 'hash_secret', 'extract_parameters', 'b64decode', 'PasswordHasher', 'Parameters', 'BCryptSHA256PasswordHasher', 'gensalt', 'hexlify', 'digest', 'hashpw', 'BCryptPasswordHasher', 'ScryptPasswordHasher', 'scrypt', 'MD5PasswordHasher', 'md5', 'hexdigest']"
"https://github.com/nodejs/node/blob/e5189cbf7e40f1d099f7ac49a36509b9bd265e1e/tools/gyp/pylib/gyp/MSVSNew.py
","

import hashlib
import os
import random
from operator import attrgetter

import gyp.common


def cmp(x, y):
    return (x > y) - (x < y)


random.seed()

ENTRY_TYPE_GUIDS = {
    \""project\"": \""{8BC9CEB8-8B4A-11D0-8D11-00A0C91BC942}\
,""    \""folder\"": \""{2150E333-8FDC-42A3-9474-1A3956D46DE8}\
,""}



def MakeGuid(name, seed=\""msvs_new\""):
    d = hashlib.md5((str(seed) + str(name)).encode(\""utf-8\"")).hexdigest().upper()
    guid = (
        \""{\""
        + d[:8]
        + \""-\""
        + d[8:12]
        + \""-\""
        + d[12:16]
        + \""-\""
        + d[16:20]
        + \""-\""
        + d[20:32]
        + \""}\""
    )
    return guid




class MSVSSolutionEntry:
    def __cmp__(self, other):
        return cmp((self.name, self.get_guid()), (other.name, other.get_guid()))


class MSVSFolder(MSVSSolutionEntry):

    def __init__(self, path, name=None, entries=None, guid=None, items=None):
        if name:
            self.name = name
        else:
            self.name = os.path.basename(path)

        self.path = path
        self.guid = guid

        self.entries = sorted(entries or [], key=attrgetter(\""path\""))
        self.items = list(items or [])

        self.entry_type_guid = ENTRY_TYPE_GUIDS[\""folder\""]

    def get_guid(self):
        if self.guid is None:
            self.guid = MakeGuid(self.path, seed=\""msvs_folder\"")
        return self.guid




class MSVSProject(MSVSSolutionEntry):

    def __init__(
        self,
        path,
        name=None,
        dependencies=None,
        guid=None,
        spec=None,
        build_file=None,
        config_platform_overrides=None,
        fixpath_prefix=None,
    ):
        self.path = path
        self.guid = guid
        self.spec = spec
        self.build_file = build_file
        self.name = name or os.path.splitext(os.path.basename(path))[0]

        self.dependencies = list(dependencies or [])

        self.entry_type_guid = ENTRY_TYPE_GUIDS[\""project\""]

        if config_platform_overrides:
            self.config_platform_overrides = config_platform_overrides
        else:
            self.config_platform_overrides = {}
        self.fixpath_prefix = fixpath_prefix
        self.msbuild_toolset = None

    def set_dependencies(self, dependencies):
        self.dependencies = list(dependencies or [])

    def get_guid(self):
        if self.guid is None:
            self.guid = MakeGuid(self.name)
        return self.guid

    def set_msbuild_toolset(self, msbuild_toolset):
        self.msbuild_toolset = msbuild_toolset




class MSVSSolution:

    def __init__(
        self, path, version, entries=None, variants=None, websiteProperties=True
    ):
        self.path = path
        self.websiteProperties = websiteProperties
        self.version = version

        self.entries = list(entries or [])

        if variants:
            self.variants = variants[:]
        else:
            self.variants = [\""Debug|Win32\"", \""Release|Win32\""]

        self.Write()

    def Write(self, writer=gyp.common.WriteOnDiff):
        all_entries = set()
        entries_to_check = self.entries[:]
        while entries_to_check:
            e = entries_to_check.pop(0)

            if e in all_entries:
                continue

            all_entries.add(e)

            if isinstance(e, MSVSFolder):
                entries_to_check += e.entries

        all_entries = sorted(all_entries, key=attrgetter(\""path\""))

        f = writer(self.path)
        f.write(
            \""Microsoft Visual Studio Solution File, \""
            \""Format Version %s\\r\\n\"" % self.version.SolutionVersion()
        )

        sln_root = os.path.split(self.path)[0]
        for e in all_entries:
            relative_path = gyp.common.RelativePath(e.path, sln_root)
            folder_name = relative_path.replace(\""/\"", \""\\\\\"") or \"".\""
            f.write(
                'Project(\""%s\"") = \""%s\"", \""%s\"", \""%s\""\\r\\n'
                % (
                )
            )

            if self.websiteProperties:
                f.write(
                    \""\\tProjectSection(WebsiteProperties) = preProject\\r\\n\""
                    '\\t\\tDebug.AspNetCompiler.Debug = \""True\""\\r\\n'
                    '\\t\\tRelease.AspNetCompiler.Debug = \""False\""\\r\\n'
                    \""\\tEndProjectSection\\r\\n\""
                )

            if isinstance(e, MSVSFolder) and e.items:
                f.write(\""\\tProjectSection(SolutionItems) = preProject\\r\\n\"")
                for i in e.items:
                    f.write(f\""\\t\\t{i} = {i}\\r\\n\"")
                f.write(\""\\tEndProjectSection\\r\\n\"")

            if isinstance(e, MSVSProject) and e.dependencies:
                f.write(\""\\tProjectSection(ProjectDependencies) = postProject\\r\\n\"")
                for d in e.dependencies:
                    f.write(f\""\\t\\t{d.get_guid()} = {d.get_guid()}\\r\\n\"")
                f.write(\""\\tEndProjectSection\\r\\n\"")

            f.write(\""EndProject\\r\\n\"")

        f.write(\""Global\\r\\n\"")

        f.write(\""\\tGlobalSection(SolutionConfigurationPlatforms) = preSolution\\r\\n\"")
        for v in self.variants:
            f.write(f\""\\t\\t{v} = {v}\\r\\n\"")
        f.write(\""\\tEndGlobalSection\\r\\n\"")

        config_guids = []
        config_guids_overrides = {}
        for e in all_entries:
            if isinstance(e, MSVSProject):
                config_guids.append(e.get_guid())
                config_guids_overrides[e.get_guid()] = e.config_platform_overrides
        config_guids.sort()

        f.write(\""\\tGlobalSection(ProjectConfigurationPlatforms) = postSolution\\r\\n\"")
        for g in config_guids:
            for v in self.variants:
                nv = config_guids_overrides[g].get(v, v)
                f.write(
                    \""\\t\\t%s.%s.ActiveCfg = %s\\r\\n\""
                    % (
                    )
                )

                f.write(
                    \""\\t\\t%s.%s.Build.0 = %s\\r\\n\""
                    % (
                    )
                )
        f.write(\""\\tEndGlobalSection\\r\\n\"")

        f.write(\""\\tGlobalSection(SolutionProperties) = preSolution\\r\\n\"")
        f.write(\""\\t\\tHideSolutionNode = FALSE\\r\\n\"")
        f.write(\""\\tEndGlobalSection\\r\\n\"")

        if any(e.entries for e in all_entries if isinstance(e, MSVSFolder)):
            f.write(\""\\tGlobalSection(NestedProjects) = preSolution\\r\\n\"")
            for e in all_entries:
                if not isinstance(e, MSVSFolder):
                for subentry in e.entries:
                    f.write(f\""\\t\\t{subentry.get_guid()} = {e.get_guid()}\\r\\n\"")
            f.write(\""\\tEndGlobalSection\\r\\n\"")

        f.write(\""EndGlobal\\r\\n\"")

        f.close()
","['cmp', 'seed', 'MakeGuid', 'md5', 'str', 'encode', 'hexdigest', 'upper', '__cmp__', 'get_guid', 'MSVSFolder', '__init__', 'basename', 'sorted', 'attrgetter', 'list', 'MSVSProject', 'splitext', 'set_dependencies', 'set_msbuild_toolset', 'Write', 'set', 'pop', 'add', 'isinstance', 'writer', 'write', 'SolutionVersion', 'split', 'RelativePath', 'replace', 'Project', 'tProjectSection', 'tGlobalSection', 'append', 'sort', 'get', 'any', 'close']"
"https://github.com/pytorch/pytorch/blob/76bf10e551743cc8448d711e3a1ee58b6f2d3015/tools/code_analyzer/gen_oplist.py
","rt argparse
import json
import os
import sys
from functools import reduce
from typing import Any, List, Set

import yaml
from tools.lite_interpreter.gen_selected_mobile_ops_header import (
    write_selected_mobile_ops,
)
from torchgen.selective_build.selector import (
    combine_selective_builders,
    SelectiveBuilder,
)


def extract_all_operators(selective_builder: SelectiveBuilder) -> Set[str]:
    ops = []
    for op_name in selective_builder.operators.keys():
        ops.append(op_name)
    return set(ops)


def extract_training_operators(selective_builder: SelectiveBuilder) -> Set[str]:
    ops = []
    for op_name, op in selective_builder.operators.items():
        if op.is_used_for_training:
            ops.append(op_name)
    return set(ops)


def throw_if_any_op_includes_overloads(selective_builder: SelectiveBuilder) -> None:
    ops = []
    for op_name, op in selective_builder.operators.items():
        if op.include_all_overloads:
            ops.append(op_name)
    if ops:
        raise Exception(
            (
                \""Operators that include all overloads are \""
                + \""not allowed since --allow-include-all-overloads \""
                + \""was specified: {}\""
            ).format(\"", \"".join(ops))
        )


def gen_supported_mobile_models(model_dicts: List[Any], output_dir: str) -> None:

    md5_hashes = set()
    for model_dict in model_dicts:
        if \""debug_info\"" in model_dict:
            debug_info = json.loads(model_dict[\""debug_info\""][0])
            if debug_info[\""is_new_style_rule\""]:
                for asset_info in debug_info[\""asset_info\""].values():
                    md5_hashes.update(asset_info[\""md5_hash\""])

    supported_hashes = \""\""
    for md5 in md5_hashes:
        supported_hashes += f'\""{md5}\"",\\n'
    with open(
        os.path.join(output_dir, \""SupportedMobileModelsRegistration.cpp\""), \""wb\""
    ) as out_file:
        source = supported_mobile_models_source.format(
            supported_hashes_template=supported_hashes
        )
        out_file.write(source.encode(\""utf-8\""))


def main(argv: List[Any]) -> None:
    parser = argparse.ArgumentParser(description=\""Generate operator lists\"")
    parser.add_argument(
        \""--output-dir\
,""        \""--output_dir\
,""        help=(
            \""The directory to store the output yaml files (selected_mobile_ops.h, \""
            + \""selected_kernel_dtypes.h, selected_operators.yaml)\""
        ),
        required=True,
    )
    parser.add_argument(
        \""--model-file-list-path\
,""        \""--model_file_list_path\
,""        help=(
            \""Path to a file that contains the locations of individual \""
            + \""model YAML files that contain the set of used operators. This \""
            + \""file path must have a leading @-symbol, which will be stripped \""
            + \""out before processing.\""
        ),
        required=True,
    )
    parser.add_argument(
        \""--allow-include-all-overloads\
,""        \""--allow_include_all_overloads\
,""        help=(
            \""Flag to allow operators that include all overloads. \""
            + \""If not set, operators registered without using the traced style will\""
            + \""break the build.\""
        ),
        action=\""store_true\
,""        default=False,
        required=False,
    )
    options = parser.parse_args(argv)

    if os.path.isfile(options.model_file_list_path):
        print(\""Processing model file: \"", options.model_file_list_path)
        model_dicts = []
        model_dict = yaml.safe_load(open(options.model_file_list_path))
        model_dicts.append(model_dict)
    else:
        print(\""Processing model directory: \"", options.model_file_list_path)
        assert options.model_file_list_path[0] == \""@\""
        model_file_list_path = options.model_file_list_path[1:]

        model_dicts = []
        with open(model_file_list_path) as model_list_file:
            model_file_names = model_list_file.read().split()
            for model_file_name in model_file_names:
                with open(model_file_name, \""rb\"") as model_file:
                    model_dict = yaml.safe_load(model_file)
                    model_dicts.append(model_dict)

    selective_builders = [SelectiveBuilder.from_yaml_dict(m) for m in model_dicts]

    gen_supported_mobile_models(model_dicts, options.output_dir)

    selective_builder = SelectiveBuilder.from_yaml_dict({})
    if len(selective_builders) > 0:
        selective_builder = reduce(
            combine_selective_builders,
            selective_builders,
        )

    if not options.allow_include_all_overloads:
        throw_if_any_op_includes_overloads(selective_builder)
    with open(
        os.path.join(options.output_dir, \""selected_operators.yaml\""), \""wb\""
    ) as out_file:
        out_file.write(
            yaml.safe_dump(
                selective_builder.to_dict(), default_flow_style=False
            ).encode(\""utf-8\""),
        )

    write_selected_mobile_ops(
        os.path.join(options.output_dir, \""selected_mobile_ops.h\""),
        selective_builder,
    )


if __name__ == \""__main__\"":
    main(sys.argv[1:])
","['extract_all_operators', 'keys', 'append', 'set', 'extract_training_operators', 'items', 'throw_if_any_op_includes_overloads', 'Exception', 'format', 'join', 'gen_supported_mobile_models', 'loads', 'values', 'update', 'open', 'write', 'encode', 'main', 'ArgumentParser', 'add_argument', 'parse_args', 'isfile', 'safe_load', 'read', 'split', 'from_yaml_dict', 'len', 'reduce', 'safe_dump', 'to_dict', 'write_selected_mobile_ops']"
"https://github.com/localstack/localstack/blob/32a7946f878ace1a8bdd098bb3c9d53898424a78/localstack/utils/analytics/metadata.py
","import dataclasses
import logging
import os
import platform
from typing import Literal, Optional

from localstack import config, constants
from localstack.runtime import hooks
from localstack.utils.bootstrap import Container
from localstack.utils.files import rm_rf
from localstack.utils.functions import call_safe
from localstack.utils.json import FileMappedDocument
from localstack.utils.objects import singleton_factory
from localstack.utils.strings import long_uid, md5

LOG = logging.getLogger(__name__)

_PHYSICAL_ID_SALT = \""ls\""


@dataclasses.dataclass
class ClientMetadata:
    session_id: str
    machine_id: str
    api_key: str
    system: str
    version: str
    is_ci: bool
    is_docker: bool
    is_testing: bool

    def __repr__(self):
        d = dataclasses.asdict(self)

        k = d.get(\""api_key\"")
        if k:
            k = \""*\"" * len(k)
        d[\""api_key\""] = k

        return \""ClientMetadata(%s)\"" % d


def get_version_string() -> str:
    gh = config.LOCALSTACK_BUILD_GIT_HASH
    if gh:
        return f\""{constants.VERSION}:{gh}\""
    else:
        return constants.VERSION


def read_client_metadata() -> ClientMetadata:
    return ClientMetadata(
        session_id=get_session_id(),
        machine_id=get_machine_id(),
        system=get_system(),
        version=get_version_string(),
        is_ci=os.getenv(\""CI\"") is not None,
        is_docker=config.is_in_docker,
        is_testing=config.is_local_test_mode(),
    )


@singleton_factory
def get_session_id() -> str:
    return _generate_session_id()


@singleton_factory
def get_client_metadata() -> ClientMetadata:
    metadata = read_client_metadata()

    if config.DEBUG_ANALYTICS:
        LOG.info(\""resolved client metadata: %s\"", metadata)

    return metadata


@singleton_factory
def get_machine_id() -> str:
    cache_path = os.path.join(config.dirs.cache, \""machine.json\"")
    try:
        doc = FileMappedDocument(cache_path)
    except Exception:
        call_safe(rm_rf, args=(cache_path,))

        try:
            doc = FileMappedDocument(cache_path)
        except Exception:
            return _generate_machine_id()

    if \""machine_id\"" not in doc:
        doc[\""machine_id\""] = _generate_machine_id()
        call_safe(doc.save)

    return doc[\""machine_id\""]


def get_localstack_edition() -> Optional[Literal[\""enterprise\"", \""pro\"", \""community\""]]:
    if os.path.exists(\""/usr/lib/localstack/.enterprise-version\""):
        return \""enterprise\""
    elif os.path.exists(\""/usr/lib/localstack/.pro-version\""):
        return \""pro\""
    elif os.path.exists(\""/usr/lib/localstack/.community-version\""):
        return \""community\""

    return None


def is_license_activated() -> bool:
    try:
    except ImportError:
        return False

    try:
        from localstack_ext.bootstrap import licensingv2

        return licensingv2.get_licensed_environment().activated
    except Exception:
        LOG.exception(\""Could not determine license activation status\"")
        return False


def _generate_session_id() -> str:
    return long_uid()


def _anonymize_physical_id(physical_id: str) -> str:
    hashed = md5(_PHYSICAL_ID_SALT + physical_id)
    return hashed[:12]


def _generate_machine_id() -> str:
    try:
        from localstack.utils.docker_utils import DOCKER_CLIENT

        docker_id = DOCKER_CLIENT.get_system_id()
        if docker_id == DOCKER_CLIENT.get_system_id():
            return f\""dkr_{_anonymize_physical_id(docker_id)}\""
    except Exception:
        pass

    if config.is_in_docker:
        return f\""gen_{long_uid()[:12]}\""

    try:
        if os.path.exists(\""/etc/machine-id\""):
            with open(\""/etc/machine-id\"") as fd:
                machine_id = str(fd.read()).strip()
                if machine_id:
                    return f\""sys_{_anonymize_physical_id(machine_id)}\""
    except Exception:
        pass

    return f\""gen_{long_uid()[:12]}\""


def get_api_key_or_auth_token() -> Optional[str]:
    auth_token = os.environ.get(\""LOCALSTACK_AUTH_TOKEN\"", \""\"").strip(\""'\\\"" \"")
    if auth_token:
        return auth_token

    api_key = os.environ.get(\""LOCALSTACK_API_KEY\"", \""\"").strip(\""'\\\"" \"")
    if api_key:
        return api_key

    return None


@singleton_factory
def get_system() -> str:
    try:
        from localstack.utils.docker_utils import DOCKER_CLIENT

        system = DOCKER_CLIENT.get_system_info()
        if system.get(\""OSType\""):
            return system.get(\""OSType\"").lower()
    except Exception:
        pass

    if config.is_in_docker:
        return \""docker\""

    return platform.system().lower()


@hooks.prepare_host()
def prepare_host_machine_id():
    get_machine_id()


@hooks.configure_localstack_container()
def _mount_machine_file(container: Container):
    from localstack.utils.container_utils.container_client import VolumeBind

    machine_file = os.path.join(config.dirs.cache, \""machine.json\"")
    if os.path.isfile(machine_file):
        target = os.path.join(config.dirs.for_container().cache, \""machine.json\"")
        container.config.volumes.add(VolumeBind(machine_file, target, read_only=True))
","['getLogger', '__repr__', 'asdict', 'get', 'len', 'ClientMetadata', 'get_version_string', 'read_client_metadata', 'get_session_id', 'get_machine_id', 'get_system', 'getenv', 'is_local_test_mode', '_generate_session_id', 'get_client_metadata', 'info', 'join', 'FileMappedDocument', 'call_safe', '_generate_machine_id', 'get_localstack_edition', 'exists', 'is_license_activated', 'get_licensed_environment', 'exception', 'long_uid', '_anonymize_physical_id', 'md5', 'get_system_id', 'open', 'str', 'read', 'strip', 'get_api_key_or_auth_token', 'get_system_info', 'lower', 'system', 'prepare_host', 'prepare_host_machine_id', 'configure_localstack_container', '_mount_machine_file', 'isfile', 'for_container', 'add', 'VolumeBind']"
"https://github.com/chromium/chromium/blob/55ab85a3e57940182548c727e4daa79da7b09983/components/cronet/tools/update_api.py
","



import argparse
import filecmp
import fileinput
import hashlib
import os
import re
import shutil
import sys
import tempfile


REPOSITORY_ROOT = os.path.abspath(
    os.path.join(os.path.dirname(__file__), os.pardir, os.pardir, os.pardir))

sys.path.insert(0, os.path.join(REPOSITORY_ROOT, 'build/android/gyp'))

API_FILENAME = os.path.abspath(os.path.join(
    os.path.dirname(__file__), '..', 'android', 'api.txt'))
INTERFACE_API_VERSION_FILENAME = os.path.abspath(os.path.join(
    os.path.dirname(__file__), '..', 'android', 'interface_api_version.txt'))

CLASS_RE = re.compile(r'.*(class|interface) ([^ ]*) .*\\{')

UNNAMED_CLASS_RE = re.compile(r'.*\\$[0-9]')

INTERNAL_CLASS_RE = re.compile(
    r'^(?!public ((final|abstract) )?(class|interface)).*')

JAR_PATH = os.path.join(build_utils.JAVA_HOME, 'bin', 'jar')
JAVAP_PATH = os.path.join(build_utils.JAVA_HOME, 'bin', 'javap')


def generate_api(api_jar, output_filename):

  with open(output_filename, 'w') as output_file:
    output_file.write(
        'DO NOT EDIT THIS FILE, USE update_api.py TO UPDATE IT\\n\\n')

  temp_dir = tempfile.mkdtemp()
  old_cwd = os.getcwd()
  api_jar_path = os.path.abspath(api_jar)
  jar_cmd = '%s xf %s' % (os.path.relpath(JAR_PATH, temp_dir), api_jar_path)
  os.chdir(temp_dir)
  if os.system(jar_cmd):
    print('ERROR: jar failed on ' + api_jar)
    return False
  os.chdir(old_cwd)
  shutil.rmtree(os.path.join(temp_dir, 'META-INF'), ignore_errors=True)

  api_class_files = []
  for root, _, filenames in os.walk(temp_dir):
    api_class_files += [os.path.join(root, f) for f in filenames]
  api_class_files.sort()

  javap_cmd = (
      '%s -protected %s >> %s' % (
          JAVAP_PATH, ' '.join(api_class_files), output_filename)
  ).replace('$', '\\\\$')
  if os.system(javap_cmd):
    print('ERROR: javap command failed: ' + javap_cmd)
    return False
  shutil.rmtree(temp_dir)

  output_file = fileinput.FileInput(output_filename, inplace=True)
  skip_to_next_class = False
  md5_hash = hashlib.md5()
  for line in output_file:
    if line.startswith('Compiled from \""'):
      continue
    if CLASS_RE.match(line):
      skip_to_next_class = (
          UNNAMED_CLASS_RE.match(line)) or (INTERNAL_CLASS_RE.match(line))
    if skip_to_next_class:
      skip_to_next_class = line != '}'
      continue
    md5_hash.update(line.encode('utf8'))
    sys.stdout.write(line)
  output_file.close()
  with open(output_filename, 'a') as output_file:
    output_file.write('Stamp: %s\\n' % md5_hash.hexdigest())
  return True


def check_up_to_date(api_jar):

  [_, temp_filename] = tempfile.mkstemp()
  if not generate_api(api_jar, temp_filename):
    return False
  ret = filecmp.cmp(API_FILENAME, temp_filename)
  os.remove(temp_filename)
  return ret


def check_api_update(old_api, new_api):
  new_hash = hashlib.md5()
  old_hash = hashlib.md5()
  seen_stamp = False
  with open(old_api, 'r') as old_api_file, open(new_api, 'r') as new_api_file:
    for old_line in old_api_file:
      while True:
        new_line = new_api_file.readline()
        if seen_stamp:
          print('ERROR: Stamp is not the last line.')
          return False
        if new_line.startswith('Stamp: ') and old_line.startswith('Stamp: '):
          if old_line != 'Stamp: %s\\n' % old_hash.hexdigest():
            print('ERROR: Prior api.txt not stamped by update_api.py')
            return False
          if new_line != 'Stamp: %s\\n' % new_hash.hexdigest():
            print('ERROR: New api.txt not stamped by update_api.py')
            return False
          seen_stamp = True
          break
        new_hash.update(new_line.encode('utf8'))
        if new_line == old_line:
          break
        if not new_line:
          if old_line.startswith('Stamp: '):
            print('ERROR: New api.txt not stamped by update_api.py')
          else:
            print('ERROR: This API was modified or removed:')
            print('           ' + old_line)
            print('       Cronet API methods and classes cannot be modified.')
          return False
      old_hash.update(old_line.encode('utf8'))
  if not seen_stamp:
    print('ERROR: api.txt not stamped by update_api.py.')
    return False
  return True


def main(args):
  parser = argparse.ArgumentParser(description='Update Cronet api.txt.')
  parser.add_argument('--api_jar',
                      help='Path to API jar (i.e. cronet_api.jar)',
                      required=True,
                      metavar='path/to/cronet_api.jar')
  parser.add_argument('--ignore_check_errors',
                      help='If true, ignore errors from verification checks',
                      required=False,
                      default=False,
                      action='store_true')
  opts = parser.parse_args(args)

  if check_up_to_date(opts.api_jar):
    return True

  [_, temp_filename] = tempfile.mkstemp()
  if not generate_api(opts.api_jar, temp_filename):
    os.remove(temp_filename)
    return False

  update_ok = check_api_update(API_FILENAME, temp_filename)
  if not update_ok:
    if opts.ignore_check_errors:
      print('ignore_check_errors set, updating API anyway')
    else:
      os.remove(temp_filename)
      return False

  with open(INTERFACE_API_VERSION_FILENAME, 'r+') as f:
    version = int(f.read())
    f.seek(0)
    f.write(str(version + 1))
  shutil.move(temp_filename, API_FILENAME)
  return True


if __name__ == '__main__':
  sys.exit(0 if main(sys.argv[1:]) else -1)
","['abspath', 'join', 'dirname', 'insert', 'compile', 'generate_api', 'open', 'write', 'mkdtemp', 'getcwd', 'relpath', 'chdir', 'system', 'rmtree', 'walk', 'sort', 'replace', 'FileInput', 'md5', 'startswith', 'match', 'update', 'encode', 'close', 'hexdigest', 'check_up_to_date', 'mkstemp', 'cmp', 'remove', 'check_api_update', 'readline', 'main', 'ArgumentParser', 'add_argument', 'parse_args', 'int', 'read', 'seek', 'str', 'move', 'exit']"
"https://github.com/ansible/ansible/blob/fbdb666411f0d2c833e2a74cbf35593b22abb69f/lib/ansible/plugins/lookup/password.py
"," __future__ import annotations




import os
import string
import time
import hashlib

from ansible.errors import AnsibleError, AnsibleAssertionError
from ansible.module_utils.common.text.converters import to_bytes, to_native, to_text
from ansible.module_utils.six import string_types
from ansible.parsing.splitter import parse_kv
from ansible.plugins.lookup import LookupBase
from ansible.utils.encrypt import BaseHash, do_encrypt, random_password, random_salt
from ansible.utils.path import makedirs_safe


VALID_PARAMS = frozenset(('length', 'encrypt', 'chars', 'ident', 'seed'))


def _read_password_file(b_path):
    content = None

    if os.path.exists(b_path):
        with open(b_path, 'rb') as f:
            b_content = f.read().rstrip()
        content = to_text(b_content, errors='surrogate_or_strict')

    return content


def _gen_candidate_chars(characters):
    '''Generate a string containing all valid chars as defined by ``characters``

    :arg characters: A list of character specs. The character specs are
        shorthand names for sets of characters like 'digits', 'ascii_letters',
        or 'punctuation' or a string to be included verbatim.

    The values of each char spec can be:

    * a name of an attribute in the 'strings' module ('digits' for example).
      The value of the attribute will be added to the candidate chars.
    * a string of characters. If the string isn't an attribute in 'string'
      module, the string will be directly added to the candidate chars.

    For example::

        characters=['digits', '?|']``

    will match ``string.digits`` and add all ascii digits.  ``'?|'`` will add
    the question mark and pipe characters directly. Return will be the string::

        u'0123456789?|'
    '''
    chars = []
    for chars_spec in characters:
        chars.append(to_text(getattr(string, to_native(chars_spec), chars_spec), errors='strict'))
    chars = u''.join(chars).replace(u'\""', u'').replace(u\""'\"", u'')
    return chars


def _parse_content(content):
    '''parse our password data format into password and salt

    :arg content: The data read from the file
    :returns: password and salt
    '''
    password = content
    salt = None
    ident = None

    salt_slug = u' salt='
    ident_slug = u' ident='
    rem = u''
    try:
        sep = content.rindex(salt_slug)
    except ValueError:
        pass
    else:
        rem = content[sep + len(salt_slug):]
        password = content[:sep]

    if rem:
        try:
            sep = rem.rindex(ident_slug)
        except ValueError:
            salt = rem
        else:
            ident = rem[sep + len(ident_slug):]
            salt = rem[:sep]

    return password, salt, ident


def _format_content(password, salt, encrypt=None, ident=None):
    if not encrypt and not salt:
        return password

    if not salt:
        raise AnsibleAssertionError('_format_content was called with encryption requested but no salt value')

    if ident:
        return u'%s salt=%s ident=%s' % (password, salt, ident)
    return u'%s salt=%s' % (password, salt)


def _write_password_file(b_path, content):
    b_pathdir = os.path.dirname(b_path)
    makedirs_safe(b_pathdir, mode=0o700)

    with open(b_path, 'wb') as f:
        os.chmod(b_path, 0o600)
        b_content = to_bytes(content, errors='surrogate_or_strict') + b'\\n'
        f.write(b_content)


def _get_lock(b_path):
    first_process = False
    b_pathdir = os.path.dirname(b_path)
    lockfile_name = to_bytes(\""%s.ansible_lockfile\"" % hashlib.sha1(b_path).hexdigest())
    lockfile = os.path.join(b_pathdir, lockfile_name)
    if not os.path.exists(lockfile) and b_path != to_bytes('/dev/null'):
        try:
            makedirs_safe(b_pathdir, mode=0o700)
            fd = os.open(lockfile, os.O_CREAT | os.O_EXCL)
            os.close(fd)
            first_process = True
        except OSError as e:
            if e.strerror != 'File exists':
                raise

    counter = 0
    while os.path.exists(lockfile) and not first_process:
        time.sleep(2 ** counter)
        if counter >= 2:
            raise AnsibleError(\""Password lookup cannot get the lock in 7 seconds, abort...\""
                               \""This may caused by un-removed lockfile\""
                               \""you can manually remove it from controller machine at %s and try again\"" % lockfile)
        counter += 1
    return first_process, lockfile


def _release_lock(lockfile):
    if os.path.exists(lockfile):
        os.remove(lockfile)


class LookupModule(LookupBase):

    def _parse_parameters(self, term):
        first_split = term.split(' ', 1)
        if len(first_split) <= 1:
            relpath = term
            params = dict()
        else:
            relpath = first_split[0]
            params = parse_kv(first_split[1])
            if '_raw_params' in params:
                relpath = u' '.join((relpath, params['_raw_params']))
                del params['_raw_params']

                if not term.startswith(relpath):
                    raise AnsibleError('Unrecognized value after key=value parameters given to password lookup')

        invalid_params = frozenset(params.keys()).difference(VALID_PARAMS)
        if invalid_params:
            raise AnsibleError('Unrecognized parameter(s) given to password lookup: %s' % ', '.join(invalid_params))

        params['length'] = int(params.get('length', self.get_option('length')))
        params['encrypt'] = params.get('encrypt', self.get_option('encrypt'))
        params['ident'] = params.get('ident', self.get_option('ident'))
        params['seed'] = params.get('seed', self.get_option('seed'))

        params['chars'] = params.get('chars', self.get_option('chars'))
        if params['chars'] and isinstance(params['chars'], string_types):
            tmp_chars = []
            if u',,' in params['chars']:
                tmp_chars.append(u',')
            tmp_chars.extend(c for c in params['chars'].replace(u',,', u',').split(u',') if c)
            params['chars'] = tmp_chars

        return relpath, params

    def run(self, terms, variables, **kwargs):
        ret = []

        self.set_options(var_options=variables, direct=kwargs)

        for term in terms:

            changed = None
            relpath, params = self._parse_parameters(term)
            path = self._loader.path_dwim(relpath)
            b_path = to_bytes(path, errors='surrogate_or_strict')
            chars = _gen_candidate_chars(params['chars'])
            ident = None
            first_process = None
            lockfile = None

            try:
                first_process, lockfile = _get_lock(b_path)

                content = _read_password_file(b_path)

                if content is None or b_path == to_bytes('/dev/null'):
                    plaintext_password = random_password(params['length'], chars, params['seed'])
                    salt = None
                    changed = True
                else:
                    plaintext_password, salt, ident = _parse_content(content)

                encrypt = params['encrypt']
                if encrypt and not salt:
                    changed = True
                    try:
                        salt = random_salt(BaseHash.algorithms[encrypt].salt_size)
                    except KeyError:
                        salt = random_salt()

                if not ident:
                    ident = params['ident']
                elif params['ident'] and ident != params['ident']:
                    raise AnsibleError('The ident parameter provided (%s) does not match the stored one (%s).' % (ident, params['ident']))

                if encrypt and not ident:
                    try:
                        ident = BaseHash.algorithms[encrypt].implicit_ident
                    except KeyError:
                        ident = None
                    if ident:
                        changed = True

                if changed and b_path != to_bytes('/dev/null'):
                    content = _format_content(plaintext_password, salt, encrypt=encrypt, ident=ident)
                    _write_password_file(b_path, content)

            finally:
                if first_process:
                    _release_lock(lockfile)

            if encrypt:
                password = do_encrypt(plaintext_password, encrypt, salt=salt, ident=ident)
                ret.append(password)
            else:
                ret.append(plaintext_password)

        return ret
","['frozenset', '_read_password_file', 'exists', 'open', 'read', 'rstrip', 'to_text', '_gen_candidate_chars', 'append', 'getattr', 'to_native', 'join', 'replace', '_parse_content', 'rindex', 'len', '_format_content', 'AnsibleAssertionError', '_write_password_file', 'dirname', 'makedirs_safe', 'chmod', 'to_bytes', 'write', '_get_lock', 'sha1', 'hexdigest', 'close', 'sleep', 'AnsibleError', '_release_lock', 'remove', 'LookupModule', '_parse_parameters', 'split', 'dict', 'parse_kv', 'startswith', 'keys', 'difference', 'parameter', 'int', 'get', 'get_option', 'isinstance', 'extend', 'run', 'set_options', 'path_dwim', 'random_password', 'random_salt', 'do_encrypt']"
"https://github.com/certbot/certbot/blob/76f9a33e459243064d16a1089b3ab5ab754f55b0/certbot/certbot/_internal/account.py
","rt datetime
import functools
import hashlib
import logging
import shutil
import socket
from typing import Any
from typing import Callable
from typing import cast
from typing import Dict
from typing import List
from typing import Mapping
from typing import Optional

from cryptography.hazmat.primitives import serialization
import josepy as jose
import pyrfc3339
import pytz

from acme import fields as acme_fields
from acme import messages
from acme.client import ClientV2
from certbot import configuration
from certbot import errors
from certbot import interfaces
from certbot import util
from certbot._internal import constants
from certbot.compat import filesystem
from certbot.compat import os

logger = logging.getLogger(__name__)


class Account:

    class Meta(jose.JSONObjectWithFields):
        creation_dt: datetime.datetime = acme_fields.rfc3339(\""creation_dt\"")
        creation_host: str = jose.field(\""creation_host\"")
        register_to_eff: str = jose.field(\""register_to_eff\"", omitempty=True)

    def __init__(self, regr: messages.RegistrationResource, key: jose.JWK,
                 meta: Optional['Meta'] = None) -> None:
        self.key = key
        self.regr = regr
        self.meta = self.Meta(
            creation_dt=datetime.datetime.now(tz=pytz.UTC).replace(microsecond=0),
            creation_host=socket.getfqdn(),
            register_to_eff=None) if meta is None else meta

        try:
            hasher = hashlib.md5()
        except ValueError:
            hasher = hashlib.new('md5', **cast(Mapping[str, Any], {\""usedforsecurity\"": False}))

        hasher.update(self.key.key.public_key().public_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PublicFormat.SubjectPublicKeyInfo)
        )

        self.id = hasher.hexdigest()

    @property
    def slug(self) -> str:
        return \""{1}@{0} ({2})\"".format(pyrfc3339.generate(
            self.meta.creation_dt), self.meta.creation_host, self.id[:4])

    def __repr__(self) -> str:
        return \""<{0}({1}, {2}, {3})>\"".format(
            self.__class__.__name__, self.regr, self.id, self.meta)

    def __eq__(self, other: Any) -> bool:
        return (isinstance(other, self.__class__) and
                self.key == other.key and self.regr == other.regr and
                self.meta == other.meta)


class AccountMemoryStorage(interfaces.AccountStorage):

    def __init__(self, initial_accounts: Optional[Dict[str, Account]] = None) -> None:
        self.accounts = initial_accounts if initial_accounts is not None else {}

    def find_all(self) -> List[Account]:
        return list(self.accounts.values())

    def save(self, account: Account, client: ClientV2) -> None:
        if account.id in self.accounts:
            logger.debug(\""Overwriting account: %s\"", account.id)
        self.accounts[account.id] = account

    def load(self, account_id: str) -> Account:
        try:
            return self.accounts[account_id]
        except KeyError:
            raise errors.AccountNotFound(account_id)


class AccountFileStorage(interfaces.AccountStorage):
    def __init__(self, config: configuration.NamespaceConfig) -> None:
        self.config = config
        util.make_or_verify_dir(config.accounts_dir, 0o700, self.config.strict_permissions)

    def _account_dir_path(self, account_id: str) -> str:
        return self._account_dir_path_for_server_path(account_id, self.config.server_path)

    def _account_dir_path_for_server_path(self, account_id: str, server_path: str) -> str:
        accounts_dir = self.config.accounts_dir_for_server_path(server_path)
        return os.path.join(accounts_dir, account_id)

    @classmethod
    def _regr_path(cls, account_dir_path: str) -> str:
        return os.path.join(account_dir_path, \""regr.json\"")

    @classmethod
    def _key_path(cls, account_dir_path: str) -> str:
        return os.path.join(account_dir_path, \""private_key.json\"")

    @classmethod
    def _metadata_path(cls, account_dir_path: str) -> str:
        return os.path.join(account_dir_path, \""meta.json\"")

    def _find_all_for_server_path(self, server_path: str) -> List[Account]:
        accounts_dir = self.config.accounts_dir_for_server_path(server_path)
        try:
            candidates = os.listdir(accounts_dir)
        except OSError:
            return []

        accounts = []
        for account_id in candidates:
            try:
                accounts.append(self._load_for_server_path(account_id, server_path))
            except errors.AccountStorageError:
                logger.debug(\""Account loading problem\"", exc_info=True)

        if not accounts and server_path in constants.LE_REUSE_SERVERS:
            prev_server_path = constants.LE_REUSE_SERVERS[server_path]
            prev_accounts = self._find_all_for_server_path(prev_server_path)
            if prev_accounts:
                try:
                    self._symlink_to_accounts_dir(prev_server_path, server_path)
                except OSError:
                    return []
            accounts = prev_accounts
        return accounts

    def find_all(self) -> List[Account]:
        return self._find_all_for_server_path(self.config.server_path)

    def _symlink_to_account_dir(self, prev_server_path: str, server_path: str,
                                account_id: str) -> None:
        prev_account_dir = self._account_dir_path_for_server_path(account_id, prev_server_path)
        new_account_dir = self._account_dir_path_for_server_path(account_id, server_path)
        os.symlink(prev_account_dir, new_account_dir)

    def _symlink_to_accounts_dir(self, prev_server_path: str, server_path: str) -> None:
        accounts_dir = self.config.accounts_dir_for_server_path(server_path)
        if os.path.islink(accounts_dir):
            os.unlink(accounts_dir)
        else:
            os.rmdir(accounts_dir)
        prev_account_dir = self.config.accounts_dir_for_server_path(prev_server_path)
        os.symlink(prev_account_dir, accounts_dir)

    def _load_for_server_path(self, account_id: str, server_path: str) -> Account:
        account_dir_path = self._account_dir_path_for_server_path(account_id, server_path)
            if server_path in constants.LE_REUSE_SERVERS:
                prev_server_path = constants.LE_REUSE_SERVERS[server_path]
                prev_loaded_account = self._load_for_server_path(account_id, prev_server_path)
                accounts_dir = self.config.accounts_dir_for_server_path(server_path)
                if os.listdir(accounts_dir):
                    self._symlink_to_account_dir(prev_server_path, server_path, account_id)
                else:
                    self._symlink_to_accounts_dir(prev_server_path, server_path)
                return prev_loaded_account
            raise errors.AccountNotFound(f\""Account at {account_dir_path} does not exist\"")

        try:
            with open(self._regr_path(account_dir_path)) as regr_file:
                regr = messages.RegistrationResource.json_loads(regr_file.read())
            with open(self._key_path(account_dir_path)) as key_file:
                key = jose.JWK.json_loads(key_file.read())
            with open(self._metadata_path(account_dir_path)) as metadata_file:
                meta = Account.Meta.json_loads(metadata_file.read())
        except IOError as error:
            raise errors.AccountStorageError(error)

        return Account(regr, key, meta)

    def load(self, account_id: str) -> Account:
        return self._load_for_server_path(account_id, self.config.server_path)

    def save(self, account: Account, client: ClientV2) -> None:
        try:
            dir_path = self._prepare(account)
            self._create(account, dir_path)
            self._update_meta(account, dir_path)
            self._update_regr(account, dir_path)
        except IOError as error:
            raise errors.AccountStorageError(error)

    def update_regr(self, account: Account) -> None:
        try:
            dir_path = self._prepare(account)
            self._update_regr(account, dir_path)
        except IOError as error:
            raise errors.AccountStorageError(error)

    def update_meta(self, account: Account) -> None:
        try:
            dir_path = self._prepare(account)
            self._update_meta(account, dir_path)
        except IOError as error:
            raise errors.AccountStorageError(error)

    def delete(self, account_id: str) -> None:
        account_dir_path = self._account_dir_path(account_id)
        if not os.path.isdir(account_dir_path):
            raise errors.AccountNotFound(f\""Account at {account_dir_path} does not exist\"")
        self._delete_account_dir_for_server_path(account_id, self.config.server_path)

        if not os.listdir(self.config.accounts_dir):
            self._delete_accounts_dir_for_server_path(self.config.server_path)

    def _delete_account_dir_for_server_path(self, account_id: str, server_path: str) -> None:
        link_func = functools.partial(self._account_dir_path_for_server_path, account_id)
        nonsymlinked_dir = self._delete_links_and_find_target_dir(server_path, link_func)
        shutil.rmtree(nonsymlinked_dir)

    def _delete_accounts_dir_for_server_path(self, server_path: str) -> None:
        link_func = self.config.accounts_dir_for_server_path
        nonsymlinked_dir = self._delete_links_and_find_target_dir(server_path, link_func)
        os.rmdir(nonsymlinked_dir)

    def _delete_links_and_find_target_dir(self, server_path: str,
                                          link_func: Callable[[str], str]) -> str:
        dir_path = link_func(server_path)

        reused_servers = {}
        for k, v in constants.LE_REUSE_SERVERS.items():
            reused_servers[v] = k

        possible_next_link = True
        while possible_next_link:
            possible_next_link = False
            if server_path in reused_servers:
                next_server_path = reused_servers[server_path]
                next_dir_path = link_func(next_server_path)
                if os.path.islink(next_dir_path) and filesystem.readlink(next_dir_path) == dir_path:
                    possible_next_link = True
                    server_path = next_server_path
                    dir_path = next_dir_path

        while os.path.islink(dir_path):
            target = filesystem.readlink(dir_path)
            os.unlink(dir_path)
            dir_path = target

        return dir_path

    def _prepare(self, account: Account) -> str:
        account_dir_path = self._account_dir_path(account.id)
        util.make_or_verify_dir(account_dir_path, 0o700, self.config.strict_permissions)
        return account_dir_path

    def _create(self, account: Account, dir_path: str) -> None:
        with util.safe_open(self._key_path(dir_path), \""w\"", chmod=0o400) as key_file:
            key_file.write(account.key.json_dumps())

    def _update_regr(self, account: Account, dir_path: str) -> None:
        with open(self._regr_path(dir_path), \""w\"") as regr_file:
            regr = messages.RegistrationResource(
                body={},
                uri=account.regr.uri)
            regr_file.write(regr.json_dumps())

    def _update_meta(self, account: Account, dir_path: str) -> None:
        with open(self._metadata_path(dir_path), \""w\"") as metadata_file:
            metadata_file.write(account.meta.json_dumps())
","['getLogger', 'Meta', 'rfc3339', 'field', '__init__', 'now', 'replace', 'getfqdn', 'md5', 'new', 'cast', 'update', 'public_key', 'public_bytes', 'hexdigest', 'slug', 'format', 'generate', '__repr__', '__eq__', 'isinstance', 'AccountMemoryStorage', 'find_all', 'list', 'values', 'save', 'debug', 'load', 'AccountNotFound', 'AccountFileStorage', 'make_or_verify_dir', '_account_dir_path', '_account_dir_path_for_server_path', 'accounts_dir_for_server_path', 'join', '_regr_path', '_key_path', '_metadata_path', '_find_all_for_server_path', 'listdir', 'append', '_load_for_server_path', '_symlink_to_accounts_dir', '_symlink_to_account_dir', 'symlink', 'islink', 'unlink', 'rmdir', 'open', 'json_loads', 'read', 'AccountStorageError', 'Account', '_prepare', '_create', '_update_meta', '_update_regr', 'update_regr', 'update_meta', 'delete', 'isdir', '_delete_account_dir_for_server_path', '_delete_accounts_dir_for_server_path', 'partial', '_delete_links_and_find_target_dir', 'rmtree', 'link_func', 'items', 'readlink', 'safe_open', 'write', 'json_dumps', 'RegistrationResource']"
"https://github.com/magenta/magenta/blob/548dc4e2e6a8e3ac65e1921bd94fe589d661d47b/magenta/models/onsets_frames_transcription/data.py
","

import collections
import functools
import io
import re
import wave
import zlib

import librosa
from magenta.models.onsets_frames_transcription import audio_transform
from magenta.models.onsets_frames_transcription import constants
from magenta.models.onsets_frames_transcription import drum_mappings
from magenta.models.onsets_frames_transcription import melspec_input
from note_seq import audio_io
from note_seq import sequences_lib
from note_seq.protobuf import music_pb2
import numpy as np
import tensorflow.compat.v1 as tf


def hparams_frame_size(hparams):
  if hparams.spec_type == 'raw':
    return hparams.spec_hop_length
  return hparams.spec_n_bins


def hparams_frames_per_second(hparams):
  return hparams.sample_rate / hparams.spec_hop_length


def _wav_to_cqt(wav_audio, hparams):
  y = audio_io.wav_data_to_samples(wav_audio, hparams.sample_rate)

  cqt = np.abs(
      librosa.core.cqt(
          y,
          hparams.sample_rate,
          hop_length=hparams.spec_hop_length,
          fmin=hparams.spec_fmin,
          n_bins=hparams.spec_n_bins,
          bins_per_octave=hparams.cqt_bins_per_octave),
      dtype=np.float32)

  cqt = cqt.T
  return cqt


def _wav_to_mel(wav_audio, hparams):
  y = audio_io.wav_data_to_samples(wav_audio, hparams.sample_rate)

  mel = librosa.feature.melspectrogram(
      y,
      hparams.sample_rate,
      hop_length=hparams.spec_hop_length,
      fmin=hparams.spec_fmin,
      n_mels=hparams.spec_n_bins,
      htk=hparams.spec_mel_htk).astype(np.float32)

  mel = mel.T
  return mel


def _wav_to_framed_samples(wav_audio, hparams):
  y = audio_io.wav_data_to_samples(wav_audio, hparams.sample_rate)

  hl = hparams.spec_hop_length
  n_frames = int(np.ceil(y.shape[0] / hl))
  frames = np.zeros((n_frames, hl), dtype=np.float32)

  cutoff = (n_frames - 1) * hl
  frames[:n_frames - 1, :] = np.reshape(y[:cutoff], (n_frames - 1, hl))
  remain_len = len(y[cutoff:])
  frames[n_frames - 1, :remain_len] = y[cutoff:]

  return frames


def wav_to_spec(wav_audio, hparams):
  if hparams.spec_type == 'raw':
    spec = _wav_to_framed_samples(wav_audio, hparams)
  else:
    if hparams.spec_type == 'cqt':
      spec = _wav_to_cqt(wav_audio, hparams)
    elif hparams.spec_type == 'mel':
      spec = _wav_to_mel(wav_audio, hparams)
    else:
      raise ValueError('Invalid spec_type: {}'.format(hparams.spec_type))

    if hparams.spec_log_amplitude:
      spec = librosa.power_to_db(spec)

  return spec


def wav_to_spec_op(wav_audio, hparams):
  if hparams.spec_type == 'tflite_compat_mel':
    assert hparams.spec_log_amplitude
    spec = tflite_compat_mel(wav_audio, hparams=hparams)
  else:
    spec = tf.py_func(
        functools.partial(wav_to_spec, hparams=hparams),
        [wav_audio],
        tf.float32,
        name='wav_to_spec')
    spec.set_shape([None, hparams_frame_size(hparams)])
  return spec


def tflite_compat_mel(wav_audio, hparams):
  samples, decoded_sample_rate = tf.audio.decode_wav(
      wav_audio, desired_channels=1)
  samples = tf.squeeze(samples, axis=1)
  with tf.control_dependencies(
      [tf.assert_equal(decoded_sample_rate, hparams.sample_rate)]):
    return tflite_compat_mel_from_samples(samples, hparams)


def tflite_compat_mel_from_samples(samples, hparams):
  features = melspec_input.build_mel_calculation_graph(
      samples, hparams.sample_rate,
      window_length_seconds=2048 / hparams.sample_rate,
      hop_length_seconds=(
          hparams.spec_hop_length / hparams.sample_rate),
      num_mel_bins=hparams.spec_n_bins,
      lower_edge_hz=hparams.spec_fmin,
      upper_edge_hz=hparams.sample_rate / 2.0,
      frame_width=1,
      frame_hop=1,
  return tf.squeeze(features, 1)


def get_spectrogram_hash_op(spectrogram):
  def get_spectrogram_hash(spectrogram):
    spectrogram_serialized = io.BytesIO()
    np.save(spectrogram_serialized, spectrogram)
    spectrogram_hash = np.int64(zlib.adler32(spectrogram_serialized.getvalue()))
    spectrogram_serialized.close()
    return spectrogram_hash
  spectrogram_hash = tf.py_func(get_spectrogram_hash, [spectrogram], tf.int64,
                                name='get_spectrogram_hash')
  spectrogram_hash.set_shape([])
  return spectrogram_hash


def wav_to_num_frames(wav_audio, frames_per_second):
  w = wave.open(io.BytesIO(wav_audio))
  return np.int32(w.getnframes() / w.getframerate() * frames_per_second)


def wav_to_num_frames_op(wav_audio, frames_per_second):
  res = tf.py_func(
      functools.partial(wav_to_num_frames, frames_per_second=frames_per_second),
      [wav_audio],
      tf.int32,
      name='wav_to_num_frames_op')
  res.set_shape(())
  return res


def transform_wav_data_op(wav_data_tensor, hparams, jitter_amount_sec):

  def transform_wav_data(wav_data):
    if jitter_amount_sec:
      wav_data = audio_io.jitter_wav_data(wav_data, hparams.sample_rate,
                                          jitter_amount_sec)
    wav_data = audio_transform.transform_wav_audio(wav_data, hparams)

    return [wav_data]

  return tf.py_func(
      transform_wav_data, [wav_data_tensor],
      tf.string,
      name='transform_wav_data_op')


def sequence_to_pianoroll_op(sequence_tensor, velocity_range_tensor, hparams):

  def sequence_to_pianoroll_fn(sequence_tensor, velocity_range_tensor):
    velocity_range = music_pb2.VelocityRange.FromString(velocity_range_tensor)
    sequence = music_pb2.NoteSequence.FromString(sequence_tensor)
    sequence = sequences_lib.apply_sustain_control_changes(sequence)
    roll = sequences_lib.sequence_to_pianoroll(
        sequence,
        frames_per_second=hparams_frames_per_second(hparams),
        min_pitch=constants.MIN_MIDI_PITCH,
        max_pitch=constants.MAX_MIDI_PITCH,
        min_frame_occupancy_for_label=hparams.min_frame_occupancy_for_label,
        onset_mode=hparams.onset_mode,
        onset_length_ms=hparams.onset_length,
        offset_length_ms=hparams.offset_length,
        onset_delay_ms=hparams.onset_delay,
        min_velocity=velocity_range.min,
        max_velocity=velocity_range.max)
    return (roll.active, roll.weights, roll.onsets, roll.onset_velocities,
            roll.offsets)

  res, weighted_res, onsets, velocities, offsets = tf.py_func(
      sequence_to_pianoroll_fn, [sequence_tensor, velocity_range_tensor],
      [tf.float32, tf.float32, tf.float32, tf.float32, tf.float32],
      name='sequence_to_pianoroll_op')
  res.set_shape([None, constants.MIDI_PITCHES])
  weighted_res.set_shape([None, constants.MIDI_PITCHES])
  onsets.set_shape([None, constants.MIDI_PITCHES])
  offsets.set_shape([None, constants.MIDI_PITCHES])
  velocities.set_shape([None, constants.MIDI_PITCHES])

  return res, weighted_res, onsets, offsets, velocities


def jitter_label_op(sequence_tensor, jitter_amount_sec):

  def jitter_label(sequence_tensor):
    sequence = music_pb2.NoteSequence.FromString(sequence_tensor)
    sequence = sequences_lib.shift_sequence_times(sequence, jitter_amount_sec)
    return sequence.SerializeToString()

  return tf.py_func(jitter_label, [sequence_tensor], tf.string)


def truncate_note_sequence(sequence, truncate_secs):
  sus_sequence = sequences_lib.apply_sustain_control_changes(sequence)

  truncated_seq = music_pb2.NoteSequence()

  for note in sus_sequence.notes:
    start_time = note.start_time
    end_time = note.end_time

    if start_time > truncate_secs:
      continue

    if end_time > truncate_secs:
      end_time = truncate_secs

    modified_note = truncated_seq.notes.add()
    modified_note.MergeFrom(note)
    modified_note.start_time = start_time
    modified_note.end_time = end_time
  if truncated_seq.notes:
    truncated_seq.total_time = truncated_seq.notes[-1].end_time
  return truncated_seq


def truncate_note_sequence_op(sequence_tensor, truncated_length_frames,
                              hparams):
  def truncate(sequence_tensor, num_frames):
    sequence = music_pb2.NoteSequence.FromString(sequence_tensor)
    num_secs = num_frames / hparams_frames_per_second(hparams)
    return truncate_note_sequence(sequence, num_secs).SerializeToString()
  res = tf.py_func(
      truncate,
      [sequence_tensor, truncated_length_frames],
      tf.string)
  res.set_shape(())
  return res


InputTensors = collections.namedtuple(
    'InputTensors', ('spec', 'spectrogram_hash', 'labels', 'label_weights',
                     'length', 'onsets', 'offsets', 'velocities', 'sequence_id',
                     'note_sequence'))


def parse_example(example_proto):
  features = {
      'id': tf.FixedLenFeature(shape=(), dtype=tf.string),
      'sequence': tf.FixedLenFeature(shape=(), dtype=tf.string),
      'audio': tf.FixedLenFeature(shape=(), dtype=tf.string),
      'velocity_range': tf.FixedLenFeature(shape=(), dtype=tf.string),
  }
  record = tf.parse_single_example(example_proto, features)
  return record


def preprocess_example(example_proto, hparams, is_training):
  record = parse_example(example_proto)
  sequence_id = record['id']
  sequence = record['sequence']
  audio = record['audio']
  velocity_range = record['velocity_range']

  wav_jitter_amount_ms = label_jitter_amount_ms = 0
  if is_training and hparams.jitter_amount_ms > 0:
    wav_jitter_amount_ms = np.random.choice(hparams.jitter_amount_ms, size=1)
    label_jitter_amount_ms = wav_jitter_amount_ms

  if label_jitter_amount_ms > 0:
    sequence = jitter_label_op(sequence, label_jitter_amount_ms / 1000.)

  if hparams.backward_shift_amount_ms > 0:
    sequence = jitter_label_op(sequence,
                               hparams.backward_shift_amount_ms / 1000.)

  if is_training:
    audio = transform_wav_data_op(
        audio,
        hparams=hparams,
        jitter_amount_sec=wav_jitter_amount_ms / 1000.)

  spec = wav_to_spec_op(audio, hparams=hparams)
  spectrogram_hash = get_spectrogram_hash_op(spec)

  labels, label_weights, onsets, offsets, velocities = sequence_to_pianoroll_op(
      sequence, velocity_range, hparams=hparams)

  length = wav_to_num_frames_op(audio, hparams_frames_per_second(hparams))

  asserts = []
  if hparams.max_expected_train_example_len and is_training:
    asserts.append(
        tf.assert_less_equal(length, hparams.max_expected_train_example_len))

  with tf.control_dependencies(asserts):
    return InputTensors(
        spec=spec,
        spectrogram_hash=spectrogram_hash,
        labels=labels,
        label_weights=label_weights,
        length=length,
        onsets=onsets,
        offsets=offsets,
        velocities=velocities,
        sequence_id=sequence_id,
        note_sequence=sequence)


def input_tensors_to_example(inputs, hparams):
  del hparams

  feature = {
      'spec': tf.train.Feature(
          float_list=tf.train.FloatList(value=inputs.spec.flatten())),
      'spectrogram_hash': tf.train.Feature(
          int64_list=tf.train.Int64List(value=[inputs.spectrogram_hash])),
      'labels': tf.train.Feature(
          float_list=tf.train.FloatList(value=inputs.labels.flatten())),
      'label_weights': tf.train.Feature(
          float_list=tf.train.FloatList(value=inputs.label_weights.flatten())),
      'length': tf.train.Feature(
          int64_list=tf.train.Int64List(value=[inputs.length])),
      'onsets': tf.train.Feature(
          float_list=tf.train.FloatList(value=inputs.onsets.flatten())),
      'offsets': tf.train.Feature(
          float_list=tf.train.FloatList(value=inputs.offsets.flatten())),
      'velocities': tf.train.Feature(
          float_list=tf.train.FloatList(value=inputs.velocities.flatten())),
      'sequence_id': tf.train.Feature(
          bytes_list=tf.train.BytesList(value=[inputs.sequence_id])),
      'note_sequence': tf.train.Feature(
          bytes_list=tf.train.BytesList(value=[inputs.note_sequence])),
  }

  return tf.train.Example(features=tf.train.Features(feature=feature))


FeatureTensors = collections.namedtuple(
    'FeatureTensors', ('spec', 'length', 'sequence_id'))
LabelTensors = collections.namedtuple(
    'LabelTensors', ('labels', 'label_weights', 'onsets', 'offsets',
                     'velocities', 'note_sequence'))


def input_tensors_to_model_input(
    input_tensors, hparams, is_training, num_classes=constants.MIDI_PITCHES):
  length = tf.cast(input_tensors.length, tf.int32)
  labels = tf.reshape(input_tensors.labels, (-1, num_classes))
  label_weights = tf.reshape(input_tensors.label_weights, (-1, num_classes))
  onsets = tf.reshape(input_tensors.onsets, (-1, num_classes))
  offsets = tf.reshape(input_tensors.offsets, (-1, num_classes))
  velocities = tf.reshape(input_tensors.velocities, (-1, num_classes))
  spec = tf.reshape(input_tensors.spec, (-1, hparams_frame_size(hparams)))

  hparams_truncated_length = tf.cast(
      hparams.truncated_length_secs * hparams_frames_per_second(hparams),
      tf.int32)
  if hparams.truncated_length_secs:
    truncated_length = tf.reduce_min([hparams_truncated_length, length])
  else:
    truncated_length = length

  if is_training:
    truncated_note_sequence = tf.constant(0)
  else:
    truncated_note_sequence = truncate_note_sequence_op(
        input_tensors.note_sequence, truncated_length, hparams)

  if hparams.max_expected_train_example_len and is_training:
    if hparams.truncated_length_secs:
      assert_op = tf.assert_equal(hparams.max_expected_train_example_len,
                                  hparams_truncated_length)
      with tf.control_dependencies([assert_op]):
        final_length = hparams.max_expected_train_example_len
    else:
      final_length = hparams.max_expected_train_example_len
  else:
    final_length = truncated_length

  spec_delta = tf.shape(spec)[0] - final_length
  spec = tf.case(
      [(spec_delta < 0,
        lambda: tf.pad(spec, tf.stack([(0, -spec_delta), (0, 0)]))),
       (spec_delta > 0, lambda: spec[0:-spec_delta])],
      default=lambda: spec)
  labels_delta = tf.shape(labels)[0] - final_length
  labels = tf.case(
      [(labels_delta < 0,
        lambda: tf.pad(labels, tf.stack([(0, -labels_delta), (0, 0)]))),
       (labels_delta > 0, lambda: labels[0:-labels_delta])],
      default=lambda: labels)
  label_weights = tf.case(
      [(labels_delta < 0,
        lambda: tf.pad(label_weights, tf.stack([(0, -labels_delta), (0, 0)]))
       ), (labels_delta > 0, lambda: label_weights[0:-labels_delta])],
      default=lambda: label_weights)
  onsets = tf.case(
      [(labels_delta < 0,
        lambda: tf.pad(onsets, tf.stack([(0, -labels_delta), (0, 0)]))),
       (labels_delta > 0, lambda: onsets[0:-labels_delta])],
      default=lambda: onsets)
  offsets = tf.case(
      [(labels_delta < 0,
        lambda: tf.pad(offsets, tf.stack([(0, -labels_delta), (0, 0)]))),
       (labels_delta > 0, lambda: offsets[0:-labels_delta])],
      default=lambda: offsets)
  velocities = tf.case(
      [(labels_delta < 0,
        lambda: tf.pad(velocities, tf.stack([(0, -labels_delta), (0, 0)]))),
       (labels_delta > 0, lambda: velocities[0:-labels_delta])],
      default=lambda: velocities)

  features = FeatureTensors(
      spec=tf.reshape(spec, (final_length, hparams_frame_size(hparams), 1)),
      length=truncated_length,
      sequence_id=tf.constant(0) if is_training else input_tensors.sequence_id)
  labels = LabelTensors(
      labels=tf.reshape(labels, (final_length, num_classes)),
      label_weights=tf.reshape(label_weights, (final_length, num_classes)),
      onsets=tf.reshape(onsets, (final_length, num_classes)),
      offsets=tf.reshape(offsets, (final_length, num_classes)),
      velocities=tf.reshape(velocities, (final_length, num_classes)),
      note_sequence=truncated_note_sequence)

  if hparams.drum_data_map:
    labels_dict = labels._asdict()
    for k in ('labels', 'onsets', 'offsets'):
      labels_dict[k] = drum_mappings.map_pianoroll(
          labels_dict[k],
          mapping_name=hparams.drum_data_map,
          reduce_mode='any',
          min_pitch=constants.MIN_MIDI_PITCH)
    for k in ('label_weights', 'velocities'):
      labels_dict[k] = drum_mappings.map_pianoroll(
          labels_dict[k],
          mapping_name=hparams.drum_data_map,
          reduce_mode='max',
          min_pitch=constants.MIN_MIDI_PITCH)
    if labels_dict['note_sequence'].dtype == tf.string:
      labels_dict['note_sequence'] = tf.py_func(
          functools.partial(
              drum_mappings.map_sequences, mapping_name=hparams.drum_data_map),
          [labels_dict['note_sequence']],
          tf.string,
          name='get_drum_sequences',
          stateful=False)
      labels_dict['note_sequence'].set_shape(())
    labels = LabelTensors(**labels_dict)

  return features, labels


def generate_sharded_filenames(sharded_filenames):
  filenames = []
  for filename in sharded_filenames.split(','):
    match = re.match(r'^([^@]+)@(\\d+)$', filename)
    if not match:
      filenames.append(filename)
    else:
      num_shards = int(match.group(2))
      base = match.group(1)
      for i in range(num_shards):
        filenames.append('{}-{:0=5d}-of-{:0=5d}'.format(base, i, num_shards))
  return filenames


def sharded_tfrecord_reader(fname):
  for sfname in generate_sharded_filenames(fname):
    for r in tf.python_io.tf_record_iterator(sfname):
      yield  r


def read_examples(examples, is_training, shuffle_examples,
                  skip_n_initial_records, hparams):
  if is_training and not shuffle_examples:
    raise ValueError('shuffle_examples must be True if is_training is True')

  if isinstance(examples, str):
    sharded_filenames = generate_sharded_filenames(examples)
    if len(sharded_filenames) == 1:
      filenames = tf.data.Dataset.list_files(
          generate_sharded_filenames(examples), shuffle=shuffle_examples)
    else:
      if shuffle_examples:
        np.random.shuffle(sharded_filenames)
      filenames = tf.data.Dataset.from_tensor_slices(sharded_filenames)

    if shuffle_examples:
      input_dataset = filenames.apply(
          tf.data.experimental.parallel_interleave(
              tf.data.TFRecordDataset, sloppy=True, cycle_length=8))
    else:
      input_dataset = tf.data.TFRecordDataset(filenames)
  else:
    input_dataset = tf.data.Dataset.from_tensor_slices(examples)

  if shuffle_examples:
    input_dataset = input_dataset.shuffle(hparams.shuffle_buffer_size)
  if is_training:
    input_dataset = input_dataset.repeat()
  if skip_n_initial_records:
    input_dataset = input_dataset.skip(skip_n_initial_records)

  return input_dataset


def parse_preprocessed_example(example_proto):
  features = {
      'spec': tf.VarLenFeature(dtype=tf.float32),
      'spectrogram_hash': tf.FixedLenFeature(shape=(), dtype=tf.int64),
      'labels': tf.VarLenFeature(dtype=tf.float32),
      'label_weights': tf.VarLenFeature(dtype=tf.float32),
      'length': tf.FixedLenFeature(shape=(), dtype=tf.int64),
      'onsets': tf.VarLenFeature(dtype=tf.float32),
      'offsets': tf.VarLenFeature(dtype=tf.float32),
      'velocities': tf.VarLenFeature(dtype=tf.float32),
      'sequence_id': tf.FixedLenFeature(shape=(), dtype=tf.string),
      'note_sequence': tf.FixedLenFeature(shape=(), dtype=tf.string),
  }
  record = tf.parse_single_example(example_proto, features)
  input_tensors = InputTensors(
      spec=tf.sparse.to_dense(record['spec']),
      spectrogram_hash=record['spectrogram_hash'],
      labels=tf.sparse.to_dense(record['labels']),
      label_weights=tf.sparse.to_dense(record['label_weights']),
      length=record['length'],
      onsets=tf.sparse.to_dense(record['onsets']),
      offsets=tf.sparse.to_dense(record['offsets']),
      velocities=tf.sparse.to_dense(record['velocities']),
      sequence_id=record['sequence_id'],
      note_sequence=record['note_sequence'])
  return input_tensors


def create_batch(dataset, hparams, is_training, batch_size=None):
  if not batch_size:
    batch_size = hparams.batch_size
  if hparams.max_expected_train_example_len and is_training:
    dataset = dataset.batch(batch_size, drop_remainder=True)
  else:
    dataset = dataset.padded_batch(
        batch_size,
        padded_shapes=dataset.output_shapes,
        drop_remainder=True)
  return dataset


def combine_tensor_batch(tensor, lengths, max_length, batch_size):
  combined = tf.concat([tensor[i, :lengths[i]] for i in range(batch_size)],
                       axis=0)
  final_length = max_length * batch_size
  combined_padded = tf.pad(combined,
                           [(0, final_length - tf.shape(combined)[0])] +
                           [(0, 0)] * (combined.shape.rank - 1))
  combined_padded.set_shape([final_length] + combined_padded.shape[1:])
  return combined_padded


def splice_examples(dataset, hparams, is_training):
  if (not is_training) or hparams.splice_n_examples == 0:
    return dataset
  else:
    dataset = dataset.padded_batch(
        hparams.splice_n_examples, padded_shapes=dataset.output_shapes)

    def _splice(features, labels):
      combine = functools.partial(
          combine_tensor_batch,
          lengths=features.length,
          max_length=hparams.max_expected_train_example_len,
          batch_size=hparams.splice_n_examples)

      combined_features = FeatureTensors(
          spec=combine(features.spec),
          length=tf.reduce_sum(features.length),
          sequence_id=tf.constant(0))
      combined_labels = LabelTensors(
          labels=combine(labels.labels),
          label_weights=combine(labels.label_weights),
          onsets=combine(labels.onsets),
          offsets=combine(labels.offsets),
          velocities=combine(labels.velocities),
          note_sequence=tf.constant(0))
      return combined_features, combined_labels

    combined_dataset = dataset.map(_splice)
    return combined_dataset


def provide_batch(examples,
                  preprocess_examples,
                  params,
                  is_training,
                  shuffle_examples,
                  skip_n_initial_records):
  hparams = params

  input_dataset = read_examples(
      examples, is_training, shuffle_examples, skip_n_initial_records, hparams)

  if preprocess_examples:
    input_map_fn = functools.partial(
        preprocess_example, hparams=hparams, is_training=is_training)
  else:
    input_map_fn = parse_preprocessed_example
  input_tensors = input_dataset.map(input_map_fn)

  model_input = input_tensors.map(
      functools.partial(
          input_tensors_to_model_input,
          hparams=hparams, is_training=is_training))

  model_input = splice_examples(model_input, hparams, is_training)
  dataset = create_batch(model_input, hparams=hparams, is_training=is_training)
  return dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
","['hparams_frame_size', 'hparams_frames_per_second', '_wav_to_cqt', 'wav_data_to_samples', 'abs', 'cqt', '_wav_to_mel', 'melspectrogram', 'astype', '_wav_to_framed_samples', 'int', 'ceil', 'zeros', 'reshape', 'len', 'wav_to_spec', 'ValueError', 'format', 'power_to_db', 'wav_to_spec_op', 'tflite_compat_mel', 'py_func', 'partial', 'set_shape', 'decode_wav', 'squeeze', 'control_dependencies', 'assert_equal', 'tflite_compat_mel_from_samples', 'build_mel_calculation_graph', 'get_spectrogram_hash_op', 'get_spectrogram_hash', 'BytesIO', 'save', 'int64', 'adler32', 'getvalue', 'close', 'wav_to_num_frames', 'open', 'int32', 'getnframes', 'getframerate', 'wav_to_num_frames_op', 'transform_wav_data_op', 'transform_wav_data', 'jitter_wav_data', 'transform_wav_audio', 'sequence_to_pianoroll_op', 'sequence_to_pianoroll_fn', 'FromString', 'apply_sustain_control_changes', 'sequence_to_pianoroll', 'jitter_label_op', 'jitter_label', 'shift_sequence_times', 'SerializeToString', 'truncate_note_sequence', 'NoteSequence', 'add', 'MergeFrom', 'truncate_note_sequence_op', 'truncate', 'namedtuple', 'parse_example', 'FixedLenFeature', 'parse_single_example', 'preprocess_example', 'choice', 'append', 'assert_less_equal', 'InputTensors', 'input_tensors_to_example', 'Feature', 'FloatList', 'flatten', 'Int64List', 'BytesList', 'Example', 'Features', 'input_tensors_to_model_input', 'cast', 'reduce_min', 'constant', 'shape', 'case', 'pad', 'stack', 'FeatureTensors', 'LabelTensors', '_asdict', 'map_pianoroll', 'generate_sharded_filenames', 'split', 'match', 'group', 'range', 'sharded_tfrecord_reader', 'tf_record_iterator', 'read_examples', 'isinstance', 'list_files', 'shuffle', 'from_tensor_slices', 'apply', 'parallel_interleave', 'TFRecordDataset', 'repeat', 'skip', 'parse_preprocessed_example', 'VarLenFeature', 'to_dense', 'create_batch', 'batch', 'padded_batch', 'combine_tensor_batch', 'concat', 'splice_examples', '_splice', 'combine', 'reduce_sum', 'map', 'provide_batch', 'prefetch']"
"https://github.com/o3de/o3de/blob/c7d07cc2ad12d2062ad21318e04669642ae753b6/cmake/AzAutoGen.py
","

import io
import os
import re
import sys
import time
import errno
import fnmatch
import fileinput
import logging
import argparse
import hashlib
import pathlib
from xml.sax.saxutils import escape, unescape, quoteattr

logging.basicConfig(format='[%(levelname)s] %(name)s: %(message)s')
logger = logging.getLogger('AzAutoGen')
logger.setLevel(logging.INFO)

MAX_ERRORS = 100
errorCount = 0

class AutoGenConfig:
    def __init__(self, targetName, cacheDir, outputDir, projectDir, inputFiles, expansionRules, dryrun, verbose, pythonPaths):
        self.targetName = targetName
        self.cacheDir = cacheDir
        self.outputDir = outputDir
        self.projectDir = projectDir
        self.inputFiles = inputFiles
        self.expansionRules = expansionRules
        self.dryrun = dryrun
        self.verbose = verbose
        self.pythonPaths = pythonPaths

def SanitizeTargetName(targetName):
    return re.sub(r'[^\\w]', '', targetName.lstrip('0123456789'))

def ParseInputFile(inputFilePath):
    result = []
    if inputFilePath:
        with open(inputFilePath, 'r') as file:
            inputFileContent = file.readline()
            inputFiles = inputFileContent.strip().split(\"";\"")
            result = inputFiles
    return result

def PrintError(*objs):
    print(*objs, file=sys.stderr)
    global errorCount
    errorCount += 1
    if errorCount > MAX_ERRORS:
        print(\""Maximum errors exceeded (%d) please check the tty for errors\"" % MAX_ERRORS, file=sys.stderr)
        sys.exit(1)

def PrintUnhandledExcptionInfo():
    print(\""An unexpected error occurred, please report the error you encountered and include your build output\"", file=sys.stderr)

def TransformEscape(string):
    return escape(quoteattr(unescape(string)))

def BooleanTrue(string):
    testString = string.lower().strip()
    return testString == \""true\"" or testString == \""1\""

def CamelToHuman(string):
    return string[0].upper() + re.sub(r'((?<=[a-z])[A-Z]|(?<!\\A)[A-Z](?=[a-z]))', r' \\1', string[1:])

def StripFloat(string):
    return re.sub(r'(\\d+(\\.\\d*)?|\\.\\d+)f', r'\\g<1>0', string)

def CreateHashGuid(string):
    hash = hashlib.new('md5')
    hash.update(string.encode('utf-8'))
    hashStr = hash.hexdigest()
    return (\""{\"" + hashStr[0:8] + \""-\"" + hashStr[8:12] + \""-\"" + hashStr[12:16] + \""-\"" + hashStr[16:20] + \""-\"" + hashStr[20:] + \""}\"").upper()

def CreateAZHashValue64(btyes):
    hash = hashlib.new('sha256')
    hash.update(btyes)
    hashStr = hash.hexdigest()

def EtreeToString(xmlNode):
    return etree.tostring(xmlNode)

def EtreeToStringStripped(xmlNode):
    for elem in xmlNode.iter():
        if elem.text: elem.text = elem.text.strip()
        if elem.tail: elem.tail = elem.tail.strip()
    return etree.tostring(xmlNode)

def SanitizePath(path):
    return (path or '').replace('\\\\', '/').replace('//', '/')

def SearchPaths(filename, paths=[]):
    if len(paths) > 0:
        for path in paths:
            testFile = os.path.join(path, filename)
            if os.path.exists(testFile):
                return os.path.abspath(testFile)
    if os.path.exists(filename):
        return os.path.abspath(filename)
    return None

def ComputeOutputPath(inputFiles, projectDir, outputDir):
        commonInputPath = os.path.dirname(commonInputPath)

def ProcessTemplateConversion(autogenConfig, dataInputSet, dataInputFiles, templateFile, outputFile, templateCache):
    if autogenConfig.dryrun or not dataInputFiles:
        return
    try:
        outputFile = os.path.abspath(outputFile)
        outputPath = os.path.dirname(outputFile)
        treeRoots = []
        for dataInputFile in sorted(dataInputFiles):
            try:
                if dataInputFile in dataInputSet.keys():
                    treeRoots.append(dataInputSet.get(dataInputFile))
                elif os.path.splitext(dataInputFile)[1] == \"".xml\"":
                    xml = etree.parse(dataInputFile)
                    xmlroot = xml.getroot()
                    xmlroot = xml.getroot()
                    dataInputSet[dataInputFile] = xml.getroot()
                    treeRoots.append(xml.getroot())
                else:
                    with open(dataInputFile) as jsonFile:
                        jsonData = json.load(jsonFile)
                        dataInputSet[dataInputFile] = jsonData
                        treeRoots.append(jsonData)
            except IOError as e:
                PrintError('%s(%s) : %s' % (fileinput.filename(), str(fileinput.filelineno()), e.message))
        compareFD = io.StringIO()

        searchPaths = [os.path.dirname(templateFile)]
        templateLoader = jinja2.FileSystemLoader(searchpath = searchPaths)
        templateEnv    = jinja2.Environment(bytecode_cache = templateCache, loader = templateLoader, trim_blocks = True, extensions = [\""jinja2.ext.do\"",])
        templateEnv.filters['relpath'       ] = lambda x: os.path.relpath(x, outputPath)
        templateEnv.filters['dirname'       ] = os.path.dirname
        templateEnv.filters['basename'      ] = os.path.basename
        templateEnv.filters['splitext'      ] = os.path.splitext
        templateEnv.filters['split'         ] = os.path.split
        templateEnv.filters['startswith'    ] = str.startswith
        templateEnv.filters['int'           ] = int
        templateEnv.filters['str'           ] = str
        templateEnv.filters['escape'        ] = TransformEscape
        templateEnv.filters['len'           ] = len
        templateEnv.filters['range'         ] = range
        templateEnv.filters['stripFloat'    ] = StripFloat
        templateEnv.filters['camelToHuman'  ] = CamelToHuman
        templateEnv.filters['booleanTrue'   ] = BooleanTrue
        templateEnv.filters['createHashGuid'] = CreateHashGuid
        templateEnv.filters['createAZHashValue64'] = CreateAZHashValue64
        templateEnv.filters['etreeToString' ] = EtreeToString
        templateEnv.filters['etreeToStringStripped' ] = EtreeToStringStripped
        templateJinja  = templateEnv.get_template(os.path.basename(templateFile))
        templateVars   = \\
            { \\
                \""autogenTargetName\"": autogenConfig.targetName, \\
                \""dataFiles\""        : treeRoots, \\
                \""dataFileNames\""    : dataInputFiles, \\
                \""templateName\""     : templateFile, \\
                \""outputFile\""       : outputFile, \\
                \""filename\""         : os.path.splitext(os.path.basename(outputFile))[0], \\
            }
        try:
            outputExtension = os.path.splitext(outputFile)[1]
            if outputExtension == \"".xml\"" or outputExtension == \"".xhtml\"" or outputExtension == \"".xsd\"":
                compareFD.write('<?xml version=\""1.0\""?>\\n')
                compareFD.write('<!-- Copyright (c) Contributors to the Open 3D Engine Project.                                         -->\\n')
                compareFD.write('<!-- For complete copyright and license terms please see the LICENSE at the root of this distribution. -->\\n')
                compareFD.write('\\n')
                compareFD.write('<!-- SPDX-License-Identifier: Apache-2.0 OR MIT                                -->\\n')
                compareFD.write('\\n')
                compareFD.write('<!-- This file is generated automatically at compile time, DO NOT EDIT BY HAND -->\\n')
                compareFD.write('<!-- Template Source {0};\\n * XML Sources {1}-->\\n'.format(templateFile, ', '.join(dataInputFiles)))
                compareFD.write('\\n')
            elif outputExtension == \"".lua\"":
                compareFD.write('-- Copyright (c) Contributors to the Open 3D Engine Project.\\n')
                compareFD.write('-- For complete copyright and license terms please see the LICENSE at the root of this distribution.\\n')
                compareFD.write('\\n')
                compareFD.write('-- SPDX-License-Identifier: Apache-2.0 OR MIT\\n')
                compareFD.write('\\n')
                compareFD.write('-- This file is generated automatically at compile time, DO NOT EDIT BY HAND\\n')
                compareFD.write('-- Template Source {0};\\n * XML Sources {1}\\n'.format(templateFile, ', '.join(dataInputFiles)))
                compareFD.write('\\n')
            elif outputExtension == \"".h\"" or outputExtension == \"".hpp\"" or outputExtension == \"".inl\"" or outputExtension == \"".c\"" or outputExtension == \"".cpp\"":
                compareFD.write('/*\\n')
                compareFD.write(' * Copyright (c) Contributors to the Open 3D Engine Project.\\n')
                compareFD.write(' * For complete copyright and license terms please see the LICENSE at the root of this distribution.\\n')
                compareFD.write(' *\\n')
                compareFD.write(' * SPDX-License-Identifier: Apache-2.0 OR MIT\\n')
                compareFD.write(' *\\n')
                compareFD.write(' * This file is generated automatically at compile time, DO NOT EDIT BY HAND\\n')
                compareFD.write(' * Template Source {0};\\n * Data Sources {1}\\n'.format(templateFile, ', '.join(dataInputFiles)))
                compareFD.write(' */\\n')
                compareFD.write('\\n')
            compareFD.write(templateJinja.render(templateVars))
            compareFD.write('\\n')
        except jinja2.exceptions.TemplateNotFound as e:
            PrintError('%s(1) : error TemplateNotFound %s' % (os.path.abspath(templateFile), e.message))
        except IOError as e:
            PrintError('%s(%s) : error I/O(%s) accessing %s : %s' % (fileinput.filename(), str(fileinput.filelineno()), e.errno, e.filename, e.strerror))
    except jinja2.exceptions.TemplateSyntaxError as e:
        PrintError('%s(%s) : error Template processing error: %s' % (os.path.abspath(e.filename), e.lineno, e.message))
    except jinja2.exceptions.UndefinedError as e:
        PrintError('%s(1) : error Template processing error: %s with %s' % (os.path.abspath(templateFile), e.message, ', '.join([os.path.basename(dataInputFile) for dataInputFile in dataInputFiles])))
    try:
        os.makedirs(os.path.dirname(outputFile))
    except OSError as e:
        if e.errno == errno.EEXIST:
            pass
        else:
            raise
    try:
        if os.path.isfile(outputFile):
            with open(outputFile, 'r+') as currentFile:
                currentFileStringData = currentFile.read()
                if currentFileStringData == compareFD.getvalue():
                    if autogenConfig.verbose == True:
                        print('Generated file %s is unchanged, skipping' % (outputFile))
                else:
                    currentFile.truncate()
                    with open(outputFile, 'w+') as currentFile:
                        currentFile.write(compareFD.getvalue())
                        if autogenConfig.verbose == True:
                            print(f'Generated {outputFile} with template {templateFile} and inputs, '.join(dataInputFiles))
                        else:
                            print('Generated %s' % (os.path.basename(outputFile)))
        else:
            with open(outputFile, 'w+') as outputFD:
                outputFD.write(compareFD.getvalue())
                if autogenConfig.verbose == True:
                    print(f'Generated {outputFile} using template {templateFile} and inputs, '.join(dataInputFiles))
                else:
                    print('Generated %s' % (os.path.basename(outputFile)))

    except IOError as e:
        PrintError('%s(%s) : error I/O(%s) accessing %s : %s' % (fileinput.filename(), str(fileinput.filelineno()), e.errno, e.filename, e.strerror))
    except:
        PrintError('%s(%s) : error Processing: %s' % (fileinput.filename(), str(fileinput.filelineno()), line))
        PrintUnhandledExcptionInfo()
        raise
    compareFD.close()

def ProcessExpansionRule(autogenConfig, sourceFiles, templateFiles, templateCache, expansionRule, dataInputSet, outputFiles):
    try:
        expansionRuleSet = expansionRule.split(\"",\"")
        inputFiles = expansionRuleSet[0]
        templateFile = None
        outputFile = expansionRuleSet[2]
        for fullPathTemplate in templateFiles:
            if expansionRuleSet[1] in fullPathTemplate:
                templateFile = fullPathTemplate
                break
        if templateFile is None:
            print(\""No matching template file found for %s, template may be missing from your _files.cmake\"" % expansionRuleSet[1])
            return
        testSingle = os.path.join(autogenConfig.projectDir, inputFiles)
        if os.path.isfile(testSingle):
            dataInputFiles = [os.path.abspath(testSingle)]
            outputFileAbsolute = outputFile.replace(\""$path\"", ComputeOutputPath(dataInputFiles, autogenConfig.projectDir, autogenConfig.outputDir))
            outputFileAbsolute = outputFileAbsolute.replace(\""$fileprefix\"", os.path.splitext(os.path.basename(testSingle))[0].split(\"".\"")[0])
            outputFileAbsolute = outputFileAbsolute.replace(\""$file\"", os.path.splitext(os.path.basename(testSingle))[0])
            outputFileAbsolute = SanitizePath(outputFileAbsolute)
            ProcessTemplateConversion(autogenConfig, dataInputSet, dataInputFiles, templateFile, outputFileAbsolute, templateCache)
            outputFiles.append(pathlib.PurePath(outputFileAbsolute))
        else:
            if \""$fileprefix\"" in outputFile or \""$file\"" in outputFile:
                for filename in fnmatch.filter(sourceFiles, inputFiles):
                    dataInputFiles = [os.path.abspath(filename)]
                    outputFileAbsolute = outputFile.replace(\""$path\"", ComputeOutputPath(dataInputFiles, autogenConfig.projectDir, autogenConfig.outputDir))
                    outputFileAbsolute = outputFileAbsolute.replace(\""$fileprefix\"", os.path.splitext(os.path.basename(filename))[0].split(\"".\"")[0])
                    outputFileAbsolute = outputFileAbsolute.replace(\""$file\"", os.path.splitext(os.path.basename(filename))[0])
                    outputFileAbsolute = SanitizePath(outputFileAbsolute)
                    ProcessTemplateConversion(autogenConfig, dataInputSet, dataInputFiles, templateFile, outputFileAbsolute, templateCache)
                    outputFiles.append(pathlib.PurePath(outputFileAbsolute))
            else:
                dataInputFiles = [os.path.abspath(file) for file in fnmatch.filter(sourceFiles, inputFiles)]
                if \""$path\"" in outputFile:
                    outputFileAbsolute = outputFile.replace(\""$path\"", ComputeOutputPath(dataInputFiles, autogenConfig.projectDir, autogenConfig.outputDir))
                    outputFileAbsolute = os.path.join(autogenConfig.outputDir, outputFile)
                outputFileAbsolute = SanitizePath(outputFileAbsolute)
                ProcessTemplateConversion(autogenConfig, dataInputSet, dataInputFiles, templateFile, outputFileAbsolute, templateCache)
                outputFiles.append(pathlib.PurePath(outputFileAbsolute))
    except IOError as e:
        PrintError('%s : error I/O(%s) accessing %s : %s' % (expansionRule, e.errno, e.filename, e.strerror))
    except:
        PrintError('%s : error Processing expansion rule' % expansionRule)
        PrintUnhandledExcptionInfo()
        raise

def ExecuteExpansionRules(autogenConfig, dataInputSet, outputFiles, pruneNonGenerated):
    global MAX_ERRORS, errorCount
    currentPath = os.getcwd()
    startTime = time.time()
    try:
        os.makedirs(autogenConfig.cacheDir)
    except OSError as e:
        if e.errno == errno.EEXIST:
            pass
        else:
            raise
    sourceFiles = []
    templateFiles = []
    for inputFile in autogenConfig.inputFiles:
        if inputFile.endswith(\"".xml\"") or inputFile.endswith(\"".json\""):
            sourceFiles.append(os.path.join(autogenConfig.projectDir, inputFile))
        elif inputFile.endswith(\"".jinja\""):
            templateFiles.append(os.path.join(autogenConfig.projectDir, inputFile))
    templateCache = jinja2.FileSystemBytecodeCache(autogenConfig.cacheDir)
    for expansionRule in autogenConfig.expansionRules:
        ProcessExpansionRule(autogenConfig, sourceFiles, templateFiles, templateCache, expansionRule, dataInputSet, outputFiles)
    if not autogenConfig.dryrun:
        if pruneNonGenerated:
            PruneNonGeneratedFiles(autogenConfig, outputFiles)
        elapsedTime = time.time() - startTime
        millis = int(round(elapsedTime * 10))
        m, s = divmod(elapsedTime, 60)
        h, m = divmod(m, 60)
        print('Total Time %d:%02d:%02d.%02d' % (h, m, s, millis))
    return errorCount == 0

def PruneNonGeneratedFiles(autogenConfig : AutoGenConfig, outputFiles : list[pathlib.PurePath]):
    '''
    Removes all files from the generated files output directories which was not generated during this invocation
    :param autogenConfig: Stores the configuration structure containing the output directory paths for generated files
    :param outputFiles: Contains the list of output files generated during the current run
    '''
    generatedOutputDirs = set()
    for outputFile in outputFiles:
        generatedOutputDirs.add(pathlib.Path(outputFile.parent))

    for outputDir in generatedOutputDirs:
        filesToRemove = []
        if outputDir.is_dir():
            for genFile in outputDir.iterdir():
                if genFile.is_file() and not genFile in outputFiles:
                    filesToRemove.append(genFile)
        if filesToRemove:
            logger.info(f'The following files will be pruned from the generated output directory \""{outputDir}\"":\\n' \\
                f'{[str(path) for path in filesToRemove]}')
            for fileToRemove in filesToRemove:
                fileToRemove.unlink()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(\""targetName\"", help=\""AzAutoGen build target name\"")
    parser.add_argument(\""cacheDir\"", help=\""location to store jinja template cache files\"")
    parser.add_argument(\""outputDir\"", help=\""location to output generated files\"")
    parser.add_argument(\""projectDir\"", help=\""location to build directory against\"")
    parser.add_argument(\""inputFilePath\"", help=\""input file which contains autogen required files to run azcg expansion rules against\"")
    parser.add_argument(\""expansionRules\"", help=\""set of azcg expansion rules for matching data files to template files\"")
    parser.add_argument(\""-n\"", \""--dryrun\"", action='store_true', help=\""does not execute autogen, only outputs the set of files that autogen would generate\"")
    parser.add_argument(\""-v\"", \""--verbose\"", action='store_true', help=\""output only the set of files that would be generated by an expansion run\"")
    parser.add_argument(\""-p\"", \""--pythonPaths\"", action='append', nargs='+', default=[\""\""], help=\""set of additional python paths to use for module imports\"")
    parser.add_argument(\""--prune\"", action='store_true', default=False,
        help=\""Prunes any files in the outputDir that was not generated by the current invocation\"")

    args = parser.parse_args()
    autogenConfig = AutoGenConfig(SanitizeTargetName(args.targetName),
                                  os.path.abspath(SanitizePath(args.cacheDir)),
                                  os.path.abspath(SanitizePath(args.outputDir)),
                                  os.path.abspath(SanitizePath(args.projectDir)),
                                  ParseInputFile(args.inputFilePath.strip()),
                                  args.expansionRules.split(\"";\""),
                                  args.dryrun,
                                  args.verbose,
                                  args.pythonPaths)

    for pythonPath in autogenConfig.pythonPaths:
        sys.path.append(pythonPath)
    import jinja2
    import xml.etree.cElementTree as etree
    import json

    dataInputSet = {}
    outputFiles  = []
    autoGenResult = ExecuteExpansionRules(autogenConfig, dataInputSet, outputFiles, args.prune)
    if autogenConfig.dryrun:
        print(\""%s\"" % ';'.join([str(path) for path in outputFiles]))
    if autoGenResult:
        sys.exit(0)
    else:
        sys.exit(1)
","['basicConfig', 'getLogger', 'setLevel', '__init__', 'SanitizeTargetName', 'sub', 'lstrip', 'ParseInputFile', 'open', 'readline', 'strip', 'split', 'PrintError', 'exit', 'PrintUnhandledExcptionInfo', 'TransformEscape', 'escape', 'quoteattr', 'unescape', 'BooleanTrue', 'lower', 'CamelToHuman', 'upper', 'StripFloat', 'CreateHashGuid', 'new', 'update', 'encode', 'hexdigest', 'CreateAZHashValue64', 'EtreeToString', 'tostring', 'EtreeToStringStripped', 'iter', 'SanitizePath', 'replace', 'SearchPaths', 'len', 'join', 'exists', 'abspath', 'ComputeOutputPath', 'dirname', 'ProcessTemplateConversion', 'sorted', 'keys', 'append', 'get', 'splitext', 'parse', 'getroot', 'load', 's', 'filename', 'str', 'filelineno', 'StringIO', 'FileSystemLoader', 'Environment', 'relpath', 'get_template', 'basename', 'write', 'format', 'render', 'O', 'makedirs', 'isfile', 'read', 'getvalue', 'truncate', 'close', 'ProcessExpansionRule', 'PurePath', 'filter', 'ExecuteExpansionRules', 'getcwd', 'time', 'endswith', 'FileSystemBytecodeCache', 'PruneNonGeneratedFiles', 'int', 'round', 'divmod', 'set', 'add', 'Path', 'is_dir', 'iterdir', 'is_file', 'info', 'unlink', 'ArgumentParser', 'add_argument', 'parse_args', 'AutoGenConfig']"
"https://github.com/openedx/edx-platform/blob/c240fd8b52bbe6b3b3c5ad543fbe7f0764e04186/xmodule/capa/xqueue_interface.py
"," typing import Dict, Optional, TYPE_CHECKING

import hashlib
import json
import logging

import requests
from django.conf import settings
from django.urls import reverse
from requests.auth import HTTPBasicAuth

if TYPE_CHECKING:
    from xmodule.capa_block import ProblemBlock

log = logging.getLogger(__name__)
dateformat = '%Y%m%d%H%M%S'

XQUEUE_METRIC_NAME = 'edxapp.xqueue'



def make_hashkey(seed):
    h = hashlib.md5()
    h.update(str(seed).encode('latin-1'))
    return h.hexdigest()


def make_xheader(lms_callback_url, lms_key, queue_name):
    return json.dumps({
        'lms_callback_url': lms_callback_url,
        'lms_key': lms_key,
        'queue_name': queue_name
    })


def parse_xreply(xreply):
    try:
        xreply = json.loads(xreply)
    except ValueError as err:
        log.error(err)
        return (1, 'unexpected reply from server')

    return_code = xreply['return_code']
    content = xreply['content']

    return (return_code, content)


class XQueueInterface:

    def __init__(self, url: str, django_auth: Dict[str, str], requests_auth: Optional[HTTPBasicAuth] = None):
        self.url = url
        self.auth = django_auth
        self.session = requests.Session()
        self.session.auth = requests_auth

    def send_to_queue(self, header, body, files_to_upload=None):

        header_info = json.loads(header)

        (error, msg) = self._send_to_queue(header, body, files_to_upload)

        if error and (msg == 'login_required'):
            (error, content) = self._login()
            if error != 0:
                log.debug(\""Failed to login to queue: %s\"", content)
                return (error, content)
            if files_to_upload is not None:
                for f in files_to_upload:
                    f.seek(0)
            (error, msg) = self._send_to_queue(header, body, files_to_upload)

        return error, msg

        payload = {
            'username': self.auth['username'],
            'password': self.auth['password']
        }
        return self._http_post(self.url + '/xqueue/login/', payload)

        payload = {
            'xqueue_header': header,
            'xqueue_body': body
        }
        files = {}
        if files_to_upload is not None:
            for f in files_to_upload:
                files.update({f.name: f})

        return self._http_post(self.url + '/xqueue/submit/', payload, files=files)

        try:
            response = self.session.post(
                url, data=data, files=files, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)
            )
        except requests.exceptions.ConnectionError as err:
            log.error(err)
            return 1, 'cannot connect to server'

        except requests.exceptions.ReadTimeout as err:
            log.error(err)
            return 1, 'failed to read from the server'

        if response.status_code not in [200]:
            return 1, 'unexpected HTTP status code [%d]' % response.status_code

        return parse_xreply(response.text)


class XQueueService:

    def __init__(self, block: 'ProblemBlock'):
        basic_auth = settings.XQUEUE_INTERFACE.get('basic_auth')
        requests_auth = HTTPBasicAuth(*basic_auth) if basic_auth else None
        self._interface = XQueueInterface(
            settings.XQUEUE_INTERFACE['url'], settings.XQUEUE_INTERFACE['django_auth'], requests_auth
        )

        self._block = block

    @property
    def interface(self):
        return self._interface

    def construct_callback(self, dispatch: str = 'score_update') -> str:
        relative_xqueue_callback_url = reverse(
            'xqueue_callback',
            kwargs=dict(
                course_id=str(self._block.scope_ids.usage_id.context_key),
                userid=str(self._block.scope_ids.user_id),
                mod_id=str(self._block.scope_ids.usage_id),
                dispatch=dispatch,
            ),
        )
        xqueue_callback_url_prefix = settings.XQUEUE_INTERFACE.get('callback_url', settings.LMS_ROOT_URL)
        return xqueue_callback_url_prefix + relative_xqueue_callback_url

    @property
    def default_queuename(self) -> str:
        course_id = self._block.scope_ids.usage_id.context_key
        return f'{course_id.org}-{course_id.course}'.replace(' ', '_')

    @property
    def waittime(self) -> int:
        return settings.XQUEUE_WAITTIME_BETWEEN_REQUESTS
","['getLogger', 'make_hashkey', 'md5', 'update', 'str', 'encode', 'hexdigest', 'make_xheader', 'dumps', 'parse_xreply', 'loads', 'error', '__init__', 'Session', 'send_to_queue', '_send_to_queue', '_login', 'debug', 'seek', '_http_post', 'post', 'get', 'HTTPBasicAuth', 'XQueueInterface', 'interface', 'construct_callback', 'reverse', 'dict', 'default_queuename', 'replace', 'waittime']"
"https://github.com/tensorflow/tensorflow/blob/3065bd3b2d2aceb8d9e2018d2187eb8145a86513/tensorflow/python/tpu/tensor_tracer.py
","
import collections
import hashlib
import operator
import os
import os.path
import sys

import numpy as np

from tensorflow.core.framework import summary_pb2
from tensorflow.python.eager import monitoring
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import func_graph
from tensorflow.python.framework import function
from tensorflow.python.framework import graph_io
from tensorflow.python.framework import ops
from tensorflow.python.framework import tensor as tensor_lib
from tensorflow.python.framework import tensor_util
from tensorflow.python.lib.io import file_io
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import array_ops_stack
from tensorflow.python.ops import cond
from tensorflow.python.ops import control_flow_case
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.ops import control_flow_util
from tensorflow.python.ops import gen_math_ops
from tensorflow.python.ops import init_ops
from tensorflow.python.ops import linalg_ops
from tensorflow.python.ops import logging_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import nn_impl
from tensorflow.python.ops import state_ops
from tensorflow.python.ops import string_ops
from tensorflow.python.ops import summary_ops_v2 as summary
from tensorflow.python.ops import variable_scope
from tensorflow.python.platform import analytics
from tensorflow.python.platform import gfile
from tensorflow.python.platform import remote_utils
from tensorflow.python.platform import tf_logging as logging
from tensorflow.python.summary import summary_iterator
from tensorflow.python.tpu import tensor_tracer_flags
from tensorflow.python.tpu import tensor_tracer_report
from tensorflow.python.tpu import tpu_replication
from tensorflow.python.tpu.ops import tpu_ops
from tensorflow.python.training import training_util

_DEVICE_TYPE_TPU = 'tpu'
_DEVICE_TYPE_CPU = 'cpu'
_TRACE_MODE_PART_TENSOR_SIZE = 3

_REASON_OUTSIDE_OP_RANGE = 'not-traced-outside-op-range'
_REASON_UNSAFE_OP = 'not-traced-unsafe-op'
_REASON_WHILELOOP_OP = 'not-traced-special-whileloop-op'
_REASON_CONTROLFLOW_OP = 'not-traced-control-flow-op'
_REASON_IN_CONTROL_FLOW = 'not-traced-in-control-flow'
_REASON_UNSAFE_SCALAR = 'not-traced-unsafe-scalar'
_REASON_SKIP_SCALAR = 'not-traced-scalar'
_REASON_LESS_INTERESTING_OP = 'not-traced-less-interesting-op'
_REASON_DEVICE_MISMATCH = 'not-traced-device-mismatch'
_REASON_DYNAMIC_SHAPE = 'not-traced-dynamic-shape'
_REASON_SCALAR_GET_TRACED = 'traced-scalar'
_REASON_TENSOR_GET_TRACED = 'traced-tensor'
_REASON_USER_INCLUDED = 'traced-user-included'
_REASON_USER_EXCLUDED = 'not-traced-user-excluded'
_REASON_NOT_EXECUTED = 'not-traced-not-in-exec-path'
_REASON_NON_NUMERIC_TENSOR = 'not-traced-non-numeric-tensor'
_REASON_FEEDS_WHILELOOP_OP = 'not-traced-feeds-special-whileloop-op'

_OUTPUT_STREAM_ESCAPE = 'file://'
_TENSOR_TRACER_COLLECTION = 'tensor_tracer_variables'
TENSOR_TRACER_SUMMARY_COLLECTION = 'tensor_tracer_summary_writers'
_TRACE_FILE_NAME = 'trace.all'
_COMPACT_TRACE_FILE_PREFIX = 'compact_trace.'
_COMPACT_TRACE_ENTRY_INIT_VALUE = -1.0
_TENSOR_TRACER_STORAGE = 'tensor_tracer_storage'
_TT_SNAPSHOT = 'tensor_tracer_snapshot'

_TT_SUMMARY_NORM = tensor_tracer_flags.TT_SUMMARY_NORM
_TT_SUMMARY_MAX = tensor_tracer_flags.TT_SUMMARY_MAX
_TT_SUMMARY_MAX_ABS = tensor_tracer_flags.TT_SUMMARY_MAX_ABS
_TT_SUMMARY_MIN = tensor_tracer_flags.TT_SUMMARY_MIN
_TT_SUMMARY_MEAN = tensor_tracer_flags.TT_SUMMARY_MEAN
_TT_SUMMARY_VAR = tensor_tracer_flags.TT_SUMMARY_VAR
_TT_SUMMARY_SIZE = tensor_tracer_flags.TT_SUMMARY_SIZE
_TT_SUMMARY_SPARSITY = tensor_tracer_flags.TT_SUMMARY_SPARSITY

_TT_SUMMARY_TAG = 'tensor_tracer_summary'
_TT_TENSORBOARD_PLUGIN_NAME = 'tensor_tracer'
_TT_HOSTCALL_KEY = 'tensor_tracer_host_call'
_TT_EVENT_FILE_SUFFIX = '.tensor_tracer'

_TT_SUMMARY_MAX_QUEUE = 10

tt_gauge = monitoring.BoolGauge('/tensorflow/api/tensor_tracer/v1',
                                'tensor tracer usage', 'method')


def _graph_summary_tag(graph):

  if graph is None:
    raise RuntimeError('graph is None')
  hash_id = hashlib.md5()
  hash_id.update(repr(graph).encode('utf-8'))
  return hash_id.hexdigest()


def set_parameters(tensor_tracer_params=None):
  enable_flags = '--%s=1' % tensor_tracer_flags.FLAG_NAME_ENABLE
  if tensor_tracer_params:
    for key, value in tensor_tracer_params.items():
      enable_flags += ' --%s=%s' % (key, value)
  os.environ[tensor_tracer_flags.FLAGS_ENV_VAR] = enable_flags


def op_priority(op_type):
  if op_type in ('Const', 'Shape', 'BroadcastGradientArgs', 'Range',
                 'VariableShape', 'Fill', 'OneHot', 'ShapeN'):
    return 7

  if op_type in ('Identity', 'Cast', 'Reshape', 'ExpandDims', 'StopGradient',
                 'PreventGradient', 'Squeeze', 'Gather', 'GatherNd'):
    return 6
  if op_type in ('ConcatV2', 'Concat', 'StridedSlice', 'Slice', 'Pack', 'Tile',
                 'CollectivePermute', 'SplitV', 'DynamicPartition'):
    return 5
  if op_type in ('Pad', 'RandomUniformInt', 'GreaterEqual'):
    return 4
  if op_type in ('Sum', 'AddV2', 'Add', 'AddN', 'BiasAdd', 'CrossReplicaSum'):
    return 3
  if op_type in ('Neg', 'Sub'):
    return 2
  if op_type in ('Mul', 'Square', 'MatMul', 'RandomUniform', 'Select',
                 'Maximum', 'Mean', 'Variance', 'Exp', 'Rsqrt'):
    return 1

  return 2


def read_tensor_tracer_event_file(event_file):

  step_occurrence_count = collections.defaultdict(int)

  step_occurrence_list = []

  for trace_event in summary_iterator.summary_iterator(event_file):
    if not trace_event.HasField('summary'):
      continue
    if len(trace_event.summary.value) != 1:
      raise ValueError('Single step contains %d summary values,'
                       ' expected 1.' % len(trace_event.summary.value))
    step = trace_event.step

    occurrence_idx = step_occurrence_count[step] - 1
    occurrence_size = len(step_occurrence_list)

    if occurrence_idx == occurrence_size:
      new_occurrence = collections.defaultdict(dict)
      step_occurrence_list.append(new_occurrence)
    else:
      if occurrence_idx > occurrence_size:
        raise ValueError('Unexpected: occurrence_idx (%d) > '
                         'occurrence_size (%d)' % (occurrence_idx,
                                                   occurrence_size))
    tensor_value = trace_event.summary.value[0]
    tensor_name = tensor_value.tag

    real_shape = [d.size for d in tensor_value.tensor.tensor_shape.dim]
    tensor_content = np.frombuffer(
        tensor_value.tensor.tensor_content,
        dtypes.DType(tensor_value.tensor.dtype).as_numpy_dtype()
        ).reshape(real_shape)
    step_occurrence_list[occurrence_idx][step][tensor_name] = tensor_content
  return step_occurrence_list


def trace_tensor(tensor, tracepoint_name=None):
  if tracepoint_name is None:
    tracepoint_name = tensor.name
  tensor.graph.get_collection(_TENSOR_TRACER_COLLECTION)
  tensor.graph.add_to_collection(_TENSOR_TRACER_COLLECTION,
                                 (tensor, tracepoint_name))
  return tensor


def keras_layer_tracepoint(layer, checkpoint_name):
  try:
    outputs = layer.output
    if tensor_util.is_tf_type(outputs):
      trace_tensor(outputs, '%s' % (checkpoint_name))
    else:
      idx = 0
      for output_tensor in outputs:
        if tensor_util.is_tf_type(outputs):
          trace_tensor(output_tensor, '%s_%d' % (checkpoint_name, idx))
        idx += 1
  except AttributeError:
    pass
  except RuntimeError:
    pass
  return layer


class TensorTracer:
  _traced_graphs = set()

  @staticmethod
  def is_enabled():
    try:
      enable = tensor_tracer_flags.TTParameters().is_enabled()
      if enable: tt_gauge.get_cell('is_enabled').set(True)
      return enable
    except (ValueError, RuntimeError) as e:
      logging.warning(
          'Tensor Tracer V1 flags processing error encountered in is_enabled '
          'check. %s', e)
      return True

  @staticmethod
  def check_device_type(device_type):

    if device_type not in (_DEVICE_TYPE_TPU, _DEVICE_TYPE_CPU):
      raise ValueError('Invalid device_type \""%s\""'%device_type)

  @staticmethod
  def check_trace_mode(device_type, trace_mode):
    if trace_mode == tensor_tracer_flags.TRACE_MODE_FULL_TENSOR_SUMMARY:
      if device_type != _DEVICE_TYPE_TPU:
        raise ValueError('Device_type \""%s\"" is not yet supported for '
                         'trace mode \""%s\""' % (device_type, trace_mode))

  @staticmethod
  def loop_cond_op(op):
    return op.type in ('LoopCond', 'RefLoopCond')

  @staticmethod
  def while_loop_op(op):
    return  (control_flow_util.IsLoopSwitch(op) or
             control_flow_util.IsLoopMerge(op) or
             control_flow_util.IsLoopEnter(op) or
             control_flow_util.IsLoopExit(op) or
             TensorTracer.loop_cond_op(op) or
             op.type in ('RefNextIteration', 'NextIteration'))

  @staticmethod
  def control_flow_op(op):
    return  (control_flow_util.IsSwitch(op) or
             control_flow_util.IsMerge(op))

  @staticmethod
  def unsafe_op(op):

    if op.type == 'Assign':
      return True
    return False

  @staticmethod
  def device_mismatch(device_type, op):
    if device_type == _DEVICE_TYPE_TPU:
      return tpu_replication._TPU_REPLICATE_ATTR not in op.node_def.attr
    return False

  @staticmethod
  def unsafe_scalar_trace(op):

    if op.type in ('LoopCond', 'Enter', 'Merge', 'Const',
                   'Switch', 'Less', 'ReadVariableOp'):
      return True
    if op.type in ('VarHandleOp', 'IteratorToStringHandle',
                   'IteratorGetNext', 'OneShotIterator',
                   'IteratorV2', 'MakeIterator',
                   'BatchDatasetV2', 'MapDataset',
                   'FixedLengthRecordDataset', 'TakeDataset', 'ZipDataset',
                   'Placeholder', 'PlaceholderWithDefault', 'StridedSlice'):
      return True
    return False

  def _is_interesting_op(self, op):
    return op_priority(op.type) <= self._parameters.trace_level

  @staticmethod
  def reason(op_idx, details):

    return '%d %s'%(op_idx, details)

  def __init__(self):
    self._replica_id = None
    self._tt_config = tensor_tracer_report.TensorTracerConfig()
    self._parameters = tensor_tracer_flags.TTParameters()
    self._host_call_fn = {}
    self._cache_variables = {}
    self._history_value_cache = {}

    self._traced_op_names = set()
    self._report_proto = None
    self._temp_cache_var = {}
    self._report_proto_path = ''
    self._outmost_context = None

  def report_proto(self):
    if self._report_proto:
      return self._report_proto
    else:
      raise ValueError('Call to report_proto must be done after tracing.'
                       'Report proto only exists for '
                       'trace_mode=[summary|full_tensor_summary]')

  def report_proto_path(self):
    return self._report_proto_path

  def _escape_namescopes(self, variable_name):
    return variable_name.replace('/', '_').replace(':', '_')

  def _cache_variable_for_graph(self, graph):
    if graph not in self._cache_variables:
      self._cache_variables[graph] = {}
    return self._cache_variables[graph]

  def _create_or_get_tensor_history_values_cache(self,
                                                 cache_name,
                                                 graph,
                                                 shape=None,
                                                 dtype=dtypes.float32):
    if graph is None:
      raise ValueError('Invalid graph.')

    if graph not in self._history_value_cache:
      self._history_value_cache[graph] = {}

    if cache_name not in self._history_value_cache[graph]:
      if shape is None:
        raise ValueError('shape must be provided at cache creation.')
      if dtype.is_integer:
        init_val = int(_COMPACT_TRACE_ENTRY_INIT_VALUE)
      else:
        init_val = _COMPACT_TRACE_ENTRY_INIT_VALUE

      with graph.as_default() as g, g.name_scope(None):
        self._history_value_cache[graph][
            cache_name] = variable_scope.get_variable(
                'tt_history' + '_' + self._escape_namescopes(cache_name),
                shape=shape,
                dtype=dtype,
                initializer=init_ops.constant_initializer(init_val),
                trainable=False,
                use_resource=True,
                collections=[
                    _TENSOR_TRACER_STORAGE, ops.GraphKeys.LOCAL_VARIABLES
                ])

    return self._history_value_cache[graph][cache_name]

  def _create_or_get_tensor_values_cache(self, cache_name, graph,
                                         shape=None, dtype=dtypes.float32):
    if graph is None:
      raise ValueError('Invalid graph.')

    graph_cache_var = self._cache_variable_for_graph(graph)

    if cache_name not in graph_cache_var:
      if shape is None:
        raise ValueError('shape must be provided at cache creation.')
      if dtype.is_integer:
        init_val = int(_COMPACT_TRACE_ENTRY_INIT_VALUE)
      else:
        init_val = _COMPACT_TRACE_ENTRY_INIT_VALUE

      with graph.as_default() as g, g.name_scope(None):
        graph_cache_var[cache_name] = variable_scope.get_variable(
            _TT_SNAPSHOT + '_' + self._escape_namescopes(cache_name),
            shape=shape, dtype=dtype,
            initializer=init_ops.constant_initializer(init_val),
            trainable=False,
            use_resource=True,
            collections=[_TENSOR_TRACER_STORAGE, ops.GraphKeys.LOCAL_VARIABLES])
    return graph_cache_var[cache_name]

  def _add_replica_id_to_graph(self):

    if self._tt_config.num_replicas:
      with ops.control_dependencies(None):
        self._replica_id = tpu_ops.tpu_replicated_input(
            list(range(self._tt_config.num_replicas)),
            name='tt_replica_id')
    else:
      self._replica_id = 'unknown'

  def _inside_op_range(self, idx):

    if idx < self._parameters.op_range[0]:
      return False
    return (self._parameters.op_range[1] < 0 or
            idx <= self._parameters.op_range[1])

  def _is_user_included_op(self, op):
    for opname_re in self._parameters.included_opname_re_list:
      if opname_re.match(op.name):
        return True

    for optype_re in self._parameters.included_optype_re_list:
      if optype_re.match(op.type):
        return True
    return False

  def _is_user_excluded_op(self, op):
    for opname_re in self._parameters.excluded_opname_re_list:
      if opname_re.match(op.name):
        return True
    for optype_re in self._parameters.excluded_optype_re_list:
      if optype_re.match(op.type):
        return True
    return False

  def _signature_types(self):
    if self._parameters.trace_mode in set([
        tensor_tracer_flags.TRACE_MODE_NAN_INF,
        tensor_tracer_flags.TRACE_MODE_NORM,
        tensor_tracer_flags.TRACE_MODE_HISTORY,
        tensor_tracer_flags.TRACE_MODE_MAX_ABS]):
      return {self._parameters.trace_mode: 0}
    if self._parameters.trace_mode == tensor_tracer_flags.TRACE_MODE_SUMMARY:
      return self._parameters.summary_signatures
    return {}

  def _num_signature_dimensions(self):
    return len(self._signature_types())

  def _use_temp_cache(self):
    if self._use_tensor_buffer():
      return False
    if self._use_tensor_values_cache():
      return self._parameters.use_temp_cache_var
    else:
      return False

  def _use_tensor_values_cache(self):
    return self._parameters.use_compact_trace

  def _use_tensor_buffer(self):
    return (self._parameters.trace_mode ==
            tensor_tracer_flags.TRACE_MODE_FULL_TENSOR_SUMMARY)

  def _merge_tensor_signatures(self, signatures):
    sorted_update = []
    if self._num_signature_dimensions() > 1:
      signature_indices = self._signature_types()
      for _, val in sorted(signatures.items(),
                           key=lambda item: signature_indices[item[0]]):
        sorted_update.append(val)
      updates = array_ops_stack.stack(
          sorted_update, axis=0, name='merge_single_op_signatures')
    elif self._num_signature_dimensions() == 1:
      (_, val), = signatures.items()
      updates = val
    else:
      raise ValueError('Cannot merge 0 signatures. Check the value passed for '
                       'flag --signatures.')
    return updates

  def _save_tensor_value_to_tmp_cache(self, cache_idx, updates, graph):
    updates = self._merge_tensor_signatures(updates)
    updates = array_ops.reshape(updates,
                                [self._num_signature_dimensions()])
    if graph not in self._temp_cache_var:
      raise RuntimeError('graph is not in self._temp_cache_var')
    if cache_idx >= len(self._temp_cache_var[graph]):
      raise RuntimeError('cache_idx (%d) is out of range (%d)' % (
          cache_idx, len(self._temp_cache_var[graph])))
    self._temp_cache_var[graph][cache_idx] = updates

  def _save_tensor_value_to_cache_op(self, cache_idx, updates, graph):
    updates = self._merge_tensor_signatures(updates)
    updates = array_ops.reshape(updates,
                                [1, self._num_signature_dimensions()])
    indices = constant_op.constant([cache_idx])
    cache = self._create_or_get_tensor_values_cache(_TT_SUMMARY_TAG, graph)
    return state_ops.scatter_update(cache, indices, updates).op

  def _snapshot_tensor(self, tensor):

    snapshot_variable = self._create_or_get_tensor_values_cache(
        tensor.name, tensor.op.graph,
        tensor.shape.as_list(), tensor.dtype)
    return state_ops.assign(snapshot_variable, tensor).op

  def _preprocess_traced_tensor(self, tensor):

    def _detect_nan_inf(tensor):

      if tensor.dtype.is_floating:
        mask = math_ops.reduce_any(
            gen_math_ops.logical_or(
                gen_math_ops.is_nan(tensor), gen_math_ops.is_inf(tensor)))
        output_tensor = cond.cond(
            mask,
            lambda: constant_op.constant([1.0]),
            lambda: constant_op.constant([0.0]))
      else:
        output_tensor = constant_op.constant([0.0])
      return output_tensor

    def _compute_signature(tensor, tf_op, cast_to_f32=True):
      if cast_to_f32:
        tensor = math_ops.cast(tensor, dtypes.float32)
      output_tensor = tf_op(tensor)
      if not output_tensor.get_shape().is_fully_defined():
        output_tensor = array_ops.reshape(output_tensor, [])
      return output_tensor

    def _show_size(tensor):
      tsize = _compute_signature(tensor, array_ops.size, cast_to_f32=False)
      return math_ops.cast(tsize, dtypes.float32)

    def _show_max(tensor, cast_to_f32=True):
      return _compute_signature(tensor, math_ops.reduce_max, cast_to_f32)

    def _show_min(tensor, cast_to_f32=True):
      return _compute_signature(tensor, math_ops.reduce_min, cast_to_f32)

    def _show_norm(tensor, cast_to_f32=True):
      return _compute_signature(tensor, linalg_ops.norm, cast_to_f32)

    def _show_sparsity(tensor, cast_to_f32=True, tolerance=1e-06):
      def sparsity_fn(tensor):
        non_zeros = math_ops.greater_equal(math_ops.abs(tensor), tolerance)
        nans = math_ops.is_nan(tensor)
        return nn_impl.zero_fraction(math_ops.logical_or(non_zeros, nans))

      return _compute_signature(tensor, sparsity_fn, cast_to_f32)

    def _show_mean_and_variance(tensor, cast_to_f32=True):
      if cast_to_f32:
        tensor = math_ops.cast(tensor, dtypes.float32)
      mean, var = nn_impl.moments(array_ops.reshape(tensor, [-1]), axes=[0])
      if not mean.get_shape().is_fully_defined():
        mean = array_ops.reshape(mean, [])
      if not var.get_shape().is_fully_defined():
        var = array_ops.reshape(var, [])
      return mean, var

    def _show_max_abs(tensor, cast_to_f32=True):
      return _compute_signature(
          tensor, lambda t: math_ops.reduce_max(math_ops.abs(t)), cast_to_f32)

    if self._parameters.trace_mode == tensor_tracer_flags.TRACE_MODE_NAN_INF:
      return {self._parameters.trace_mode: _detect_nan_inf(tensor)}
    if (self._parameters.trace_mode ==
        tensor_tracer_flags.TRACE_MODE_PART_TENSOR):
      return {self._parameters.trace_mode: tensor}
    if (self._parameters.trace_mode in (
        tensor_tracer_flags.TRACE_MODE_FULL_TENSOR,
        tensor_tracer_flags.TRACE_MODE_FULL_TENSOR_SUMMARY)):
      return {self._parameters.trace_mode: tensor}
    if self._parameters.trace_mode == tensor_tracer_flags.TRACE_MODE_NORM:
      return {self._parameters.trace_mode: array_ops.reshape(
          _show_norm(tensor), [1])}
    if self._parameters.trace_mode == tensor_tracer_flags.TRACE_MODE_HISTORY:
      return {self._parameters.trace_mode: array_ops.reshape(
          _show_norm(tensor), [1])}
    if self._parameters.trace_mode == tensor_tracer_flags.TRACE_MODE_MAX_ABS:
      return {self._parameters.trace_mode: _show_max_abs(tensor)}

    if self._parameters.trace_mode == tensor_tracer_flags.TRACE_MODE_SUMMARY:
      tensor = math_ops.cast(tensor, dtypes.float32)
      result_dict = {}
      if (_TT_SUMMARY_MEAN in self._signature_types() or
          _TT_SUMMARY_VAR in self._signature_types()):
        mean, variance = _show_mean_and_variance(tensor, cast_to_f32=False)

      for signature_name, _ in sorted(self._signature_types().items(),
                                      key=lambda x: x[1]):
        if signature_name == _TT_SUMMARY_NORM:
          signature_result_tensor = _show_norm(tensor, cast_to_f32=False)
        elif signature_name == _TT_SUMMARY_MAX:
          signature_result_tensor = _show_max(tensor, cast_to_f32=False)
        elif signature_name == _TT_SUMMARY_MAX_ABS:
          signature_result_tensor = _show_max_abs(tensor, cast_to_f32=False)
        elif signature_name == _TT_SUMMARY_MIN:
          signature_result_tensor = _show_min(tensor, cast_to_f32=False)
        elif signature_name == _TT_SUMMARY_SPARSITY:
          signature_result_tensor = _show_sparsity(tensor)
        elif signature_name == _TT_SUMMARY_SIZE:
          signature_result_tensor = _show_size(tensor)
        elif signature_name == _TT_SUMMARY_MEAN:
          signature_result_tensor = mean
        elif signature_name == _TT_SUMMARY_VAR:
          signature_result_tensor = variance
        else:
          raise ValueError('Unknown signature type :%s.' % signature_name)

        result_dict[signature_name] = signature_result_tensor
      return result_dict

    raise RuntimeError(
        'Unsupported signature for trace mode %s.'
        % self._parameters.trace_mode)

  def _make_tensor_trace_fun(self, tensor_name, tensor_trace_order):

    def _print_tensor(tensor_name, num_elements, tensor, output_tensor):

      if self._parameters.is_brief_mode():
        if tensor_name not in tensor_trace_order.tensorname_to_cache_idx:
          raise ValueError(
              'Tensor %s with name %s is not in the tensorname_to_cache_idx' %
              (tensor, tensor_name))
        msg = '%d' % tensor_trace_order.tensorname_to_cache_idx[tensor_name]
      else:
        msg = '\""%s\""' % tensor_name

      if self._parameters.trace_dir:
        output_path = os.path.join(
            self._parameters.trace_dir,
            _TRACE_FILE_NAME + self._get_outfile_suffix())
        output_stream = _OUTPUT_STREAM_ESCAPE + output_path
      else:
        output_stream = sys.stderr
      return logging_ops.print_v2(msg, array_ops.shape(output_tensor),
                                  '@', self._replica_id,
                                  '\\n', output_tensor, '\\n',
                                  summarize=num_elements,
                                  output_stream=output_stream)

    def _show_part_tensor(tensor):

      return _print_tensor(tensor_name, _TRACE_MODE_PART_TENSOR_SIZE,
                           tensor, tensor)

    def _show_full_tensor(tensor):

      return _print_tensor(tensor_name, -1, tensor, tensor)

    if (self._parameters.trace_mode ==
        tensor_tracer_flags.TRACE_MODE_PART_TENSOR):
      return _show_part_tensor
    if self._parameters.trace_mode in (
        tensor_tracer_flags.TRACE_MODE_NAN_INF,
        tensor_tracer_flags.TRACE_MODE_NORM,
        tensor_tracer_flags.TRACE_MODE_FULL_TENSOR,
        tensor_tracer_flags.TRACE_MODE_MAX_ABS,
        tensor_tracer_flags.TRACE_MODE_SUMMARY,
        tensor_tracer_flags.TRACE_MODE_HISTORY
        ):
      return _show_full_tensor

    raise RuntimeError('Full tensor support is not available with trace mode %s'
                       %self._parameters.trace_mode)

  def _is_in_control_flow(self, op):
    return control_flow_util.IsInCond(op)

  def _is_in_outmost_while_loop(self, op):
    ctxt = self._get_op_control_flow_context(op)
    outer_while_context = control_flow_util.GetContainingWhileContext(ctxt)
    return outer_while_context == control_flow_util.GetContainingWhileContext(
        self._outmost_context)

  def _should_trace_in_control_flow(self):
    if self._use_temp_cache():
      return False
    elif self._tt_config.device_type == _DEVICE_TYPE_TPU:
      return self._use_tensor_values_cache() or self._use_tensor_buffer()
    return True

  def _skip_op(self, op_id, op, ops_in_exec_path, report_handler):
    if TensorTracer.while_loop_op(op):
      report_handler.instrument_op(
          op, TensorTracer.reason(op_id, _REASON_WHILELOOP_OP))
      return True
    if TensorTracer.control_flow_op(op):
      report_handler.instrument_op(
          op, TensorTracer.reason(op_id, _REASON_CONTROLFLOW_OP))
      return True
    if TensorTracer.unsafe_op(op):
      report_handler.instrument_op(
          op, TensorTracer.reason(op_id, _REASON_UNSAFE_OP))
      return True
    if TensorTracer.device_mismatch(self._tt_config.device_type, op):
      report_handler.instrument_op(
          op, TensorTracer.reason(op_id, _REASON_DEVICE_MISMATCH))
      return True
    if op not in ops_in_exec_path:
      report_handler.instrument_op(
          op, TensorTracer.reason(op_id, _REASON_NOT_EXECUTED))
      return True
    if self._is_in_control_flow(op) or not self._is_in_outmost_while_loop(op):
      if not self._should_trace_in_control_flow():
        report_handler.instrument_op(
            op, TensorTracer.reason(op_id, _REASON_IN_CONTROL_FLOW))
        return True
    if self._is_user_included_op(op):
      report_handler.instrument_op(
          op, TensorTracer.reason(op_id, _REASON_USER_INCLUDED))
      if tensor_tracer_flags.TT_CHECK_FILTER.value:
        logging.info('USER_INCLUDED op %s', op.name)
      return False

    if not self._inside_op_range(op_id):
      report_handler.instrument_op(
          op, TensorTracer.reason(op_id, _REASON_OUTSIDE_OP_RANGE))
      return True
    if not self._is_interesting_op(op):
      report_handler.instrument_op(
          op, TensorTracer.reason(op_id, _REASON_LESS_INTERESTING_OP))
      return True
    if self._is_user_excluded_op(op):
      report_handler.instrument_op(
          op, TensorTracer.reason(op_id, _REASON_USER_EXCLUDED))
      if tensor_tracer_flags.TT_CHECK_FILTER.value:
        logging.info('USER_EXCLUDED op %s', op.name)
      return True
    return False

  def _skip_tensor(self, op_id, out_tensor, report_handler):

    non_numeric_tensor_types = set([dtypes.variant, dtypes.resource,
                                    dtypes.string])
    if out_tensor.dtype in non_numeric_tensor_types:

      report_handler.instrument_tensor(
          out_tensor, TensorTracer.reason(op_id, _REASON_NON_NUMERIC_TENSOR))
      return True
    if [consumer for consumer in out_tensor.consumers() if
        TensorTracer.while_loop_op(consumer)]:
      report_handler.instrument_tensor(
          out_tensor, TensorTracer.reason(op_id, _REASON_FEEDS_WHILELOOP_OP))
      return True
    if self._is_user_included_op(out_tensor.op):
      report_handler.instrument_tensor(
          out_tensor, TensorTracer.reason(op_id, _REASON_USER_INCLUDED))
      if tensor_tracer_flags.TT_CHECK_FILTER.value:
        logging.info('USER_INCLUDED tensor %s', out_tensor.name)
      return False
    if self._is_user_excluded_op(out_tensor.op):
      report_handler.instrument_tensor(
          out_tensor, TensorTracer.reason(op_id, _REASON_USER_EXCLUDED))
      if tensor_tracer_flags.TT_CHECK_FILTER.value:
        logging.info('USER_EXCLUDED tensor %s', out_tensor.name)
      return True
    if not out_tensor.get_shape().is_fully_defined():
      if self._parameters.trace_mode in (
          tensor_tracer_flags.TRACE_MODE_NAN_INF,
          tensor_tracer_flags.TRACE_MODE_NORM,
          tensor_tracer_flags.TRACE_MODE_HISTORY,
          tensor_tracer_flags.TRACE_MODE_MAX_ABS,
          tensor_tracer_flags.TRACE_MODE_SUMMARY
          ):
        report_handler.instrument_tensor(
            out_tensor, TensorTracer.reason(op_id, _REASON_TENSOR_GET_TRACED))
        return False
      else:
        report_handler.instrument_tensor(
            out_tensor, TensorTracer.reason(op_id, _REASON_DYNAMIC_SHAPE))
        return True
    rank = len(out_tensor.shape)
    if rank < 1:
      if self._parameters.trace_scalar_ops:
        if TensorTracer.unsafe_scalar_trace(out_tensor.op):
          report_handler.instrument_tensor(
              out_tensor, TensorTracer.reason(op_id, _REASON_UNSAFE_SCALAR))
          return True
        else:
          report_handler.instrument_tensor(
              out_tensor, TensorTracer.reason(op_id, _REASON_SCALAR_GET_TRACED))
          return False
      else:
        report_handler.instrument_tensor(
            out_tensor, TensorTracer.reason(op_id, _REASON_SKIP_SCALAR))
        return True
    else:
      report_handler.instrument_tensor(
          out_tensor, TensorTracer.reason(op_id, _REASON_TENSOR_GET_TRACED))
      return False

  def _filter_execution_path_operations(self, operations, fetches):

    if fetches is None:
      return set(operations)
    if not isinstance(fetches, (list, tuple)):
      fetches = [fetches]
    op_fetches = []
    for fetch in fetches:
      if isinstance(fetch, ops.Operation):
        op_fetches.append(fetch)
      elif isinstance(fetch, tensor_lib.Tensor):
        op_fetches.append(fetch.op)
      else:
        raise RuntimeError('Given fetch:%s is neither a tensor nor an op.'
                           %fetch)

    execution_path_operations = set(op_fetches)
    traverse_stack = list(op_fetches)
    while True:
      if not traverse_stack:
        break
      head_op = traverse_stack.pop()
      input_ops = [tensor_input.op for tensor_input in head_op.inputs]
      input_ops.extend(head_op.control_inputs)

      for input_op in input_ops:
        if input_op not in execution_path_operations:
          if TensorTracer.loop_cond_op(input_op):
            continue
          execution_path_operations.add(input_op)
          traverse_stack.append(input_op)
    return execution_path_operations

  def _determine_and_instrument_traced_tensors(self, graph_order,
                                               ops_in_exec_path,
                                               tensor_trace_points,
                                               report_handler):

    traced_tensors = []
    checkpoint_operations = set([tensor.op
                                 for (tensor, _) in tensor_trace_points])
    for op_id, op in enumerate(graph_order.operations):
      if checkpoint_operations and op not in checkpoint_operations:
        continue
      if self._skip_op(op_id, op, ops_in_exec_path, report_handler):
        continue
      for i in range(len(op.outputs)):
        out_tensor = op.outputs[i]
        if not self._skip_tensor(op_id, out_tensor, report_handler):
          traced_tensors.append(out_tensor)
    return traced_tensors

  def _check_trace_files(self):

    if not self._parameters.trace_dir:
      return
    if self._parameters.trace_mode == tensor_tracer_flags.TRACE_MODE_SUMMARY:
      return
    if not gfile.Exists(self._parameters.trace_dir):
      file_io.recursive_create_dir(self._parameters.trace_dir)
      if not gfile.Exists(self._parameters.trace_dir):
        raise RuntimeError('Failed to create trace directory at %s' %
                           self._parameters.trace_dir)

  def _create_temp_cache(self, num_traced_tensors, num_signatures, graph):
    init_value = constant_op.constant(_COMPACT_TRACE_ENTRY_INIT_VALUE,
                                      dtype=dtypes.float32,
                                      shape=[num_signatures])
    self._temp_cache_var[graph] = [
        init_value for _ in range(num_traced_tensors)]

  def _determine_trace_and_create_report(self, graph, ops_in_exec_path,
                                         graph_summary_tag):

    self._check_trace_files()

    graph_order = tensor_tracer_report.sort_tensors_and_ops(graph)
    tensor_trace_points = graph.get_collection(_TENSOR_TRACER_COLLECTION)

    report_handler = tensor_tracer_report.TTReportHandle()
    traced_tensors = self._determine_and_instrument_traced_tensors(
        graph_order, ops_in_exec_path, tensor_trace_points, report_handler)
    logging.info('TensorTracer is tracing %d tensors.', len(traced_tensors))
    if traced_tensors and tensor_tracer_flags.TT_CHECK_FILTER.value:
      raise RuntimeError('Verify ops being traced by tensor tracer.')

    tensor_trace_order = tensor_tracer_report.TensorTraceOrder(graph_order,
                                                               traced_tensors)
    num_signatures = self._num_signature_dimensions()
    if num_signatures and self._use_tensor_values_cache():
      if self._use_temp_cache():
        self._create_temp_cache(len(traced_tensors), num_signatures, graph)
      else:
        self._create_or_get_tensor_values_cache(
            _TT_SUMMARY_TAG, graph, [len(traced_tensors), num_signatures])
        if self._parameters.trace_mode in (
            tensor_tracer_flags.TRACE_MODE_HISTORY):
          self._create_or_get_tensor_history_values_cache(
              _TT_SUMMARY_TAG, graph, [len(traced_tensors), num_signatures])
    if self._parameters.trace_mode in (
        tensor_tracer_flags.TRACE_MODE_SUMMARY,
        tensor_tracer_flags.TRACE_MODE_FULL_TENSOR_SUMMARY):
      self._report_proto = report_handler.create_report_proto(
          self._tt_config, self._parameters, tensor_trace_order,
          tensor_trace_points, self._signature_types())
      if self._parameters.use_fingerprint_subdir:
        self._parameters.trace_dir = os.path.join(
            self._parameters.trace_dir, self._report_proto.fingerprint)
        logging.info('TensorTracer updating trace_dir to %s',
                     self._parameters.trace_dir)
      self._report_proto_path = report_handler.report_proto_path(
          self._parameters.trace_dir, graph_summary_tag)

      if self._parameters.report_file_path != _SKIP_REPORT_FILE:
        report_handler.write_report_proto(self._report_proto_path,
                                          self._report_proto, self._parameters)
    else:
      if self._parameters.trace_mode not in (
          tensor_tracer_flags.TRACE_MODE_HISTORY):
        report_handler.create_report(self._tt_config, self._parameters,
                                     tensor_trace_order, tensor_trace_points)
    return tensor_trace_order

  def _create_host_call(self):
    return self._parameters.trace_mode in (
        tensor_tracer_flags.TRACE_MODE_SUMMARY,
        tensor_tracer_flags.TRACE_MODE_FULL_TENSOR_SUMMARY)

  def _inspect_summary_cache(self, cache, replica_id, step_num, output_stream,
                             tensor_trace_order):
    def _inspect_tensor(tensor):
      if (self._parameters.trace_mode ==
          tensor_tracer_flags.TRACE_MODE_NAN_INF):
        return cond.cond(
            math_ops.greater(tensor, 0.0),
            lambda: 'has NaNs/Infs!',
            lambda: 'has no NaNs or Infs.')
      else:
        return tensor

    if not tensor_trace_order.traced_tensors:
      logging.warn('Inspect mode has no tensors in the cache to check.')
      return control_flow_ops.no_op

    if self._parameters.trace_mode == tensor_tracer_flags.TRACE_MODE_NAN_INF:
      step_has_nan_or_inf = math_ops.greater(math_ops.reduce_sum(cache), 0.0)
    else:
      step_has_nan_or_inf = math_ops.reduce_any(
          gen_math_ops.logical_or(
              gen_math_ops.is_nan(cache), gen_math_ops.is_inf(cache)))

    step_error_message = cond.cond(
        step_has_nan_or_inf,
        lambda: 'NaNs or Infs in the step!',
        lambda: 'No numerical issues have been found for the step.')

    if self._parameters.collect_summary_per_core:
      stats = ['\\n\\n', 'core:', replica_id, ',', 'step:', step_num, '-->',
               step_error_message,
               'Printing tensors for mode:%s...' % self._parameters.trace_mode]
    else:
      stats = ['\\n\\n', 'step:', step_num, '-->', step_error_message,
               'Printing tensors for mode:%s...' % self._parameters.trace_mode]

    for tensor_name, cache_idx in sorted(
        tensor_trace_order.tensorname_to_cache_idx.items(),
        key=lambda item: item[1]):
      if self._parameters.collect_summary_per_core:
        stats.extend([
            '\\n', 'core:', replica_id, ',', 'step:', step_num, ',',
            tensor_name, '-->', _inspect_tensor(cache[cache_idx, 0])])
      else:
        stats.extend([
            '\\n', 'step:', step_num, ',',
            tensor_name, '-->', _inspect_tensor(cache[cache_idx, 0])])
    return logging_ops.print_v2(*stats, summarize=-1,
                                output_stream=output_stream)

  def _inspect_history_cache(self, cache, replica_id, step_num,
                             tensor_trace_order):
    if not tensor_trace_order.traced_tensors:
      logging.warn('TT history mode has no tensors in the cache to check.')
      return control_flow_ops.no_op

    stats = ['\\n\\n', 'core:', replica_id, ',', 'step:', step_num]
    diffs = []
    for tensor_name, cache_idx in sorted(
        tensor_trace_order.tensorname_to_cache_idx.items(),
        key=lambda item: item[1]):

      tensor_to_write = cache[cache_idx, 0]
      snapshot_variable = self._create_or_get_tensor_history_values_cache(
          tensor_to_write.name, tensor_to_write.op.graph,
          tensor_to_write.shape.as_list(), tensor_to_write.dtype)

      with ops.control_dependencies([snapshot_variable]):
        old_value = state_ops.assign_add(snapshot_variable, 0.0)

      with ops.control_dependencies([old_value]):
        new_value = math_ops.cast(tensor_to_write, dtypes.float32)
        delta = math_ops.abs(math_ops.subtract(old_value, new_value))
        updated = state_ops.assign(snapshot_variable, new_value)
        diffs.append(delta)
      with ops.control_dependencies([updated]):
        new_value_from_var = state_ops.assign_add(snapshot_variable, 0.0)

      stats.extend([
          '\\n', 'core:', replica_id, ',', 'step:', step_num, ',',
          tensor_name, '-->', old_value, new_value_from_var, delta])

    diff_stack = array_ops_stack.stack(diffs)
    step_max = math_ops.reduce_max(diff_stack)

    return cond.cond(
        math_ops.greater(step_max, tensor_tracer_flags.DELTA_THRESHOLD.value),
        lambda: logging_ops.print_v2(*stats, summarize=-1),

  def _get_outfile_suffix(self):
    if remote_utils.is_remote_path(self._parameters.trace_dir):
      return remote_utils.get_appendable_file_encoding()
    else:
      return ''

  def _generate_flush_cache_op(self, num_replicas, on_tpu,
                               tensor_trace_order, graph):

    def _flush_fun(cache, replica_id, step_num):

      def _f(file_index):
        def _print_cache():
          replica_str = ('%d' % file_index)
          if self._parameters.trace_dir:
            output_path = (os.path.join(self._parameters.trace_dir,
                                        _COMPACT_TRACE_FILE_PREFIX)
                           + replica_str + self._get_outfile_suffix())
            output_stream = _OUTPUT_STREAM_ESCAPE + output_path
          else:
            output_stream = sys.stderr

          new_step_line = _REPLICA_ID_TAG + replica_str
          print_ops = []
          if self._parameters.inspect_trace:
            if self._num_signature_dimensions() > 1:
              raise ValueError('Inspecting multi signatures are not supported.')
            if self._parameters.trace_mode in (
                tensor_tracer_flags.TRACE_MODE_HISTORY):
              print_ops.append(
                  self._inspect_history_cache(
                      cache=cache,
                      replica_id=replica_id,
                      step_num=step_num,
                      tensor_trace_order=tensor_trace_order))
            else:
              print_ops.append(
                  self._inspect_summary_cache(
                      cache=cache,
                      replica_id=replica_id,
                      step_num=step_num,
                      output_stream=output_stream,
                      tensor_trace_order=tensor_trace_order))
          else:
            for i in range(self._num_signature_dimensions()):
              print_ops.append(logging_ops.print_v2(
                  new_step_line, '\\n',
                  cache[:, i], '\\n',
                  summarize=-1,
                  output_stream=output_stream))
          with ops.control_dependencies(print_ops):
            return constant_op.constant(0).op
        return _print_cache

      def _eq(file_index):
        return math_ops.equal(replica_id, file_index)

      flush_op_cases = {}
      flush_op_cases[_eq(0)] = _f(0)
      for i in range(1, num_replicas):
        if on_tpu and not self._parameters.collect_summary_per_core:
          flush_op_cases[_eq(i)] = control_flow_ops.no_op
        else:
          flush_op_cases[_eq(i)] = _f(i)
      return control_flow_case.case(flush_op_cases, exclusive=True)

    cache = self._create_or_get_tensor_values_cache(_TT_SUMMARY_TAG, graph)
    if self._use_temp_cache():
      cache_val = cache
    else:
      cache_val = cache.value()

    if on_tpu:
      if not self._parameters.collect_summary_per_core:
        cache_val = self.merge_caches_on_tpu(cache_val)
        cache_val = self.aggregate_global_cache(cache_val)[0]

      flush_op = tpu_replication.outside_compilation(
          _flush_fun, cache_val, self._replica_id,
          array_ops.identity(training_util.get_or_create_global_step()))
    else:
      global_step = training_util.get_or_create_global_step()
      flush_op = _flush_fun(cache_val, self._replica_id, global_step)

    if self._use_temp_cache():
      with ops.control_dependencies([flush_op]):
        return constant_op.constant(0).op
    else:
      with ops.control_dependencies([flush_op]):
        reset_value = constant_op.constant(_COMPACT_TRACE_ENTRY_INIT_VALUE,
                                           dtype=cache.dtype,
                                           shape=cache.shape)
        assign_op = state_ops.assign(cache, reset_value).op
        with ops.control_dependencies([assign_op]):
          return constant_op.constant(0).op

  def _flush_tensor_values_cache(self, tensor_fetches, op_fetches, on_tpu,
                                 tensor_trace_order, graph):
    if not tensor_trace_order.traced_tensors:
      logging.warn('No tensor values being traced. No flush cache op added.')
      return tensor_fetches
    with ops.control_dependencies(op_fetches +
                                  [tensor.op for tensor in tensor_fetches]):
      flush_cache_op = self._generate_flush_cache_op(
          self._tt_config.num_replicas, on_tpu, tensor_trace_order, graph)
      return control_flow_ops.tuple(tensor_fetches,
                                    control_inputs=[flush_cache_op])

  def _process_tensor_fetches(self, tensor_fetches):
    if tensor_fetches is None:
      raise RuntimeError('tensor_fetches provided to tensor_tracer cannot be '
                         'None.')
    if not isinstance(tensor_fetches, (list, tuple)):
      tensor_fetches = [tensor_fetches]
    elif not tensor_fetches:
      raise RuntimeError('tensor_fetches provided to tensor_tracer cannot be '
                         'empty list.')
    fetches = []
    for fetch in tensor_fetches:
      if isinstance(fetch, tensor_lib.Tensor):
        fetches.append(fetch)
      else:
        raise RuntimeError('Given tensor_fetch:%s is not a tensor.' % fetch)
    return fetches

  def _process_op_fetches(self, op_fetches):
    if op_fetches is None:
      return []

    if not isinstance(op_fetches, (list, tuple)):
      op_fetches = [op_fetches]

    fetches = []
    for fetch in op_fetches:
      if isinstance(fetch, ops.Operation):
        fetches.append(fetch)
      elif isinstance(fetch, tensor_lib.Tensor):
        fetches.append(fetch.op)
      else:
        logging.warning('Ignoring the given op_fetch:%s, which is not an op.' %
                        fetch)
    return fetches

  def _convert_fetches_to_input_format(self, input_fetches, current_fetches):
    if isinstance(input_fetches, tensor_lib.Tensor):
      if len(current_fetches) != 1:
        raise RuntimeError('Tensor tracer input/output fetches do not match.')
      return current_fetches[0]
    else:
      if len(current_fetches) != len(current_fetches):
        raise RuntimeError('Tensor tracer input/output fetches do not match.')
      elif isinstance(input_fetches, tuple):
        return tuple(current_fetches)
      else:
        return current_fetches

  def _get_op_control_flow_context(self, op):
    op_control_flow_context = op._control_flow_context
    if control_flow_util.IsLoopExit(op):
      op_control_flow_context = op_control_flow_context.outer_context
    return op_control_flow_context

  def merge_caches_on_tpu(self, local_tpu_cache_tensor):
    x = array_ops.broadcast_to(
        local_tpu_cache_tensor,
        shape=[self._tt_config.num_replicas] +
        local_tpu_cache_tensor.shape.as_list())

    if tensor_tracer_flags.TT_SINGLE_CORE_SUMMARIES.value:
      return x

    return tpu_ops.all_to_all(
        x, concat_dimension=0, split_dimension=0,
        split_count=self._tt_config.num_replicas,
        group_assignment=[list(range(self._tt_config.num_replicas))])

  def aggregate_global_cache(self, global_tt_summary_cache):

    agg_fn_map = self._parameters.get_signature_to_agg_fn_map()
    signature_idx_map = self._signature_types()
    aggregation_result = []
    for signature, idx in sorted(signature_idx_map.items(),
                                 key=operator.itemgetter(1)):
      if signature not in agg_fn_map:
        raise RuntimeError('No aggregation function is defined for '
                           'signature %s.' % signature)
      signature_tensor = global_tt_summary_cache[:, :, idx]
      agg_fn = agg_fn_map[signature]
      agg_tensor = agg_fn(signature_tensor, axis=0)
      aggregation_result.append(agg_tensor)

    merged_signatures = array_ops_stack.stack(aggregation_result)
    transposed_signatures = array_ops.transpose(merged_signatures)
    return array_ops.expand_dims(transposed_signatures, axis=0)

  def _prepare_host_call_fn(self, processed_t_fetches,
                            op_fetches, graph, graph_summary_tag):
    if self._parameters.trace_dir is None:
      raise ValueError('Provide a trace_dir for tensor tracer in summary mode. '
                       '--trace_dir=/model/dir')

    def _write_cache(step, event_file_suffix=None, **kwargs):
      file_suffix = _TT_EVENT_FILE_SUFFIX
      if event_file_suffix is not None:
        file_suffix = string_ops.string_join([file_suffix, event_file_suffix],
                                             separator='.')
      summary_write_ops = []
      summary_writer = summary.create_file_writer_v2(
          self._parameters.trace_dir,
          filename_suffix=file_suffix,
          max_queue=_TT_SUMMARY_MAX_QUEUE)
      graph.add_to_collection(
          TENSOR_TRACER_SUMMARY_COLLECTION, summary_writer)

      step_value = step[0]
      dt = step_value.dtype

      if dt.__ne__(dtypes.int64) and dt.__ne__(
          dtypes.uint64) and dt.__ne__(dtypes.float64):
        step_value = math_ops.cast(step_value, dtypes.int64)

      with summary_writer.as_default():
        summary_metadata = summary_pb2.SummaryMetadata(
            plugin_data=summary_pb2.SummaryMetadata.PluginData(
                plugin_name=_TT_TENSORBOARD_PLUGIN_NAME))
        for key, value in kwargs.items():
          if not self._parameters.collect_summary_per_core:
            if key == _TT_SUMMARY_TAG and value.shape.as_list()[0] != 1:
              value = self.aggregate_global_cache(value)
          with ops.control_dependencies([summary_writer.init()]):
            summary_write_ops.append(summary.write(
                _TT_SUMMARY_TAG + '/' + key + '.' + graph_summary_tag,
                value, metadata=summary_metadata,
                step=step_value))
      return control_flow_ops.group(summary_write_ops)

    global_step = training_util.get_or_create_global_step()
    step = array_ops.reshape(global_step, [1])
    self._host_call_fn = {}

    host_call_deps = op_fetches + [tensor.op for tensor in processed_t_fetches]

    caches_to_write = {}
    with ops.control_dependencies(host_call_deps):
      all_caches = self._cache_variable_for_graph(graph)
      for cache_name, cache_variable in all_caches.items():
        new_cache_shape = [1]
        new_cache_shape.extend(cache_variable.shape.as_list())
        cache = array_ops.reshape(cache_variable, new_cache_shape)
        caches_to_write[cache_name] = cache
    caches_to_write['step'] = step
    self._host_call_fn[_TT_HOSTCALL_KEY] = (_write_cache, caches_to_write)

  def host_call_deps_and_fn(self):
    return self._host_call_fn

  def get_traced_op_names(self):
    return self._traced_op_names

  def _trace_execution(self, graph,
                       tensor_fetches,
                       op_fetches=None,
                       on_tpu=True):
    def _cast_unsupported_dtypes(tensor):

      if tensor.dtype.__eq__(dtypes.int64):
        return math_ops.cast(tensor, dtypes.int32)
      if tensor.dtype.__eq__(dtypes.bfloat16) or tensor.dtype.__eq__(
          dtypes.float16):
        return math_ops.cast(tensor, dtypes.float32)
      return tensor

    trace_mode = self._parameters.trace_mode
    device_type = self._tt_config.device_type
    self._outmost_context = graph._get_control_flow_context()

    analytics.track_usage('tensor_tracer', [trace_mode, device_type])
    TensorTracer.check_device_type(device_type)
    TensorTracer.check_trace_mode(device_type, trace_mode)
    processed_t_fetches = self._process_tensor_fetches(tensor_fetches)
    op_fetches = self._process_op_fetches(op_fetches)
    all_fetches = op_fetches + [tensor.op for tensor in processed_t_fetches]

    exec_op_set = self._filter_execution_path_operations(graph.get_operations(),
                                                         all_fetches)
    graph_summary_tag = _graph_summary_tag(graph)

    tensor_trace_order = self._determine_trace_and_create_report(
        graph, exec_op_set, graph_summary_tag)

    tensor_fetch_set = set(processed_t_fetches)
    tracing_ops = []

    sorted_exec_op_list = list(exec_op_set)
    sorted_exec_op_list.sort(key=lambda op: op.name)
    for op in sorted_exec_op_list:
      for i in range(len(op.outputs)):
        out_tensor = op.outputs[i]
        tensor_name = out_tensor.name
        if tensor_name not in tensor_trace_order.tensorname_to_cache_idx:
          continue
        self._traced_op_names.add(op.name)
        consumers = out_tensor.consumers()
        consumers = [cop for cop in consumers if cop in exec_op_set]

        is_a_fetched_tensor = out_tensor in tensor_fetch_set
        if (not consumers) and (not is_a_fetched_tensor):
          continue

        op_control_flow_context = self._get_op_control_flow_context(op)
        if op_control_flow_context:
          graph._set_control_flow_context(op_control_flow_context)

        processed_tensors = self._preprocess_traced_tensor(out_tensor)

        if on_tpu:
          for signature in processed_tensors.keys():
            processed_tensors[signature] = _cast_unsupported_dtypes(
                processed_tensors[signature])

        if self._use_tensor_values_cache():
          if self._use_temp_cache():
            cache_idx = tensor_trace_order.tensorname_to_cache_idx[tensor_name]
            self._save_tensor_value_to_tmp_cache(cache_idx,
                                                 processed_tensors,
                                                 graph)
            trace_op = None
          else:
            cache_idx = tensor_trace_order.tensorname_to_cache_idx[tensor_name]
            trace_op = self._save_tensor_value_to_cache_op(cache_idx,
                                                           processed_tensors,
                                                           graph)
        elif self._use_tensor_buffer():
          if len(processed_tensors) != 1:
            raise RuntimeError('Multiple stats are only allowed in compact '
                               'mode.')
          processed_out_tensor = list(processed_tensors.values())[0]
          trace_op = self._snapshot_tensor(processed_out_tensor)
        else:

          def tpu_wrap_trace_fn(tensor, out_tensor_name):
            tensor_trace_fn = self._make_tensor_trace_fun(out_tensor_name,
                                                          tensor_trace_order)
            if on_tpu:
              return tpu_replication.outside_compilation(
                  tensor_trace_fn, tensor)
            else:
              return tensor_trace_fn(tensor)

          if len(processed_tensors) != 1:
            raise RuntimeError('Multiple stats are only allowed in compact '
                               'mode.')
          processed_out_tensor = next(iter(processed_tensors.values()))
          trace_op = tpu_wrap_trace_fn(processed_out_tensor, tensor_name)

        if op_control_flow_context:
          graph._set_control_flow_context(self._outmost_context)
        if trace_op:
          if is_a_fetched_tensor:
            tracing_ops.append(trace_op)
            continue
          for consumer_op in consumers:
            consumer_op._add_control_input(trace_op)

    graph._set_control_flow_context(self._outmost_context)
    if tracing_ops:
      processed_t_fetches = control_flow_ops.tuple(processed_t_fetches,
                                                   control_inputs=tracing_ops)
    if self._use_tensor_values_cache() or self._use_tensor_buffer():
      if self._use_temp_cache():
        graph_cache_var = self._cache_variable_for_graph(graph)
        if graph not in self._temp_cache_var:
          raise RuntimeError('graph is not in self._temp_cache_var')
        graph_cache_var[_TT_SUMMARY_TAG] = array_ops_stack.stack(
            self._temp_cache_var[graph], axis=0, name='stack_all_op_signatures')
      if self._create_host_call():
        self._prepare_host_call_fn(processed_t_fetches, op_fetches, graph,
                                   graph_summary_tag)
        if not on_tpu:
          write_cache, caches_to_write = self._host_call_fn[_TT_HOSTCALL_KEY]
          cache_write_op = write_cache(**caches_to_write)
          processed_t_fetches = control_flow_ops.tuple(
              processed_t_fetches, control_inputs=[cache_write_op])
          del self._host_call_fn[_TT_HOSTCALL_KEY]
        elif self._parameters.flush_summaries_with_outside_compile:
          write_cache, caches_to_write = self._host_call_fn[_TT_HOSTCALL_KEY]
          if (_TT_SUMMARY_TAG in caches_to_write and 'step' in caches_to_write):
            step = caches_to_write['step']
            tensor_tracer_summary = caches_to_write[_TT_SUMMARY_TAG]
            tt_core_summary = self.merge_caches_on_tpu(tensor_tracer_summary[0])
            if not self._parameters.collect_summary_per_core:
              tt_core_summary = self.aggregate_global_cache(tt_core_summary)

            def write_if_core_0(step, replica_id, tt_summary):

              return cond.cond(
                  math_ops.equal(replica_id, 0),
                                      tensor_tracer_summary=tt_summary),
                  control_flow_ops.no_op)

            write_op = tpu_replication.outside_compilation(
                write_if_core_0,
                step=step,
                replica_id=self._replica_id,
                tt_summary=tt_core_summary)
            processed_t_fetches = control_flow_ops.tuple(
                processed_t_fetches, control_inputs=[write_op])
            del self._host_call_fn[_TT_HOSTCALL_KEY]
          else:
            raise ValueError('Outside compiled flush in only supported for '
                             'summary mode')
      else:
        processed_t_fetches = self._flush_tensor_values_cache(
            processed_t_fetches, op_fetches, on_tpu=on_tpu,
            tensor_trace_order=tensor_trace_order,
            graph=graph)

    return self._convert_fetches_to_input_format(tensor_fetches,
                                                 processed_t_fetches)

  def trace_tpu(self, graph,
                tensor_fetches,
                op_fetches=None,
                num_replicas=None,
                num_replicas_per_host=None,
                num_hosts=None):
    if isinstance(graph, func_graph.FuncGraph) or isinstance(
      logging.warning('Tensor Tracer is not supported for tracing FuncGraphs. '
                      'Ignoring tracing.')
      return tensor_fetches

    if graph in TensorTracer._traced_graphs:
      logging.warning('Graph is already rewritten with tensor tracer, ignoring '
                      'multiple calls.')
      return tensor_fetches
    else:
      TensorTracer._traced_graphs.add(graph)
    self._parameters = tensor_tracer_flags.TTParameters()
    self._tt_config.device_type = _DEVICE_TYPE_TPU
    self._tt_config.num_replicas = num_replicas
    self._tt_config.num_replicas_per_host = num_replicas_per_host
    self._tt_config.num_hosts = num_hosts
    if self._tt_config.num_replicas is not None:
      if self._tt_config.num_replicas_per_host is None:
        self._tt_config.num_replicas_per_host = 8
      if self._tt_config.num_hosts is None:
        self._tt_config.num_hosts = (
            num_replicas // self._tt_config.num_replicas_per_host +
            (num_replicas % self._tt_config.num_replicas_per_host > 0))

    if self._parameters.graph_dump_path:
      graph_io.write_graph(graph, self._parameters.graph_dump_path,
                           'graph_before_tt.pbtxt')
    with graph.as_default():
      self._add_replica_id_to_graph()
      tensor_fetches = self._trace_execution(graph, tensor_fetches, op_fetches,
                                             on_tpu=True)
    if self._parameters.graph_dump_path:
      graph_io.write_graph(graph, self._parameters.graph_dump_path,
                           'graph_after_tt.pbtxt')
    return tensor_fetches

  def trace_cpu(self, graph, tensor_fetches, op_fetches=None):
    if isinstance(graph, func_graph.FuncGraph) or isinstance(
      logging.warning('Tensor Tracer is not supported for tracing FuncGraphs. '
                      'Ignoring tracing.')
      return tensor_fetches

    if graph in TensorTracer._traced_graphs:
      logging.warning('Graph is already rewritten with tensor tracer, ignoring '
                      'multiple calls.')
      return tensor_fetches
    else:
      TensorTracer._traced_graphs.add(graph)
    self._parameters = tensor_tracer_flags.TTParameters()

    self._tt_config.device_type = _DEVICE_TYPE_CPU
    self._tt_config.num_replicas = 1
    self._tt_config.num_replicas_per_host = 1
    self._tt_config.num_hosts = 1
    self._replica_id = 0
    if self._parameters.graph_dump_path:
      graph_io.write_graph(graph, self._parameters.graph_dump_path,
                           'graph_before_tt.pbtxt')
    with graph.as_default():
      tensor_fetches = self._trace_execution(graph, tensor_fetches, op_fetches,
                                             on_tpu=False)
    if self._parameters.graph_dump_path:
      graph_io.write_graph(graph, self._parameters.graph_dump_path,
                           'graph_after_tt.pbtxt')
    return tensor_fetches
","['BoolGauge', '_graph_summary_tag', 'RuntimeError', 'md5', 'update', 'repr', 'encode', 'hexdigest', 'set_parameters', 'items', 'op_priority', 'read_tensor_tracer_event_file', 'defaultdict', 'summary_iterator', 'HasField', 'len', 'ValueError', 'append', 'frombuffer', 'DType', 'as_numpy_dtype', 'reshape', 'trace_tensor', 'get_collection', 'add_to_collection', 'keras_layer_tracepoint', 'is_tf_type', 'set', 'is_enabled', 'TTParameters', 'get_cell', 'warning', 'check_device_type', 'check_trace_mode', 'loop_cond_op', 'while_loop_op', 'IsLoopSwitch', 'IsLoopMerge', 'IsLoopEnter', 'IsLoopExit', 'control_flow_op', 'IsSwitch', 'IsMerge', 'unsafe_op', 'device_mismatch', 'unsafe_scalar_trace', '_is_interesting_op', 'reason', '__init__', 'TensorTracerConfig', 'report_proto', 'report_proto_path', '_escape_namescopes', 'replace', '_cache_variable_for_graph', '_create_or_get_tensor_history_values_cache', 'int', 'as_default', 'name_scope', 'get_variable', 'constant_initializer', '_create_or_get_tensor_values_cache', '_add_replica_id_to_graph', 'control_dependencies', 'tpu_replicated_input', 'list', 'range', '_inside_op_range', '_is_user_included_op', 'match', '_is_user_excluded_op', '_signature_types', '_num_signature_dimensions', '_use_temp_cache', '_use_tensor_buffer', '_use_tensor_values_cache', '_merge_tensor_signatures', 'sorted', 'stack', '_save_tensor_value_to_tmp_cache', '_save_tensor_value_to_cache_op', 'constant', 'scatter_update', '_snapshot_tensor', 'as_list', 'assign', '_preprocess_traced_tensor', '_detect_nan_inf', 'reduce_any', 'logical_or', 'is_nan', 'is_inf', 'cond', '_compute_signature', 'cast', 'tf_op', 'get_shape', 'is_fully_defined', '_show_size', '_show_max', '_show_min', '_show_norm', '_show_sparsity', 'sparsity_fn', 'greater_equal', 'abs', 'zero_fraction', '_show_mean_and_variance', 'moments', '_show_max_abs', 'reduce_max', '_make_tensor_trace_fun', '_print_tensor', 'is_brief_mode', 'join', '_get_outfile_suffix', 'print_v2', 'shape', '_show_part_tensor', '_show_full_tensor', '_is_in_control_flow', 'IsInCond', '_is_in_outmost_while_loop', '_get_op_control_flow_context', 'GetContainingWhileContext', '_should_trace_in_control_flow', '_skip_op', 'instrument_op', 'info', '_skip_tensor', 'instrument_tensor', 'consumers', '_filter_execution_path_operations', 'isinstance', 'pop', 'extend', 'add', '_determine_and_instrument_traced_tensors', 'enumerate', '_check_trace_files', 'Exists', 'recursive_create_dir', '_create_temp_cache', '_determine_trace_and_create_report', 'sort_tensors_and_ops', 'TTReportHandle', 'TensorTraceOrder', 'create_report_proto', 'write_report_proto', 'create_report', '_create_host_call', '_inspect_summary_cache', '_inspect_tensor', 'greater', 'warn', 'reduce_sum', '_inspect_history_cache', 'assign_add', 'subtract', 'is_remote_path', 'get_appendable_file_encoding', '_generate_flush_cache_op', '_flush_fun', '_f', '_print_cache', '_eq', 'equal', 'case', 'value', 'merge_caches_on_tpu', 'aggregate_global_cache', 'outside_compilation', 'identity', 'get_or_create_global_step', '_flush_tensor_values_cache', 'tuple', '_process_tensor_fetches', '_process_op_fetches', '_convert_fetches_to_input_format', 'broadcast_to', 'all_to_all', 'get_signature_to_agg_fn_map', 'itemgetter', 'agg_fn', 'transpose', 'expand_dims', '_prepare_host_call_fn', '_write_cache', 'string_join', 'create_file_writer_v2', '__ne__', 'SummaryMetadata', 'PluginData', 'init', 'write', 'group', 'host_call_deps_and_fn', 'get_traced_op_names', '_trace_execution', '_cast_unsupported_dtypes', '__eq__', '_get_control_flow_context', 'track_usage', 'get_operations', 'sort', '_set_control_flow_context', 'keys', 'values', 'tpu_wrap_trace_fn', 'tensor_trace_fn', 'next', 'iter', '_add_control_input', 'write_cache', 'write_if_core_0', 'trace_tpu', 'write_graph', 'trace_cpu']"
"https://github.com/getsentry/sentry/blob/6f4ae8751f04fd27a12009e4f907223a2d600ce5/src/sentry/utils/mockdata/core.py
","from __future__ import annotations

import itertools
import random
import time
from datetime import datetime, timedelta, timezone
from hashlib import sha1
from random import randint
from typing import Any, Mapping, Optional
from uuid import uuid4

import click
from django.conf import settings
from django.db import IntegrityError, router, transaction
from django.db.models import F
from django.utils import timezone as django_timezone

from sentry import buffer, roles, tsdb
from sentry.constants import ObjectStatus
from sentry.event_manager import HashDiscarded
from sentry.incidents.logic import create_alert_rule, create_alert_rule_trigger, create_incident
from sentry.incidents.models import AlertRuleThresholdType, IncidentType
from sentry.models.activity import Activity
from sentry.models.broadcast import Broadcast
from sentry.models.commit import Commit
from sentry.models.commitauthor import CommitAuthor
from sentry.models.commitfilechange import CommitFileChange
from sentry.models.deploy import Deploy
from sentry.models.environment import Environment
from sentry.models.eventattachment import EventAttachment
from sentry.models.files.file import File
from sentry.models.group import Group
from sentry.models.grouprelease import GroupRelease
from sentry.models.grouptombstone import TOMBSTONE_FIELDS_FROM_GROUP, GroupTombstone
from sentry.models.organization import Organization
from sentry.models.organizationaccessrequest import OrganizationAccessRequest
from sentry.models.organizationmember import OrganizationMember
from sentry.models.project import Project
from sentry.models.release import Release
from sentry.models.releasecommit import ReleaseCommit
from sentry.models.releaseenvironment import ReleaseEnvironment
from sentry.models.releasefile import ReleaseFile
from sentry.models.releaseprojectenvironment import ReleaseProjectEnvironment
from sentry.models.repository import Repository
from sentry.models.team import Team
from sentry.models.user import User
from sentry.models.userreport import UserReport
from sentry.monitors.models import (
    CheckInStatus,
    Monitor,
    MonitorCheckIn,
    MonitorEnvironment,
    MonitorStatus,
    MonitorType,
)
from sentry.signals import mocks_loaded
from sentry.similarity import features
from sentry.tsdb.base import TSDBModel
from sentry.types.activity import ActivityType
from sentry.utils import loremipsum
from sentry.utils.hashlib import md5_text
from sentry.utils.samples import create_sample_event as _create_sample_event
from sentry.utils.samples import create_trace, generate_user, random_normal

PLATFORMS = itertools.cycle([\""ruby\"", \""php\"", \""python\"", \""java\"", \""javascript\""])

LEVELS = itertools.cycle([\""error\"", \""error\"", \""error\"", \""fatal\"", \""warning\""])

ENVIRONMENTS = itertools.cycle([\""production\"", \""production\"", \""staging\"", \""alpha\"", \""beta\"", \""\""])

MONITOR_NAMES = itertools.cycle(settings.CELERYBEAT_SCHEDULE.keys())

MONITOR_SCHEDULES = itertools.cycle([\""* * * * *\"", \""0 * * * *\"", \""0 0 * * *\""])



def make_sentence(words=None):
    if words is None:
        words = int(random.weibullvariate(8, 3))
    return \"" \"".join(random.choice(loremipsum.words) for _ in range(words))


def create_sample_event(*args, **kwargs):
    try:
        event = _create_sample_event(*args, **kwargs)
    except HashDiscarded as e:
        click.echo(f\""> Skipping Event: {e}\"")
    else:
        if event is not None:
            features.record([event])
            return event


def generate_commit_data(user):
    commits = []
    for i in range(random.randint(1, 20)):
        if i == 1:
            filename = \""raven/base.py\""
        else:
            filename = random.choice(loremipsum.words) + \"".js\""
        if random.randint(0, 5) == 1:
            author = (user.name, user.email)
        else:
            author = (
                f\""{random.choice(loremipsum.words)} {random.choice(loremipsum.words)}\
,""                f\""{random.choice(loremipsum.words)}@example.com\
,""            )

        commits.append(
            {
                \""key\"": sha1(uuid4().bytes).hexdigest(),
                \""message\"": f\""feat: Do something to {filename}\\n{make_sentence()}\
,""                \""author\"": author,
                \""files\"": [(filename, \""M\"")],
            }
        )
    return commits


def generate_tombstones(project, user):
    prev_group_id = 100000
    try:
        prev_group_id = (
            max(
                GroupTombstone.objects.order_by(\""-previous_group_id\"")[0].previous_group_id,
                prev_group_id,
            )
            + 1
        )
    except IndexError:
        pass

    for group in Group.objects.filter(project=project)[:5]:
        GroupTombstone.objects.create(
            previous_group_id=prev_group_id,
            actor_id=user.id,
            **{name: getattr(group, name) for name in TOMBSTONE_FIELDS_FROM_GROUP},
        )
        prev_group_id += 1


def create_system_time_series():
    now = datetime.utcnow().replace(tzinfo=timezone.utc)

    for _ in range(60):
        count = randint(1, 10)
        tsdb.backend.incr_multi(
            (
                (TSDBModel.internal, \""client-api.all-versions.responses.2xx\""),
                (TSDBModel.internal, \""client-api.all-versions.requests\""),
            ),
            now,
            int(count * 0.9),
        )
        tsdb.backend.incr_multi(
            ((TSDBModel.internal, \""client-api.all-versions.responses.4xx\""),),
            now,
            int(count * 0.05),
        )
        tsdb.backend.incr_multi(
            ((TSDBModel.internal, \""client-api.all-versions.responses.5xx\""),),
            now,
            int(count * 0.1),
        )
        now = now - timedelta(seconds=1)

    for _ in range(24 * 30):
        count = randint(100, 1000)
        tsdb.backend.incr_multi(
            (
                (TSDBModel.internal, \""client-api.all-versions.responses.2xx\""),
                (TSDBModel.internal, \""client-api.all-versions.requests\""),
            ),
            now,
            int(count * 4.9),
        )
        tsdb.backend.incr_multi(
            ((TSDBModel.internal, \""client-api.all-versions.responses.4xx\""),),
            now,
            int(count * 0.05),
        )
        tsdb.backend.incr_multi(
            ((TSDBModel.internal, \""client-api.all-versions.responses.5xx\""),),
            now,
            int(count * 0.1),
        )
        now = now - timedelta(hours=1)


def create_sample_time_series(event, release=None):
    if event is None:
        return

    group = event.group
    project = group.project
    key = project.key_set.all()[0]

    now = datetime.utcnow().replace(tzinfo=timezone.utc)

    environment = Environment.get_or_create(
        project=project, name=Environment.get_name_or_default(event.get_tag(\""environment\""))
    )

    if release:
        ReleaseEnvironment.get_or_create(
            project=project, release=release, environment=environment, datetime=now
        )

        grouprelease = GroupRelease.get_or_create(
            group=group, release=release, environment=environment, datetime=now
        )

    for _ in range(60):
        count = randint(1, 10)
        tsdb.backend.incr_multi(
            ((TSDBModel.project, project.id), (TSDBModel.group, group.id)),
            now,
            count,
            environment_id=environment.id,
        )
        tsdb.backend.incr_multi(
            (
                (TSDBModel.organization_total_received, project.organization_id),
                (TSDBModel.project_total_received, project.id),
                (TSDBModel.key_total_received, key.id),
            ),
            now,
            int(count * 1.1),
        )
        tsdb.backend.incr(
            TSDBModel.project_total_forwarded,
            project.id,
            now,
            int(count * 1.1),
        )
        tsdb.backend.incr_multi(
            (
                (TSDBModel.organization_total_rejected, project.organization_id),
                (TSDBModel.project_total_rejected, project.id),
                (TSDBModel.key_total_rejected, key.id),
            ),
            now,
            int(count * 0.1),
        )

        frequencies = [
            (TSDBModel.frequent_issues_by_project, {project.id: {group.id: count}}),
            (TSDBModel.frequent_environments_by_group, {group.id: {environment.id: count}}),
        ]
        if release:
            frequencies.append(
                (TSDBModel.frequent_releases_by_group, {group.id: {grouprelease.id: count}})
            )

        tsdb.backend.record_frequency_multi(frequencies, now)

        now = now - timedelta(seconds=1)

    for _ in range(24 * 30):
        count = randint(100, 1000)
        tsdb.backend.incr_multi(
            ((TSDBModel.project, group.project.id), (TSDBModel.group, group.id)),
            now,
            count,
            environment_id=environment.id,
        )
        tsdb.backend.incr_multi(
            (
                (TSDBModel.organization_total_received, project.organization_id),
                (TSDBModel.project_total_received, project.id),
                (TSDBModel.key_total_received, key.id),
            ),
            now,
            int(count * 1.1),
        )
        tsdb.backend.incr_multi(
            (
                (TSDBModel.organization_total_rejected, project.organization_id),
                (TSDBModel.project_total_rejected, project.id),
                (TSDBModel.key_total_rejected, key.id),
            ),
            now,
            int(count * 0.1),
        )

        frequencies = [
            (TSDBModel.frequent_issues_by_project, {project.id: {group.id: count}}),
            (TSDBModel.frequent_environments_by_group, {group.id: {environment.id: count}}),
        ]
        if release:
            frequencies.append(
                (TSDBModel.frequent_releases_by_group, {group.id: {grouprelease.id: count}})
            )

        tsdb.backend.record_frequency_multi(frequencies, now)

        now = now - timedelta(hours=1)


def get_superuser() -> User:
    try:
        user = User.objects.filter(is_superuser=True)[0]
        return user
    except IndexError:
        raise Exception(\""No superuser exists (run `make bootstrap`)\"")


def create_user() -> User:
    user, _ = User.objects.get_or_create(
        username=\""dummy@example.com\"", defaults={\""email\"": \""dummy@example.com\""}
    )
    user.set_password(\""dummy\"")
    user.save()

    return user


def create_broadcast() -> None:
    Broadcast.objects.create(
        title=\""Learn about Source Maps\
,""        message=\""Source maps are JSON files that contain information on how to map your transpiled source code back to their original source.\
,""    )


def get_organization() -> Organization:
    if settings.SENTRY_SINGLE_ORGANIZATION:
        org = Organization.get_default()
        click.echo(f\""Mocking org {org.name}\"")
    else:
        click.echo(\""Mocking org {}\"".format(\""Default\""))
        org, _ = Organization.objects.get_or_create(slug=\""default\"")

    return org


def create_owner(organization: Organization, user: User, role: Optional[str] = None) -> None:
    create_member(organization, user, roles.get_top_dog().id)


def create_member(
    organization: Organization, user: User, role: Optional[str] = None
) -> OrganizationMember:
    member, _ = OrganizationMember.objects.get_or_create(
        user_id=user.id, organization=organization, defaults={\""role\"": role}
    )

    return member


def create_access_request(member: OrganizationMember, team: Team) -> None:
    OrganizationAccessRequest.objects.create_or_update(member=member, team=team)


def generate_projects(organization: Organization) -> Mapping[str, Any]:
    mocks = (
        (\""Massive Dynamic\"", (\""Ludic Science\"",)),
        (\""Captain Planet\"", (\""Earth\"", \""Fire\"", \""Wind\"", \""Water\"", \""Heart\"")),
    )
    project_map = {}

    for team_name, project_names in mocks:
        click.echo(f\""> Mocking team {team_name}\"")
        team, _ = Team.objects.get_or_create(
            name=team_name, defaults={\""organization\"": organization}
        )

        for project_name in project_names:
            click.echo(f\""  > Mocking project {project_name}\"")
            project, _ = Project.objects.get_or_create(
                name=project_name,
                defaults={
                    \""organization\"": organization,
                    \""flags\"": Project.flags.has_releases,
                    \""first_event\"": django_timezone.now(),
                },
            )
            project_map[project_name] = project
            project.add_team(team)

    return project_map


def create_environment(project: Project) -> Environment:
    return Environment.get_or_create(project=project, name=next(ENVIRONMENTS))


def create_monitor(project: Project, environment: Environment) -> None:
    monitor, _ = Monitor.objects.get_or_create(
        name=next(MONITOR_NAMES),
        project_id=project.id,
        organization_id=project.organization_id,
        type=MonitorType.CRON_JOB,
        defaults={
            \""status\"": ObjectStatus.DISABLED,
            \""config\"": {\""schedule\"": next(MONITOR_SCHEDULES)},
        },
    )

    monitor_env, _ = MonitorEnvironment.objects.get_or_create(
        monitor=monitor,
        environment=environment,
        defaults={
            \""status\"": MonitorStatus.DISABLED,
            \""next_checkin\"": django_timezone.now() + timedelta(minutes=60),
            \""last_checkin\"": django_timezone.now(),
        },
    )

    MonitorCheckIn.objects.create(
        project_id=monitor.project_id,
        monitor=monitor,
        monitor_environment=monitor_env,
        status=CheckInStatus.OK if monitor_env.status == MonitorStatus.OK else CheckInStatus.ERROR,
    )


def create_release(project: Project) -> Release:
    with transaction.atomic(using=router.db_for_write(Release)):
        release, _ = Release.objects.get_or_create(
            version=sha1(uuid4().bytes).hexdigest(),
            organization_id=project.organization_id,
        )
        release.add_project(project)

    return release


def create_repository(organization: Organization) -> Repository:
    try:
        with transaction.atomic(using=router.db_for_write(Repository)):
            repo, _ = Repository.objects.get_or_create(
                organization_id=organization.id,
                provider=\""integrations:github\
,""                external_id=\""example/example\
,""                defaults={
                    \""name\"": \""Example Repo\
,""                    \""url\"": \""https://github.com/example/example\
,""                },
            )
        return repo
    except IntegrityError:
        repo = Repository.objects.get(
            organization_id=organization.id,
            provider=\""github\
,""            external_id=\""example/example\
,""            name=\""Example Repo\
,""        )
        repo.provider = \""integrations:github\""
        repo.save()

        return repo


def populate_release(
    project: Project,
    environment: Environment,
    repository: Repository,
    release: Release,
    user: User,
    commits: list[Mapping[str, Any]],
) -> None:
    authors = set()

    commit = None
    for commit_index, raw_commit in enumerate(commits):
        author = CommitAuthor.objects.get_or_create(
            organization_id=project.organization_id,
            email=raw_commit[\""author\""][1],
            defaults={\""name\"": raw_commit[\""author\""][0]},
        )[0]
        commit = Commit.objects.get_or_create(
            organization_id=project.organization_id,
            repository_id=repository.id,
            key=raw_commit[\""key\""],
            defaults={\""author\"": author, \""message\"": raw_commit[\""message\""]},
        )[0]
        authors.add(author)

        for file in raw_commit[\""files\""]:
            ReleaseFile.objects.get_or_create(
                organization_id=project.organization_id,
                release_id=release.id,
                name=file[0],
                file=File.objects.get_or_create(
                    name=file[0], type=\""release.file\"", checksum=\""abcde\"" * 8, size=13043
                )[0],
                defaults={\""organization_id\"": project.organization_id},
            )

            CommitFileChange.objects.get_or_create(
                organization_id=project.organization_id,
                commit=commit,
                filename=file[0],
                type=file[1],
            )

        ReleaseCommit.objects.get_or_create(
            organization_id=project.organization_id,
            release=release,
            commit=commit,
            order=commit_index,
        )

    Commit.objects.get_or_create(
        organization_id=project.organization_id,
        repository_id=repository.id,
        key=sha1(uuid4().bytes).hexdigest(),
        defaults={
            \""author\"": CommitAuthor.objects.get_or_create(
                organization_id=project.organization_id,
                email=user.email,
                defaults={\""name\"": user.name},
            )[0],
            \""message\"": \""feat: Do something to {}\\n{}\"".format(
                random.choice(loremipsum.words) + \"".js\"", make_sentence()
            ),
        },
    )[0]

    Activity.objects.create(
        type=ActivityType.RELEASE.value,
        project=project,
        ident=release.version,
        user_id=user.id,
        data={\""version\"": release.version},
    )

    deploy = Deploy.objects.create(
        organization_id=project.organization_id,
        release=release,
        environment_id=environment.id,
    )

    if commit:
        release.update(
            commit_count=len(commits),
            last_commit_id=commit.id,
            total_deploys=Deploy.objects.filter(release=release).count(),
            last_deploy_id=deploy.id,
            authors=[str(a.id) for a in authors],
        )

    ReleaseProjectEnvironment.objects.create_or_update(
        project=project,
        environment=environment,
        release=release,
        defaults={\""last_deploy_id\"": deploy.id},
    )

    Activity.objects.create(
        type=ActivityType.DEPLOY.value,
        project=project,
        ident=release.version,
        data={
            \""version\"": release.version,
            \""deploy_id\"": deploy.id,
            \""environment\"": environment.name,
        },
        datetime=deploy.date_finished,
    )


def generate_events(
    project: Project,
    release: Release,
    repository: Repository,
    user: User,
    num_events: int,
    extra_events: bool = False,
) -> list[Any]:
    generated_events = []

    event1 = event2 = event3 = event4 = event5 = None

    if extra_events:
        for _ in range(45):
            platform = next(PLATFORMS)

            create_sample_event(
                project=project,
                platform=platform,
                release=release.version,
                level=next(LEVELS),
                environment=next(ENVIRONMENTS),
                message=\""This is a mostly useless example %s exception\"" % platform,
                checksum=md5_text(platform + str(_)).hexdigest(),
                user=generate_user(),
            )

    for _ in range(num_events):
        event1 = create_sample_event(
            project=project,
            platform=\""python\
,""            release=release.version,
            environment=next(ENVIRONMENTS),
            user=generate_user(),
        )
        generated_events.append(event1)

        EventAttachment.objects.create(
            project_id=project.id,
            event_id=event1.event_id,
            name=\""example-logfile.txt\
,""            file_id=File.objects.get_or_create(
                name=\""example-logfile.txt\
,""                type=\""text/plain\
,""                checksum=\""abcde\"" * 8,
                size=13043,
            )[0].id,
        )

        event2 = create_sample_event(
            project=project,
            platform=\""javascript\
,""            release=release.version,
            environment=next(ENVIRONMENTS),
            sdk={\""name\"": \""raven-js\"", \""version\"": \""2.1.0\""},
            user=generate_user(),
        )
        generated_events.append(event2)

        event3 = create_sample_event(project, \""java\"")
        generated_events.append(event3)

        event4 = create_sample_event(
            project=project,
            platform=\""ruby\
,""            release=release.version,
            environment=next(ENVIRONMENTS),
            user=generate_user(),
        )
        generated_events.append(event4)

        event5 = create_sample_event(
            project=project,
            platform=\""cocoa\
,""            release=release.version,
            environment=next(ENVIRONMENTS),
            user=generate_user(),
        )
        generated_events.append(event5)

        create_sample_event(
            project=project,
            platform=\""php\
,""            release=release.version,
            environment=next(ENVIRONMENTS),
            message=LONG_MESSAGE,
            user=generate_user(),
        )

        create_sample_event(
            project=project,
            platform=\""cocoa\
,""            sample_name=\""react-native\
,""            release=release.version,
            environment=next(ENVIRONMENTS),
            user=generate_user(),
        )

        create_sample_event(
            project=project,
            platform=\""pii\
,""            release=release.version,
            environment=next(ENVIRONMENTS),
            user=generate_user(),
        )
    if event5:
        Commit.objects.get_or_create(
            organization_id=project.organization_id,
            repository_id=repository.id,
            key=sha1(uuid4().bytes).hexdigest(),
            defaults={
                \""author\"": CommitAuthor.objects.get_or_create(
                    organization_id=project.organization_id,
                    email=user.email,
                    defaults={\""name\"": user.name},
                )[0],
                \""message\"": f\""Ooops!\\nFixes {event5.group.qualified_short_id}\
,""            },
        )[0]

    create_sample_event(project=project, environment=next(ENVIRONMENTS), platform=\""csp\"")

    if event3:
        UserReport.objects.create(
            project_id=project.id,
            event_id=event3.event_id,
            group_id=event3.group.id,
            name=\""Jane Bloggs\
,""            email=\""jane@example.com\
,""            comments=make_sentence(),
        )

    return generated_events


def create_metric_alert_rule(organization: Organization, project: Project) -> None:
    alert_rule = create_alert_rule(
        organization,
        [project],
        \""My Alert Rule\
,""        \""level:error\
,""        \""count()\
,""        10,
        AlertRuleThresholdType.ABOVE,
        1,
    )
    create_alert_rule_trigger(alert_rule, \""critical\"", 10)
    create_incident(
        organization,
        type_=IncidentType.DETECTED,
        title=\""My Incident\
,""        date_started=datetime.utcnow().replace(tzinfo=timezone.utc),
        alert_rule=alert_rule,
        projects=[project],
    )


def create_mock_transactions(
    project_map, load_trends=False, load_performance_issues=False, slow=False
):
    backend_project = project_map[\""Earth\""]
    frontend_project = project_map[\""Fire\""]
    service_projects = [
        project_map[\""Wind\""],
        project_map[\""Water\""],
        project_map[\""Heart\""],
    ]
    for project in project_map.values():
        if not project.flags.has_transactions:
            project.update(flags=F(\""flags\"").bitor(Project.flags.has_transactions))

    timestamp = django_timezone.now()
    create_trace(
        slow,
        timestamp - timedelta(milliseconds=random_normal(4000, 250, 1000)),
        timestamp,
        generate_user(),
        uuid4().hex,
        None,
        {
            \""project\"": frontend_project,
            \""transaction\"": \""/plants/:plantId/\
,""            \""frontend\"": True,
            \""errors\"": 1,
            \""children\"": [
                {
                    \""project\"": backend_project,
                    \""transaction\"": \""/api/plants/\
,""                    \""children\"": [
                        {
                            \""project\"": service_projects[0],
                            \""transaction\"": \""/products/all/\
,""                            \""children\"": [],
                        },
                        {
                            \""project\"": service_projects[1],
                            \""transaction\"": \""/analytics/\
,""                            \""children\"": [],
                        },
                        {
                            \""project\"": service_projects[2],
                            \""transaction\"": \""tasks.create_invoice\
,""                            \""children\"": [
                                {
                                    \""project\"": service_projects[2],
                                    \""transaction\"": \""tasks.process_invoice\
,""                                    \""children\"": [
                                        {
                                            \""project\"": service_projects[2],
                                            \""transaction\"": \""tasks.process_invoice\
,""                                            \""children\"": [
                                                {
                                                    \""project\"": service_projects[2],
                                                    \""transaction\"": \""tasks.process_invoice\
,""                                                    \""children\"": [
                                                        {
                                                            \""project\"": service_projects[2],
                                                            \""transaction\"": \""tasks.process_invoice\
,""                                                            \""children\"": [],
                                                        },
                                                    ],
                                                },
                                            ],
                                        },
                                    ],
                                },
                            ],
                        },
                    ],
                },
            ],
        },
    )

    if load_trends:
        for day in range(14):
            for hour in range(24):
                timestamp = django_timezone.now() - timedelta(days=day, hours=hour)
                transaction_user = generate_user()
                trace_id = uuid4().hex

                frontend_span_id = uuid4().hex[:16]
                frontend_root_span_id = uuid4().hex[:16]
                frontend_duration = random_normal(2000 - 50 * day, 250, 1000)

                create_sample_event(
                    project=frontend_project,
                    platform=\""javascript-transaction\
,""                    transaction=\""/trends/:frontend/\
,""                    event_id=uuid4().hex,
                    user=transaction_user,
                    timestamp=timestamp,
                    start_timestamp=timestamp - timedelta(milliseconds=frontend_duration),
                    measurements={
                        \""fp\"": {\""value\"": random_normal(1250 - 50 * day, 200, 500)},
                        \""fcp\"": {\""value\"": random_normal(1250 - 50 * day, 200, 500)},
                        \""lcp\"": {\""value\"": random_normal(2800 - 50 * day, 400, 2000)},
                        \""fid\"": {\""value\"": random_normal(5 - 0.125 * day, 2, 1)},
                    },
                    parent_span_id=None,
                    span_id=frontend_root_span_id,
                    trace=trace_id,
                    spans=[
                        {
                            \""same_process_as_parent\"": True,
                            \""op\"": \""http\
,""                            \""description\"": \""GET /api/plants/?all_plants=1\
,""                            \""data\"": {
                                \""duration\"": random_normal(
                                    1 - 0.05 * day, 0.25, 0.01, frontend_duration / 1000
                                ),
                                \""offset\"": 0.02,
                            },
                            \""span_id\"": frontend_span_id,
                            \""trace_id\"": trace_id,
                        }
                    ],
                )
                if slow:
                    time.sleep(0.05)

                backend_duration = random_normal(1500 + 50 * day, 250, 500)

                create_sample_event(
                    project=backend_project,
                    platform=\""transaction\
,""                    transaction=\""/trends/backend/\
,""                    event_id=uuid4().hex,
                    user=transaction_user,
                    timestamp=timestamp,
                    start_timestamp=timestamp - timedelta(milliseconds=backend_duration),
                    trace=trace_id,
                    parent_span_id=frontend_root_span_id,
                    spans=[],
                )

                if slow:
                    time.sleep(0.05)

    if load_performance_issues:

        def load_n_plus_one_issue():
            trace_id = uuid4().hex
            transaction_user = generate_user()
            frontend_root_span_id = uuid4().hex[:16]

            n_plus_one_db_current_offset = timestamp
            n_plus_one_db_duration = timedelta(milliseconds=100)

            parent_span_id = uuid4().hex[:16]

            source_span = {
                \""timestamp\"": (timestamp + n_plus_one_db_duration).timestamp(),
                \""start_timestamp\"": (timestamp + timedelta(milliseconds=10)).timestamp(),
                \""description\"": \""SELECT `books_book`.`id`, `books_book`.`title`, `books_book`.`author_id` FROM `books_book` ORDER BY `books_book`.`id` DESC LIMIT 10\
,""                \""op\"": \""db\
,""                \""parent_span_id\"": parent_span_id,
                \""span_id\"": uuid4().hex[:16],
                \""hash\"": \""858fea692d4d93e8\
,""            }

            def make_repeating_span(duration):
                nonlocal timestamp
                nonlocal n_plus_one_db_current_offset
                nonlocal n_plus_one_db_duration
                n_plus_one_db_duration += timedelta(milliseconds=duration) + timedelta(
                    milliseconds=1
                )
                n_plus_one_db_current_offset = timestamp + n_plus_one_db_duration
                return {
                    \""timestamp\"": (
                        n_plus_one_db_current_offset + timedelta(milliseconds=duration)
                    ).timestamp(),
                    \""start_timestamp\"": (
                        n_plus_one_db_current_offset + timedelta(milliseconds=1)
                    ).timestamp(),
                    \""description\"": \""SELECT `books_author`.`id`, `books_author`.`name` FROM `books_author` WHERE `books_author`.`id` = %s LIMIT 21\
,""                    \""op\"": \""db\
,""                    \""span_id\"": uuid4().hex[:16],
                    \""parent_span_id\"": parent_span_id,
                    \""hash\"": \""63f1e89e6a073441\
,""                }

            repeating_spans = [make_repeating_span(200) for _ in range(10)]

            parent_span = {
                \""timestamp\"": (
                    timestamp + n_plus_one_db_duration + timedelta(milliseconds=200)
                ).timestamp(),
                \""start_timestamp\"": timestamp.timestamp(),
                \""description\"": \""new\
,""                \""op\"": \""django.view\
,""                \""parent_span_id\"": uuid4().hex[:16],
                \""span_id\"": parent_span_id,
                \""hash\"": \""0f43fb6f6e01ca52\
,""            }

            create_sample_event(
                project=backend_project,
                platform=\""transaction\
,""                transaction=\""/n_plus_one_db/backend/\
,""                event_id=uuid4().hex,
                user=transaction_user,
                timestamp=timestamp + n_plus_one_db_duration + timedelta(milliseconds=300),
                start_timestamp=timestamp,
                trace=trace_id,
                parent_span_id=frontend_root_span_id,
                spans=[
                    parent_span,
                    source_span,
                ]
                + repeating_spans,
            )

            time.sleep(1.0)

            create_sample_event(
                project=backend_project,
                platform=\""transaction\
,""                transaction=\""/file-io-main-thread/\
,""                event_id=uuid4().hex,
                user=transaction_user,
                timestamp=timestamp + timedelta(milliseconds=300),
                start_timestamp=timestamp,
                trace=trace_id,
                parent_span_id=frontend_root_span_id,
                spans=[
                    parent_span,
                    {
                        \""timestamp\"": (timestamp + timedelta(milliseconds=200)).timestamp(),
                        \""start_timestamp\"": timestamp.timestamp(),
                        \""description\"": \""1669031858711_file.txt (4.0 kB)\
,""                        \""op\"": \""file.write\
,""                        \""span_id\"": uuid4().hex[:16],
                        \""parent_span_id\"": parent_span_id,
                        \""status\"": \""ok\
,""                        \""data\"": {
                            \""blocked_ui_thread\"": True,
                            \""call_stack\"": [
                                {
                                    \""function\"": \""onClick\
,""                                    \""in_app\"": True,
                                    \""lineno\"": 2,
                                    \""module\"": \""io.sentry.samples.android.MainActivity$$ExternalSyntheticLambda6\
,""                                    \""native\"": False,
                                },
                                {
                                    \""filename\"": \""MainActivity.java\
,""                                    \""function\"": \""lambda$onCreate$5$io-sentry-samples-android-MainActivity\
,""                                    \""in_app\"": True,
                                    \""lineno\"": 93,
                                    \""module\"": \""io.sentry.samples.android.MainActivity\
,""                                    \""native\"": False,
                                },
                            ],
                            \""file.path\"": \""/data/user/0/io.sentry.samples.android/files/1669031858711_file.txt\
,""                            \""file.size\"": 4010,
                        },
                    },
                ],
            )

        def load_uncompressed_asset_issue():
            time.sleep(1.0)
            transaction_user = generate_user()
            trace_id = uuid4().hex
            parent_span_id = uuid4().hex[:16]

            parent_span = {
                \""timestamp\"": (timestamp + timedelta(milliseconds=300)).timestamp(),
                \""start_timestamp\"": timestamp.timestamp(),
                \""description\"": \""new\
,""                \""op\"": \""pageload\
,""                \""parent_span_id\"": uuid4().hex[:16],
                \""span_id\"": parent_span_id,
                \""hash\"": \""0f43fb6f6e01ca52\
,""            }

            spans = [
                {
                    \""timestamp\"": (timestamp + timedelta(milliseconds=1000)).timestamp(),
                    \""start_timestamp\"": (timestamp + timedelta(milliseconds=300)).timestamp(),
                    \""description\"": \""https://s1.sentry-cdn.com/_static/dist/sentry/entrypoints/app.js\
,""                    \""op\"": \""resource.script\
,""                    \""parent_span_id\"": parent_span_id,
                    \""span_id\"": uuid4().hex[:16],
                    \""hash\"": \""858fea692d4d93e9\
,""                    \""data\"": {
                        \""http.response_transfer_size\"": 1_000_000,
                        \""http.response_content_length\"": 1_000_000,
                        \""http.decoded_response_content_length\"": 1_000_000,
                    },
                },
            ]

            create_sample_event(
                project=backend_project,
                platform=\""transaction\
,""                transaction=\""/uncompressed-asset/\
,""                event_id=uuid4().hex,
                user=transaction_user,
                timestamp=timestamp + timedelta(milliseconds=300),
                start_timestamp=timestamp,
                trace=trace_id,
                parent_span_id=parent_span_id,
                spans=[parent_span] + spans,
            )

        def load_consecutive_db_issue():
            time.sleep(1.0)
            transaction_user = generate_user()
            trace_id = uuid4().hex
            parent_span_id = uuid4().hex[:16]

            parent_span = {
                \""timestamp\"": (timestamp + timedelta(milliseconds=300)).timestamp(),
                \""start_timestamp\"": timestamp.timestamp(),
                \""description\"": \""new\
,""                \""op\"": \""django.view\
,""                \""parent_span_id\"": uuid4().hex[:16],
                \""span_id\"": parent_span_id,
                \""hash\"": \""0f43fb6f6e01ca52\
,""            }

            spans = [
                {
                    \""timestamp\"": (timestamp + timedelta(milliseconds=1000)).timestamp(),
                    \""start_timestamp\"": (timestamp + timedelta(milliseconds=300)).timestamp(),
                    \""description\"": \""SELECT `customer`.`id` FROM `customers` WHERE `customer`.`name` = 'customerName'\
,""                    \""op\"": \""db\
,""                    \""parent_span_id\"": parent_span_id,
                    \""span_id\"": uuid4().hex[:16],
                    \""hash\"": \""858fea692d4d93e9\
,""                },
                {
                    \""timestamp\"": (timestamp + timedelta(milliseconds=2000)).timestamp(),
                    \""start_timestamp\"": (timestamp + timedelta(milliseconds=1000)).timestamp(),
                    \""description\"": \""SELECT COUNT(*) FROM `customers`\
,""                    \""op\"": \""db\
,""                    \""parent_span_id\"": parent_span_id,
                    \""span_id\"": uuid4().hex[:16],
                    \""hash\"": \""858fea692d4d93e7\
,""                },
                {
                    \""timestamp\"": (timestamp + timedelta(milliseconds=3000)).timestamp(),
                    \""start_timestamp\"": (timestamp + timedelta(milliseconds=2000)).timestamp(),
                    \""description\"": \""SELECT COUNT(*) FROM `items`\
,""                    \""op\"": \""db\
,""                    \""parent_span_id\"": parent_span_id,
                    \""span_id\"": uuid4().hex[:16],
                    \""hash\"": \""858fea692d4d93e6\
,""                },
            ]

            create_sample_event(
                project=backend_project,
                platform=\""transaction\
,""                transaction=\""/consecutive-db/\
,""                event_id=uuid4().hex,
                user=transaction_user,
                timestamp=timestamp + timedelta(milliseconds=300),
                start_timestamp=timestamp,
                trace=trace_id,
                parent_span_id=parent_span_id,
                spans=[parent_span] + spans,
            )

        def load_render_blocking_asset_issue():
            transaction_user = generate_user()
            trace_id = uuid4().hex
            parent_span_id = uuid4().hex[:16]

            spans = [
                {
                    \""timestamp\"": (timestamp + timedelta(milliseconds=1300)).timestamp(),
                    \""start_timestamp\"": (timestamp + timedelta(milliseconds=300)).timestamp(),
                    \""description\"": \""https://example.com/asset.js\
,""                    \""op\"": \""resource.script\
,""                    \""parent_span_id\"": parent_span_id,
                    \""span_id\"": uuid4().hex[:16],
                    \""hash\"": \""858fea692d4d93e8\
,""                    \""data\"": {\""http.response_content_length\"": 1000001},
                }
            ]

            create_sample_event(
                project=frontend_project,
                platform=\""transaction\
,""                transaction=\""/render-blocking-asset/\
,""                event_id=uuid4().hex,
                user=transaction_user,
                timestamp=timestamp + timedelta(milliseconds=300),
                start_timestamp=timestamp,
                trace=trace_id,
                parent_span_id=parent_span_id,
                spans=spans,
                measurements={
                    \""fcp\"": {\""value\"": 2500.0},
                },
            )

        def load_m_n_plus_one_issue():
            trace_id = uuid4().hex
            transaction_user = generate_user()

            parent_span_id = uuid4().hex[:16]
            duration = 200

            def make_repeating_span(i):
                nonlocal timestamp
                nonlocal duration
                start_timestamp = timestamp + timedelta(milliseconds=i * (duration + 1))
                end_timestamp = start_timestamp + timedelta(milliseconds=duration)
                op = \""http\"" if i % 2 == 0 else \""db\""
                description = \""GET /\"" if i % 2 == 0 else \""SELECT * FROM authors WHERE id = %s\""
                hash = \""63f1e89e6a073441\"" if i % 2 == 0 else \""a109ff3ef40f7fb3\""
                return {
                    \""timestamp\"": end_timestamp.timestamp(),
                    \""start_timestamp\"": start_timestamp.timestamp(),
                    \""description\"": description,
                    \""op\"": op,
                    \""span_id\"": uuid4().hex[:16],
                    \""parent_span_id\"": parent_span_id,
                    \""hash\"": hash,
                }

            span_count = 10
            repeating_spans = [make_repeating_span(i) for i in range(span_count)]

            parent_span = {
                \""timestamp\"": (
                    timestamp + timedelta(milliseconds=span_count * (duration + 1))
                ).timestamp(),
                \""start_timestamp\"": timestamp.timestamp(),
                \""description\"": \""execute\
,""                \""op\"": \""graphql.execute\
,""                \""parent_span_id\"": uuid4().hex[:16],
                \""span_id\"": parent_span_id,
                \""hash\"": \""0f43fb6f6e01ca52\
,""            }

            create_sample_event(
                project=backend_project,
                platform=\""transaction\
,""                transaction=\""/m_n_plus_one_db/backend/\
,""                event_id=uuid4().hex,
                user=transaction_user,
                timestamp=timestamp + timedelta(milliseconds=span_count * (duration + 1) + 100),
                start_timestamp=timestamp,
                trace=trace_id,
                spans=[parent_span] + repeating_spans,
            )

        def generate_performance_issues():
            load_n_plus_one_issue()
            load_consecutive_db_issue()
            load_uncompressed_asset_issue()
            load_render_blocking_asset_issue()
            load_m_n_plus_one_issue()

        generate_performance_issues()


def main(
    skip_default_setup=False,
    num_events=1,
    extra_events=False,
    load_trends=False,
    load_performance_issues=False,
    slow=False,
):
    owner = get_superuser()
    user = create_user()
    create_broadcast()

    organization = get_organization()
    create_owner(organization, owner)
    member = create_member(organization, user, role=roles.get_default().id)

    project_map = generate_projects(organization)
    if not skip_default_setup:
        for project in project_map.values():
            environment = create_environment(project)
            create_monitor(project, environment)
            create_access_request(member, project.teams.first())

            generate_tombstones(project, user)
            release = create_release(project)
            repo = create_repository(organization)
            raw_commits = generate_commit_data(user)
            populate_release(
                project=project,
                environment=environment,
                repository=repo,
                release=release,
                user=user,
                commits=raw_commits,
            )
            create_metric_alert_rule(organization, project)
            events = generate_events(
                project=project,
                release=release,
                repository=repo,
                user=user,
                num_events=num_events,
                extra_events=extra_events,
            )
            for event in events:
                create_sample_time_series(event, release=release)

            if hasattr(buffer, \""process_pending\""):
                click.echo(\""    > Processing pending buffers\"")
                buffer.process_pending()

            mocks_loaded.send(project=project, sender=__name__)

    create_mock_transactions(project_map, load_trends, load_performance_issues, slow)
    create_system_time_series()
","['cycle', 'keys', 'make_sentence', 'int', 'weibullvariate', 'join', 'choice', 'range', 'create_sample_event', '_create_sample_event', 'echo', 'record', 'generate_commit_data', 'randint', 'append', 'sha1', 'uuid4', 'hexdigest', 'generate_tombstones', 'max', 'order_by', 'filter', 'create', 'getattr', 'create_system_time_series', 'utcnow', 'replace', 'incr_multi', 'timedelta', 'create_sample_time_series', 'all', 'get_or_create', 'get_name_or_default', 'get_tag', 'incr', 'record_frequency_multi', 'get_superuser', 'Exception', 'create_user', 'set_password', 'save', 'create_broadcast', 'get_organization', 'get_default', 'format', 'create_owner', 'create_member', 'get_top_dog', 'create_access_request', 'create_or_update', 'generate_projects', 'now', 'add_team', 'create_environment', 'next', 'create_monitor', 'create_release', 'atomic', 'db_for_write', 'add_project', 'create_repository', 'get', 'populate_release', 'set', 'enumerate', 'add', 'update', 'len', 'count', 'str', 'generate_events', 'md5_text', 'generate_user', 'create_metric_alert_rule', 'create_alert_rule', 'create_alert_rule_trigger', 'create_incident', 'create_mock_transactions', 'values', 'F', 'bitor', 'create_trace', 'random_normal', 'sleep', 'load_n_plus_one_issue', 'timestamp', 'make_repeating_span', 'load_uncompressed_asset_issue', 'load_consecutive_db_issue', 'COUNT', 'load_render_blocking_asset_issue', 'load_m_n_plus_one_issue', 'generate_performance_issues', 'main', 'first', 'hasattr', 'process_pending', 'send']"
"https://github.com/langchain-ai/langchain/blob/b4312aac5c0567088353178fb70fdb356b372e12/libs/langchain/langchain/vectorstores/vectara.py
","from __future__ import annotations

import json
import logging
import os
from hashlib import md5
from typing import Any, Iterable, List, Optional, Tuple, Type

import requests

from langchain.pydantic_v1 import Field
from langchain.schema import Document
from langchain.schema.embeddings import Embeddings
from langchain.schema.vectorstore import VectorStore, VectorStoreRetriever

logger = logging.getLogger(__name__)


class Vectara(VectorStore):

    def __init__(
        self,
        vectara_customer_id: Optional[str] = None,
        vectara_corpus_id: Optional[str] = None,
        vectara_api_key: Optional[str] = None,
        vectara_api_timeout: int = 60,
        source: str = \""langchain\
,""    ):
        self._vectara_customer_id = vectara_customer_id or os.environ.get(
            \""VECTARA_CUSTOMER_ID\""
        )
        self._vectara_corpus_id = vectara_corpus_id or os.environ.get(
            \""VECTARA_CORPUS_ID\""
        )
        self._vectara_api_key = vectara_api_key or os.environ.get(\""VECTARA_API_KEY\"")
        if (
            self._vectara_customer_id is None
            or self._vectara_corpus_id is None
            or self._vectara_api_key is None
        ):
            logger.warning(
                \""Can't find Vectara credentials, customer_id or corpus_id in \""
                \""environment.\""
            )
        else:
            logger.debug(f\""Using corpus id {self._vectara_corpus_id}\"")
        self._source = source

        adapter = requests.adapters.HTTPAdapter(max_retries=3)
        self._session.mount(\""http://\"", adapter)
        self.vectara_api_timeout = vectara_api_timeout

    @property
    def embeddings(self) -> Optional[Embeddings]:
        return None

    def _get_post_headers(self) -> dict:
        return {
            \""x-api-key\"": self._vectara_api_key,
            \""customer-id\"": self._vectara_customer_id,
            \""Content-Type\"": \""application/json\
,""            \""X-Source\"": self._source,
        }

    def _delete_doc(self, doc_id: str) -> bool:
        body = {
            \""customer_id\"": self._vectara_customer_id,
            \""corpus_id\"": self._vectara_corpus_id,
            \""document_id\"": doc_id,
        }
        response = self._session.post(
            \""https://api.vectara.io/v1/delete-doc\
,""            data=json.dumps(body),
            verify=True,
            headers=self._get_post_headers(),
            timeout=self.vectara_api_timeout,
        )
        if response.status_code != 200:
            logger.error(
                f\""Delete request failed for doc_id = {doc_id} with status code \""
                f\""{response.status_code}, reason {response.reason}, text \""
                f\""{response.text}\""
            )
            return False
        return True

    def _index_doc(self, doc: dict) -> str:
        request: dict[str, Any] = {}
        request[\""customer_id\""] = self._vectara_customer_id
        request[\""corpus_id\""] = self._vectara_corpus_id
        request[\""document\""] = doc

        response = self._session.post(
            headers=self._get_post_headers(),
            url=\""https://api.vectara.io/v1/index\
,""            data=json.dumps(request),
            timeout=self.vectara_api_timeout,
            verify=True,
        )

        status_code = response.status_code

        result = response.json()
        status_str = result[\""status\""][\""code\""] if \""status\"" in result else None
        if status_code == 409 or status_str and (status_str == \""ALREADY_EXISTS\""):
            return \""E_ALREADY_EXISTS\""
        elif status_str and (status_str == \""FORBIDDEN\""):
            return \""E_NO_PERMISSIONS\""
        else:
            return \""E_SUCCEEDED\""

    def add_files(
        self,
        files_list: Iterable[str],
        metadatas: Optional[List[dict]] = None,
        **kwargs: Any,
    ) -> List[str]:
        doc_ids = []
        for inx, file in enumerate(files_list):
            if not os.path.exists(file):
                logger.error(f\""File {file} does not exist, skipping\"")
                continue
            md = metadatas[inx] if metadatas else {}
            files: dict = {
                \""file\"": (file, open(file, \""rb\"")),
                \""doc_metadata\"": json.dumps(md),
            }
            headers = self._get_post_headers()
            headers.pop(\""Content-Type\"")
            response = self._session.post(
                f\""https://api.vectara.io/upload?c={self._vectara_customer_id}&o={self._vectara_corpus_id}&d=True\
,""                files=files,
                verify=True,
                headers=headers,
                timeout=self.vectara_api_timeout,
            )

            if response.status_code == 409:
                doc_id = response.json()[\""document\""][\""documentId\""]
                logger.info(
                    f\""File {file} already exists on Vectara (doc_id={doc_id}), skipping\""
                )
            elif response.status_code == 200:
                doc_id = response.json()[\""document\""][\""documentId\""]
                doc_ids.append(doc_id)
            else:
                logger.info(f\""Error indexing file {file}: {response.json()}\"")

        return doc_ids

    def add_texts(
        self,
        texts: Iterable[str],
        metadatas: Optional[List[dict]] = None,
        doc_metadata: Optional[dict] = None,
        **kwargs: Any,
    ) -> List[str]:
        doc_hash = md5()
        for t in texts:
            doc_hash.update(t.encode())
        doc_id = doc_hash.hexdigest()
        if metadatas is None:
            metadatas = [{} for _ in texts]
        if doc_metadata:
            doc_metadata[\""source\""] = \""langchain\""
        else:
            doc_metadata = {\""source\"": \""langchain\""}
        doc = {
            \""document_id\"": doc_id,
            \""metadataJson\"": json.dumps(doc_metadata),
            \""section\"": [
                {\""text\"": text, \""metadataJson\"": json.dumps(md)}
                for text, md in zip(texts, metadatas)
            ],
        }

        success_str = self._index_doc(doc)
        if success_str == \""E_ALREADY_EXISTS\"":
            self._delete_doc(doc_id)
            self._index_doc(doc)
        elif success_str == \""E_NO_PERMISSIONS\"":
            print(
            )
        return [doc_id]

    def similarity_search_with_score(
        self,
        query: str,
        k: int = 5,
        lambda_val: float = 0.025,
        filter: Optional[str] = None,
        score_threshold: Optional[float] = None,
        n_sentence_context: int = 2,
        **kwargs: Any,
    ) -> List[Tuple[Document, float]]:
        data = json.dumps(
            {
                \""query\"": [
                    {
                        \""query\"": query,
                        \""start\"": 0,
                        \""num_results\"": k,
                        \""context_config\"": {
                            \""sentences_before\"": n_sentence_context,
                            \""sentences_after\"": n_sentence_context,
                        },
                        \""corpus_key\"": [
                            {
                                \""customer_id\"": self._vectara_customer_id,
                                \""corpus_id\"": self._vectara_corpus_id,
                                \""metadataFilter\"": filter,
                                \""lexical_interpolation_config\"": {\""lambda\"": lambda_val},
                            }
                        ],
                    }
                ]
            }
        )

        response = self._session.post(
            headers=self._get_post_headers(),
            url=\""https://api.vectara.io/v1/query\
,""            data=data,
            timeout=self.vectara_api_timeout,
        )

        if response.status_code != 200:
            logger.error(
                \""Query failed %s\
,""                f\""(code {response.status_code}, reason {response.reason}, details \""
                f\""{response.text})\
,""            )
            return []

        result = response.json()

        if score_threshold:
            responses = [
                r
                for r in result[\""responseSet\""][0][\""response\""]
                if r[\""score\""] > score_threshold
            ]
        else:
            responses = result[\""responseSet\""][0][\""response\""]
        documents = result[\""responseSet\""][0][\""document\""]

        metadatas = []
        for x in responses:
            md = {m[\""name\""]: m[\""value\""] for m in x[\""metadata\""]}
            doc_num = x[\""documentIndex\""]
            doc_md = {m[\""name\""]: m[\""value\""] for m in documents[doc_num][\""metadata\""]}
            md.update(doc_md)
            metadatas.append(md)

        docs_with_score = [
            (
                Document(
                    page_content=x[\""text\""],
                    metadata=md,
                ),
                x[\""score\""],
            )
            for x, md in zip(responses, metadatas)
        ]

        return docs_with_score

    def similarity_search(
        self,
        query: str,
        k: int = 5,
        lambda_val: float = 0.025,
        filter: Optional[str] = None,
        n_sentence_context: int = 2,
        **kwargs: Any,
    ) -> List[Document]:
        docs_and_scores = self.similarity_search_with_score(
            query,
            k=k,
            lambda_val=lambda_val,
            filter=filter,
            score_threshold=None,
            n_sentence_context=n_sentence_context,
            **kwargs,
        )
        return [doc for doc, _ in docs_and_scores]

    @classmethod
    def from_texts(
        cls: Type[Vectara],
        texts: List[str],
        embedding: Optional[Embeddings] = None,
        metadatas: Optional[List[dict]] = None,
        **kwargs: Any,
    ) -> Vectara:
        doc_metadata = kwargs.pop(\""doc_metadata\"", {})
        vectara = cls(**kwargs)
        vectara.add_texts(texts, metadatas, doc_metadata=doc_metadata, **kwargs)
        return vectara

    @classmethod
    def from_files(
        cls: Type[Vectara],
        files: List[str],
        embedding: Optional[Embeddings] = None,
        metadatas: Optional[List[dict]] = None,
        **kwargs: Any,
    ) -> Vectara:
        vectara = cls(**kwargs)
        vectara.add_files(files, metadatas)
        return vectara

    def as_retriever(self, **kwargs: Any) -> VectaraRetriever:
        tags = kwargs.pop(\""tags\"", None) or []
        tags.extend(self._get_retriever_tags())
        return VectaraRetriever(vectorstore=self, **kwargs, tags=tags)


class VectaraRetriever(VectorStoreRetriever):

    vectorstore: Vectara
    search_kwargs: dict = Field(
        default_factory=lambda: {
            \""lambda_val\"": 0.025,
            \""k\"": 5,
            \""filter\"": \""\
,""            \""n_sentence_context\"": \""2\
,""        }
    )

    def add_texts(
        self,
        texts: List[str],
        metadatas: Optional[List[dict]] = None,
        doc_metadata: Optional[dict] = None,
    ) -> None:
        self.vectorstore.add_texts(texts, metadatas, doc_metadata or {})
","['getLogger', 'Vectara', '__init__', 'get', 'warning', 'debug', 'HTTPAdapter', 'mount', 'embeddings', '_get_post_headers', '_delete_doc', 'post', 'dumps', 'error', '_index_doc', 'json', 'add_files', 'enumerate', 'exists', 'open', 'pop', 'info', 'append', 'add_texts', 'md5', 'update', 'encode', 'hexdigest', 'zip', 'similarity_search_with_score', 'Document', 'similarity_search', 'from_texts', 'cls', 'from_files', 'as_retriever', 'extend', '_get_retriever_tags', 'VectaraRetriever', 'Field']"
"https://github.com/codelucas/newspaper/blob/f622011177f6c2e95e48d6076561e21c016f08c3/newspaper/article.py
","tle__ = 'newspaper'
__author__ = 'Lucas Ou-Yang'
__license__ = 'MIT'
__copyright__ = 'Copyright 2014, Lucas Ou-Yang'

import logging
import copy
import os
import glob
from urllib.parse import urlparse

import requests

from . import images
from . import network
from . import nlp
from . import settings
from . import urls

from .cleaners import DocumentCleaner
from .configuration import Configuration
from .extractors import ContentExtractor
from .outputformatters import OutputFormatter
from .utils import (URLHelper, RawHelper, extend_config,
                    get_available_languages, extract_meta_refresh)
from .videos.extractors import VideoExtractor

log = logging.getLogger(__name__)


class ArticleDownloadState(object):
    NOT_STARTED = 0
    FAILED_RESPONSE = 1
    SUCCESS = 2


class ArticleException(Exception):
    pass


class Article(object):
    def __init__(self, url, title='', source_url='', config=None, **kwargs):
        if isinstance(title, Configuration) or \\
                isinstance(source_url, Configuration):
            raise ArticleException(
                'Configuration object being passed incorrectly as title or '
                'source_url! Please verify `Article`s __init__() fn.')

        self.config = config or Configuration()
        self.config = extend_config(self.config, kwargs)

        self.extractor = ContentExtractor(self.config)

        if source_url == '':
            scheme = urls.get_scheme(url)
            if scheme is None:
                scheme = 'http'
            source_url = scheme + '://' + urls.get_domain(url)

        if source_url is None or source_url == '':
            raise ArticleException('input url bad format')

        self.source_url = source_url

        self.url = urls.prepare_url(url, self.source_url)

        self.title = title

        self.top_img = self.top_image = ''

        self.meta_img = ''

        self.imgs = self.images = []

        self.movies = []

        self.text = ''

        self.keywords = []

        self.meta_keywords = []

        self.tags = set()

        self.authors = []

        self.publish_date = ''

        self.summary = ''

        self.html = ''

        self.article_html = ''

        self.is_parsed = False
        self.download_state = ArticleDownloadState.NOT_STARTED
        self.download_exception_msg = None

        self.meta_description = \""\""

        self.meta_lang = \""\""

        self.meta_favicon = \""\""

        self.meta_site_name = \""\""

        self.meta_data = {}

        self.canonical_link = \""\""

        self.top_node = None

        self.clean_top_node = None

        self.doc = None

        self.clean_doc = None

        self.additional_data = {}

    def build(self):
        self.download()
        self.parse()
        self.nlp()

    def _parse_scheme_file(self, path):
        try:
            with open(path, \""r\"") as fin:
                return fin.read()
        except OSError as e:
            self.download_state = ArticleDownloadState.FAILED_RESPONSE
            self.download_exception_msg = e.strerror
            return None

    def _parse_scheme_http(self):
        try:
            return network.get_html_2XX_only(self.url, self.config)
        except requests.exceptions.RequestException as e:
            self.download_state = ArticleDownloadState.FAILED_RESPONSE
            self.download_exception_msg = str(e)
            return None

    def download(self, input_html=None, title=None, recursion_counter=0):
        if input_html is None:
            parsed_url = urlparse(self.url)
            if parsed_url.scheme == \""file\"":
                html = self._parse_scheme_file(parsed_url.path)
            else:
                html = self._parse_scheme_http()
            if html is None:
                log.debug('Download failed on URL %s because of %s' %
                          (self.url, self.download_exception_msg))
                return
        else:
            html = input_html

        if self.config.follow_meta_refresh:
            meta_refresh_url = extract_meta_refresh(html)
            if meta_refresh_url and recursion_counter < 1:
                return self.download(
                    input_html=network.get_html(meta_refresh_url),
                    recursion_counter=recursion_counter + 1)

        self.set_html(html)
        self.set_title(title)

    def parse(self):
        self.throw_if_not_downloaded_verbose()

        self.doc = self.config.get_parser().fromstring(self.html)
        self.clean_doc = copy.deepcopy(self.doc)

        if self.doc is None:
            return

        parse_candidate = self.get_parse_candidate()

        document_cleaner = DocumentCleaner(self.config)
        output_formatter = OutputFormatter(self.config)

        title = self.extractor.get_title(self.clean_doc)
        self.set_title(title)

        authors = self.extractor.get_authors(self.clean_doc)
        self.set_authors(authors)

        meta_lang = self.extractor.get_meta_lang(self.clean_doc)
        self.set_meta_language(meta_lang)

        if self.config.use_meta_language:
            self.extractor.update_language(self.meta_lang)
            output_formatter.update_language(self.meta_lang)

        meta_favicon = self.extractor.get_favicon(self.clean_doc)
        self.set_meta_favicon(meta_favicon)

        meta_site_name = self.extractor.get_meta_site_name(self.clean_doc)
        self.set_meta_site_name(meta_site_name)

        meta_description = \\
            self.extractor.get_meta_description(self.clean_doc)
        self.set_meta_description(meta_description)

        canonical_link = self.extractor.get_canonical_link(
            self.url, self.clean_doc)
        self.set_canonical_link(canonical_link)

        tags = self.extractor.extract_tags(self.clean_doc)
        self.set_tags(tags)

        meta_keywords = self.extractor.get_meta_keywords(
            self.clean_doc)
        self.set_meta_keywords(meta_keywords)

        meta_data = self.extractor.get_meta_data(self.clean_doc)
        self.set_meta_data(meta_data)

        self.publish_date = self.extractor.get_publishing_date(
            self.url,
            self.clean_doc)

        self.doc = document_cleaner.clean(self.doc)

        self.top_node = self.extractor.calculate_best_node(self.doc)
        if self.top_node is not None:
            video_extractor = VideoExtractor(self.config, self.top_node)
            self.set_movies(video_extractor.get_videos())

            self.top_node = self.extractor.post_cleanup(self.top_node)
            self.clean_top_node = copy.deepcopy(self.top_node)

            text, article_html = output_formatter.get_formatted(
                self.top_node)
            self.set_article_html(article_html)
            self.set_text(text)

        self.fetch_images()

        self.is_parsed = True
        self.release_resources()

    def fetch_images(self):
        if self.clean_doc is not None:
            meta_img_url = self.extractor.get_meta_img_url(
                self.url, self.clean_doc)
            self.set_meta_img(meta_img_url)

            imgs = self.extractor.get_img_urls(self.url, self.clean_doc)
            if self.meta_img:
                imgs.add(self.meta_img)
            self.set_imgs(imgs)

        if self.clean_top_node is not None and not self.has_top_image():
            first_img = self.extractor.get_first_img_url(
                self.url, self.clean_top_node)
            if self.config.fetch_images:
                self.set_top_img(first_img)
            else:
                self.set_top_img_no_check(first_img)

        if not self.has_top_image() and self.config.fetch_images:
            self.set_reddit_top_img()

    def has_top_image(self):
        return self.top_img is not None and self.top_img != ''

    def is_valid_url(self):
        return urls.valid_url(self.url)

    def is_valid_body(self):
        if not self.is_parsed:
            raise ArticleException('must parse article before checking \\
                                    if it\\'s body is valid!')
        meta_type = self.extractor.get_meta_type(self.clean_doc)
        wordcount = self.text.split(' ')
        sentcount = self.text.split('.')

        if (meta_type == 'article' and len(wordcount) >
                (self.config.MIN_WORD_COUNT)):
            log.debug('%s verified for article and wc' % self.url)
            return True

        if not self.is_media_news() and not self.text:
            log.debug('%s caught for no media no text' % self.url)
            return False

        if self.title is None or len(self.title.split(' ')) < 2:
            log.debug('%s caught for bad title' % self.url)
            return False

        if len(wordcount) < self.config.MIN_WORD_COUNT:
            log.debug('%s caught for word cnt' % self.url)
            return False

        if len(sentcount) < self.config.MIN_SENT_COUNT:
            log.debug('%s caught for sent cnt' % self.url)
            return False

        if self.html is None or self.html == '':
            log.debug('%s caught for no html' % self.url)
            return False

        log.debug('%s verified for default true' % self.url)
        return True

    def is_media_news(self):
        safe_urls = ['/video', '/slide', '/gallery', '/powerpoint',
                     '/fashion', '/glamour', '/cloth']
        for s in safe_urls:
            if s in self.url:
                return True
        return False

    def nlp(self):
        self.throw_if_not_downloaded_verbose()
        self.throw_if_not_parsed_verbose()

        nlp.load_stopwords(self.config.get_language())
        text_keyws = list(nlp.keywords(self.text).keys())
        title_keyws = list(nlp.keywords(self.title).keys())
        keyws = list(set(title_keyws + text_keyws))
        self.set_keywords(keyws)

        max_sents = self.config.MAX_SUMMARY_SENT

        summary_sents = nlp.summarize(title=self.title, text=self.text, max_sents=max_sents)
        summary = '\\n'.join(summary_sents)
        self.set_summary(summary)

    def get_parse_candidate(self):
        if self.html:
            return RawHelper.get_parsing_candidate(self.url, self.html)
        return URLHelper.get_parsing_candidate(self.url)

    def build_resource_path(self):
        res_path = self.get_resource_path()
        if not os.path.exists(res_path):
            os.mkdir(res_path)

    def get_resource_path(self):
        res_dir_fn = 'article_resources'
        resource_directory = os.path.join(settings.TOP_DIRECTORY, res_dir_fn)
        if not os.path.exists(resource_directory):
            os.mkdir(resource_directory)
        dir_path = os.path.join(resource_directory, '%s_' % self.link_hash)
        return dir_path

    def release_resources(self):
        path = self.get_resource_path()
        for fname in glob.glob(path):
            try:
                os.remove(fname)
            except OSError:
                pass

    def set_reddit_top_img(self):
        try:
            s = images.Scraper(self)
            self.set_top_img(s.largest_image_url())
        except TypeError as e:
            if \""Can't convert 'NoneType' object to str implicitly\"" in e.args[0]:
                log.debug('No pictures found. Top image not set, %s' % e)
            elif 'timed out' in e.args[0]:
                log.debug('Download of picture timed out. Top image not set, %s' % e)
            else:
                log.critical('TypeError other than None type error. '
                             'Cannot set top image using the Reddit '
                             'algorithm. Possible error with PIL., %s' % e)
        except Exception as e:
            log.critical('Other error with setting top image using the '
                         'Reddit algorithm. Possible error with PIL, %s' % e)

    def set_title(self, input_title):
        if input_title:
            self.title = input_title[:self.config.MAX_TITLE]

    def set_text(self, text):
        text = text[:self.config.MAX_TEXT]
        if text:
            self.text = text

    def set_html(self, html):
        if html:
            if isinstance(html, bytes):
                html = self.config.get_parser().get_unicode_html(html)
            self.html = html
            self.download_state = ArticleDownloadState.SUCCESS

    def set_article_html(self, article_html):
        if article_html:
            self.article_html = article_html

    def set_meta_img(self, src_url):
        self.meta_img = src_url
        self.set_top_img_no_check(src_url)

    def set_top_img(self, src_url):
        if src_url is not None:
            s = images.Scraper(self)
            if s.satisfies_requirements(src_url):
                self.set_top_img_no_check(src_url)

    def set_top_img_no_check(self, src_url):
        self.top_img = src_url
        self.top_image = src_url

    def set_imgs(self, imgs):
        self.images = imgs
        self.imgs = imgs

    def set_keywords(self, keywords):
        if not isinstance(keywords, list):
            raise Exception(\""Keyword input must be list!\"")
        if keywords:
            self.keywords = keywords[:self.config.MAX_KEYWORDS]

    def set_authors(self, authors):
        if not isinstance(authors, list):
            raise Exception(\""authors input must be list!\"")
        if authors:
            self.authors = authors[:self.config.MAX_AUTHORS]

    def set_summary(self, summary):
        self.summary = summary[:self.config.MAX_SUMMARY]

    def set_meta_language(self, meta_lang):
        if meta_lang and len(meta_lang) >= 2 and \\
           meta_lang in get_available_languages():
            self.meta_lang = meta_lang[:2]

    def set_meta_keywords(self, meta_keywords):
        self.meta_keywords = [k.strip() for k in meta_keywords.split(',')]

    def set_meta_favicon(self, meta_favicon):
        self.meta_favicon = meta_favicon

    def set_meta_site_name(self, meta_site_name):
        self.meta_site_name = meta_site_name

    def set_meta_description(self, meta_description):
        self.meta_description = meta_description

    def set_meta_data(self, meta_data):
        self.meta_data = meta_data

    def set_canonical_link(self, canonical_link):
        self.canonical_link = canonical_link

    def set_tags(self, tags):
        self.tags = tags

    def set_movies(self, movie_objects):
        movie_urls = [o.src for o in movie_objects if o and o.src]
        self.movies = movie_urls

    def throw_if_not_downloaded_verbose(self):
        if self.download_state == ArticleDownloadState.NOT_STARTED:
            raise ArticleException('You must `download()` an article first!')
        elif self.download_state == ArticleDownloadState.FAILED_RESPONSE:
            raise ArticleException('Article `download()` failed with %s on URL %s' %
                  (self.download_exception_msg, self.url))

    def throw_if_not_parsed_verbose(self):
        if not self.is_parsed:
            raise ArticleException('You must `parse()` an article first!')
","['getLogger', 'ArticleDownloadState', 'ArticleException', 'Article', '__init__', 'isinstance', 'Configuration', 'extend_config', 'ContentExtractor', 'get_scheme', 'get_domain', 'prepare_url', 'set', 'build', 'download', 'parse', 'nlp', '_parse_scheme_file', 'open', 'read', '_parse_scheme_http', 'get_html_2XX_only', 'str', 'urlparse', 'debug', 'extract_meta_refresh', 'get_html', 'set_html', 'set_title', 'throw_if_not_downloaded_verbose', 'get_parser', 'fromstring', 'deepcopy', 'get_parse_candidate', 'DocumentCleaner', 'OutputFormatter', 'get_title', 'get_authors', 'set_authors', 'get_meta_lang', 'set_meta_language', 'update_language', 'get_favicon', 'set_meta_favicon', 'get_meta_site_name', 'set_meta_site_name', 'get_meta_description', 'set_meta_description', 'get_canonical_link', 'set_canonical_link', 'extract_tags', 'set_tags', 'get_meta_keywords', 'set_meta_keywords', 'get_meta_data', 'set_meta_data', 'get_publishing_date', 'clean', 'calculate_best_node', 'VideoExtractor', 'set_movies', 'get_videos', 'post_cleanup', 'get_formatted', 'set_article_html', 'set_text', 'fetch_images', 'release_resources', 'get_meta_img_url', 'set_meta_img', 'get_img_urls', 'add', 'set_imgs', 'has_top_image', 'get_first_img_url', 'set_top_img', 'set_top_img_no_check', 'set_reddit_top_img', 'is_valid_url', 'valid_url', 'is_valid_body', 'get_meta_type', 'split', 'len', 'is_media_news', 'throw_if_not_parsed_verbose', 'load_stopwords', 'get_language', 'list', 'keywords', 'keys', 'set_keywords', 'summarize', 'join', 'set_summary', 'get_parsing_candidate', 'build_resource_path', 'get_resource_path', 'exists', 'mkdir', 'glob', 'remove', 'Scraper', 'largest_image_url', 'critical', 'get_unicode_html', 'satisfies_requirements', 'Exception', 'get_available_languages', 'strip']"
"https://github.com/microsoft/autogen/blob/370ebf5e0042a8ea030474eda6f532b10d9e048f/autogen/code_utils.py
","import logging
import os
import pathlib
import re
import subprocess
import sys
import time
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from hashlib import md5
from typing import Callable, Dict, List, Optional, Tuple, Union

from autogen import oai

try:
    import docker
except ImportError:
    docker = None

DEFAULT_MODEL = \""gpt-4\""
FAST_MODEL = \""gpt-3.5-turbo\""
CODE_BLOCK_PATTERN = r\""```(\\w*)\\n(.*?)\\n```\""
WORKING_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), \""extensions\"")
UNKNOWN = \""unknown\""
TIMEOUT_MSG = \""Timeout\""
DEFAULT_TIMEOUT = 600
WIN32 = sys.platform == \""win32\""
PATH_SEPARATOR = WIN32 and \""\\\\\"" or \""/\""

logger = logging.getLogger(__name__)


def content_str(content: Union[str, List]) -> str:
    if type(content) is str:
        return content
    rst = \""\""
    for item in content:
        if item[\""type\""] == \""text\"":
            rst += item[\""text\""]
        else:
            assert isinstance(item, dict) and item[\""type\""] == \""image_url\"", \""Wrong content format.\""
            rst += \""<image>\""
    return rst


def infer_lang(code):
    if code.startswith(\""python \"") or code.startswith(\""pip\"") or code.startswith(\""python3 \""):
        return \""sh\""

    try:
        compile(code, \""test\"", \""exec\"")
        return \""python\""
    except SyntaxError:
        return UNKNOWN


def extract_code(
    text: Union[str, List], pattern: str = CODE_BLOCK_PATTERN, detect_single_line_code: bool = False
) -> List[Tuple[str, str]]:
    text = content_str(text)
    if not detect_single_line_code:
        match = re.findall(pattern, text, flags=re.DOTALL)
        return match if match else [(UNKNOWN, text)]

    code_pattern = re.compile(r\""`{3}(\\w+)?\\s*([\\s\\S]*?)`{3}|`([^`]+)`\"")
    code_blocks = code_pattern.findall(text)

    extracted = []
    for lang, group1, group2 in code_blocks:
        if group1:
            extracted.append((lang.strip(), group1.strip()))
        elif group2:
            extracted.append((\""\"", group2.strip()))

    return extracted


def generate_code(pattern: str = CODE_BLOCK_PATTERN, **config) -> Tuple[str, float]:
    response = oai.Completion.create(**config)
    return extract_code(oai.Completion.extract_text(response)[0], pattern), response[\""cost\""]


_IMPROVE_FUNCTION_CONFIG = {
    params = {**_IMPROVE_FUNCTION_CONFIG, **config}
    with open(file_name, \""r\"") as f:
        file_string = f.read()
    response = oai.Completion.create(
        {\""func_name\"": func_name, \""objective\"": objective, \""file_string\"": file_string}, **params
    )
    return oai.Completion.extract_text(response)[0], response[\""cost\""]


_IMPROVE_CODE_CONFIG = {

    Args:
        files (list): A list of file names containing the source code.
        objective (str): The objective to achieve.
        suggest_only (bool): Whether to return only the suggestions or the improved code.
        config (Optional, dict): The configuration for the API call.

    Returns:
        str: The improved code if suggest_only=False; a list of suggestions if suggest_only=True (default).
        float: The cost of the generation.
{file_string}

    This function is not tested on MacOS.

    Args:
        code (Optional, str): The code to execute.
            If None, the code from the file specified by filename will be executed.
            Either code or filename must be provided.
        timeout (Optional, int): The maximum execution time in seconds.
            If None, a default timeout will be used. The default timeout is 600 seconds. On Windows, the timeout is not enforced when use_docker=False.
        filename (Optional, str): The file name to save the code or where the code is stored when `code` is None.
            If None, a file with a randomly generated name will be created.
            The randomly generated file will be deleted after execution.
            The file name must be a relative path. Relative paths are relative to the working directory.
        work_dir (Optional, str): The working directory for the code execution.
            If None, a default working directory will be used.
            The default working directory is the \""extensions\"" directory under
            \""path_to_autogen\"".
        use_docker (Optional, list, str or bool): The docker image to use for code execution.
            If a list or a str of image name(s) is provided, the code will be executed in a docker container
            with the first image successfully pulled.
            If None, False or empty, the code will be executed in the current environment.
            Default is None, which will be converted into an empty list when docker package is available.
            Expected behaviour:
                - If `use_docker` is explicitly set to True and the docker package is available, the code will run in a Docker container.
                - If `use_docker` is explicitly set to True but the Docker package is missing, an error will be raised.
                - If `use_docker` is not set (i.e., left default to None) and the Docker package is not available, a warning will be displayed, but the code will run natively.
            If the code is executed in the current environment,
            the code must be trusted.
        lang (Optional, str): The language of the code. Default is \""python\"".

    Returns:
        int: 0 if the code executes successfully.
        str: The error message if the code fails to execute; the stdout otherwise.
        image: The docker image name after container run when docker is used.

func signature:
{definition}
assertions:\""\""\
,""    \""model\"": FAST_MODEL,
    \""max_tokens\"": 256,
    \""stop\"": \""\\n\\n\
,""}


def generate_assertions(definition: str, **config) -> Tuple[str, float]:
    params = {**_GENERATE_ASSERTIONS_CONFIG, **config}
    response = oai.Completion.create(
        {\""definition\"": definition},
        **params,
    )
    assertions = oai.Completion.extract_text(response)[0]
    return assertions, response[\""cost\""]


def _remove_check(response):
    pos = response.find(\""def check(\"")
    if pos == -1:
        return response
    return response[:pos]


def eval_function_completions(
    responses: List[str],
    definition: str,
    test: Optional[str] = None,
    entry_point: Optional[str] = None,
    assertions: Optional[Union[str, Callable[[str], Tuple[str, float]]]] = None,
    timeout: Optional[float] = 3,
    use_docker: Optional[bool] = True,
) -> Dict:
    n = len(responses)
    if assertions is None:
        success_list = []
        for i in range(n):
            response = _remove_check(responses[i])
            code = (
                f\""{response}\\n{test}\\ncheck({entry_point})\""
                if response.startswith(\""def\"")
                else f\""{definition}{response}\\n{test}\\ncheck({entry_point})\""
            )
            success = execute_code(code, timeout=timeout, use_docker=use_docker)[0] == 0
            success_list.append(success)
        return {
            \""expected_success\"": 1 - pow(1 - sum(success_list) / n, n),
            \""success\"": any(s for s in success_list),
        }
    if callable(assertions) and n > 1:
        assertions, gen_cost = assertions(definition)
    else:
        assertions, gen_cost = None, 0
    if n > 1 or test is None:
        for i in range(n):
            response = responses[i] = _remove_check(responses[i])
            code = (
                f\""{response}\\n{assertions}\"" if response.startswith(\""def\"") else f\""{definition}{response}\\n{assertions}\""
            )
            succeed_assertions = execute_code(code, timeout=timeout, use_docker=use_docker)[0] == 0
            if succeed_assertions:
                break
    else:
        succeed_assertions = False
        i, response = 0, responses[0]
    if test is None:
        return {
            \""index_selected\"": i,
            \""succeed_assertions\"": succeed_assertions,
            \""gen_cost\"": gen_cost,
            \""assertions\"": assertions,
        }
    code_test = (
        f\""{response}\\n{test}\\ncheck({entry_point})\""
        if response.startswith(\""def\"")
        else f\""{definition}{response}\\n{test}\\ncheck({entry_point})\""
    )
    success = execute_code(code_test, timeout=timeout, use_docker=use_docker)[0] == 0
    return {
        \""index_selected\"": i,
        \""succeed_assertions\"": succeed_assertions,
        \""success\"": success,
        \""gen_cost\"": gen_cost,
        \""assertions\"": assertions,
    }


_FUNC_COMPLETION_STOP = [\""\\nclass\"", \""\\ndef\"", \""\\nif\"", \""\\nprint\""]
_IMPLEMENT_CONFIGS = [
    {\""model\"": FAST_MODEL, \""prompt\"": _FUNC_COMPLETION_PROMPT, \""temperature\"": 0, \""cache_seed\"": 0},
    {\""model\"": FAST_MODEL, \""prompt\"": _FUNC_COMPLETION_PROMPT, \""stop\"": _FUNC_COMPLETION_STOP, \""n\"": 7, \""cache_seed\"": 0},
    {\""model\"": DEFAULT_MODEL, \""prompt\"": _FUNC_COMPLETION_PROMPT, \""temperature\"": 0, \""cache_seed\"": 1},
    {\""model\"": DEFAULT_MODEL, \""prompt\"": _FUNC_COMPLETION_PROMPT, \""stop\"": _FUNC_COMPLETION_STOP, \""n\"": 2, \""cache_seed\"": 2},
    {\""model\"": DEFAULT_MODEL, \""prompt\"": _FUNC_COMPLETION_PROMPT, \""stop\"": _FUNC_COMPLETION_STOP, \""n\"": 1, \""cache_seed\"": 2},
]


class PassAssertionFilter:
    def __init__(self, assertions):
        self._assertions = assertions
        self.cost = 0
        self.metrics = self.responses = None

    def pass_assertions(self, context, response, **_):
        responses = oai.Completion.extract_text(response)
        metrics = eval_function_completions(responses, context[\""definition\""], assertions=self._assertions)
        self._assertions = metrics[\""assertions\""]
        self.cost += metrics[\""gen_cost\""]
        self.metrics = metrics
        self.responses = responses
        return metrics[\""succeed_assertions\""]


def implement(
    definition: str,
    configs: Optional[List[Dict]] = None,
    assertions: Optional[Union[str, Callable[[str], Tuple[str, float]]]] = generate_assertions,
) -> Tuple[str, float]:
    cost = 0
    configs = configs or _IMPLEMENT_CONFIGS
    if len(configs) > 1 and callable(assertions):
        assertions, cost = assertions(definition)
    assertion_filter = PassAssertionFilter(assertions)
    response = oai.Completion.create(
        {\""definition\"": definition}, config_list=configs, filter_func=assertion_filter.pass_assertions
    )
    cost += assertion_filter.cost + response[\""cost\""]
    return assertion_filter.responses[assertion_filter.metrics[\""index_selected\""]], cost, response[\""config_id\""]

","['n', 'join', 'dirname', 'realpath', 'getLogger', 'content_str', 'type', 'isinstance', 'infer_lang', 'startswith', 'compile', 'extract_code', 'findall', 'append', 'strip', 'generate_code', 'create', 'extract_text', 'open', 'read', 'name', 'generate_assertions', '_remove_check', 'find', 'check', 'eval_function_completions', 'len', 'range', 'ncheck', 'execute_code', 'pow', 'sum', 'any', 'callable', 'assertions', '__init__', 'pass_assertions', 'implement', 'PassAssertionFilter']"
"https://github.com/getredash/redash/blob/8bfc57430dfb627e83856dfb1cddf69de3658572/redash/utils/__init__.py
","import binascii
import codecs
import csv
import datetime
import decimal
import hashlib
import io
import os
import random
import re
import uuid

import pystache
import pytz
import simplejson
import sqlparse
from flask import current_app
from funcy import select_values
from sqlalchemy.orm.query import Query

from redash import settings

from .human_time import parse_human_time

COMMENTS_REGEX = re.compile(r\""/\\*.*?\\*/\"")
WRITER_ENCODING = os.environ.get(\""REDASH_CSV_WRITER_ENCODING\"", \""utf-8\"")
WRITER_ERRORS = os.environ.get(\""REDASH_CSV_WRITER_ERRORS\"", \""strict\"")


def utcnow():
    return datetime.datetime.now(pytz.utc)


def dt_from_timestamp(timestamp, tz_aware=True):
    timestamp = datetime.datetime.utcfromtimestamp(float(timestamp))

    if tz_aware:
        timestamp = timestamp.replace(tzinfo=pytz.utc)

    return timestamp


def slugify(s):
    return re.sub(r\""[^a-z0-9_\\-]+\"", \""-\"", s.lower())


def gen_query_hash(sql):
    sql = COMMENTS_REGEX.sub(\""\"", sql)
    sql = \""\"".join(sql.split())
    return hashlib.md5(sql.encode(\""utf-8\"")).hexdigest()


def generate_token(length):
    chars = \""abcdefghijklmnopqrstuvwxyz\"" \""ABCDEFGHIJKLMNOPQRSTUVWXYZ\"" \""0123456789\""

    rand = random.SystemRandom()
    return \""\"".join(rand.choice(chars) for x in range(length))


class JSONEncoder(simplejson.JSONEncoder):

    def default(self, o):
        if isinstance(o, Query):
            result = list(o)
        elif isinstance(o, decimal.Decimal):
            result = float(o)
        elif isinstance(o, (datetime.timedelta, uuid.UUID)):
            result = str(o)
        elif isinstance(o, datetime.datetime):
            result = o.isoformat()
            if o.microsecond:
                result = result[:23] + result[26:]
            if result.endswith(\""+00:00\""):
                result = result[:-6] + \""Z\""
        elif isinstance(o, datetime.date):
            result = o.isoformat()
        elif isinstance(o, datetime.time):
            if o.utcoffset() is not None:
                raise ValueError(\""JSON can't represent timezone-aware times.\"")
            result = o.isoformat()
            if o.microsecond:
                result = result[:12]
        elif isinstance(o, memoryview):
            result = binascii.hexlify(o).decode()
        elif isinstance(o, bytes):
            result = binascii.hexlify(o).decode()
        else:
            result = super(JSONEncoder, self).default(o)
        return result


def json_loads(data, *args, **kwargs):
    return simplejson.loads(data, *args, **kwargs)


def json_dumps(data, *args, **kwargs):
    kwargs.setdefault(\""cls\"", JSONEncoder)
    kwargs.setdefault(\""encoding\"", None)
    kwargs.setdefault(\""ignore_nan\"", True)
    return simplejson.dumps(data, *args, **kwargs)


def mustache_render(template, context=None, **kwargs):
    renderer = pystache.Renderer(escape=lambda u: u)
    return renderer.render(template, context, **kwargs)


def mustache_render_escape(template, context=None, **kwargs):
    renderer = pystache.Renderer()
    return renderer.render(template, context, **kwargs)


def build_url(request, host, path):
    parts = request.host.split(\"":\"")
    if len(parts) > 1:
        port = parts[1]
        if (port, request.scheme) not in ((\""80\"", \""http\""), (\""443\"", \""https\"")):
            host = \""{}:{}\"".format(host, port)

    return \""{}://{}{}\"".format(request.scheme, host, path)


class UnicodeWriter:

    def __init__(self, f, dialect=csv.excel, encoding=WRITER_ENCODING, **kwds):
        self.queue = io.StringIO()
        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)
        self.stream = f
        self.encoder = codecs.getincrementalencoder(encoding)()

    def _encode_utf8(self, val):
        if isinstance(val, str):
            return val.encode(WRITER_ENCODING, WRITER_ERRORS)

        return val

    def writerow(self, row):
        self.writer.writerow([self._encode_utf8(s) for s in row])
        data = self.queue.getvalue()
        data = data.decode(WRITER_ENCODING)
        data = self.encoder.encode(data)
        self.stream.write(data)
        self.queue.truncate(0)

    def writerows(self, rows):
        for row in rows:
            self.writerow(row)


def collect_parameters_from_request(args):
    parameters = {}

    for k, v in args.items():
        if k.startswith(\""p_\""):
            parameters[k[2:]] = v

    return parameters


def base_url(org):
    if settings.MULTI_ORG:
        return \""https://{}/{}\"".format(settings.HOST, org.slug)

    return settings.HOST


def filter_none(d):
    return select_values(lambda v: v is not None, d)


def to_filename(s):
    s = re.sub(r'[<>:\""\\\\\\/|?*]+', \"" \"", s, flags=re.UNICODE)
    s = re.sub(r\""\\s+\"", \""_\"", s, flags=re.UNICODE)
    return s.strip(\""_\"")


def deprecated():
    def wrapper(K):
        setattr(K, \""deprecated\"", True)
        return K

    return wrapper


def render_template(path, context):
    return current_app.jinja_env.get_template(path).render(**context)
","['compile', 'get', 'utcnow', 'now', 'dt_from_timestamp', 'utcfromtimestamp', 'float', 'replace', 'slugify', 'sub', 'lower', 'gen_query_hash', 'join', 'split', 'md5', 'encode', 'hexdigest', 'generate_token', 'SystemRandom', 'choice', 'range', 'JSONEncoder', 'default', 'isinstance', 'list', 'str', 'isoformat', 'endswith', 'utcoffset', 'ValueError', 'hexlify', 'decode', 'super', 'json_loads', 'loads', 'json_dumps', 'setdefault', 'dumps', 'mustache_render', 'Renderer', 'render', 'mustache_render_escape', 'build_url', 'len', 'format', '__init__', 'StringIO', 'writer', 'getincrementalencoder', '_encode_utf8', 'writerow', 'getvalue', 'write', 'truncate', 'writerows', 'collect_parameters_from_request', 'items', 'startswith', 'base_url', 'filter_none', 'select_values', 'to_filename', 'strip', 'deprecated', 'wrapper', 'setattr', 'render_template', 'get_template']"
"https://github.com/miguelgrinberg/flasky/blob/f37dbad01acd963403429dd839f0ff43d49157af/app/models.py
","from datetime import datetime
import hashlib
from werkzeug.security import generate_password_hash, check_password_hash
from itsdangerous import TimedJSONWebSignatureSerializer as Serializer
from markdown import markdown
import bleach
from flask import current_app, request, url_for
from flask_login import UserMixin, AnonymousUserMixin
from app.exceptions import ValidationError
from . import db, login_manager


class Permission:
    FOLLOW = 1
    COMMENT = 2
    WRITE = 4
    MODERATE = 8
    ADMIN = 16


class Role(db.Model):
    __tablename__ = 'roles'
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(64), unique=True)
    default = db.Column(db.Boolean, default=False, index=True)
    permissions = db.Column(db.Integer)
    users = db.relationship('User', backref='role', lazy='dynamic')

    def __init__(self, **kwargs):
        super(Role, self).__init__(**kwargs)
        if self.permissions is None:
            self.permissions = 0

    @staticmethod
    def insert_roles():
        roles = {
            'User': [Permission.FOLLOW, Permission.COMMENT, Permission.WRITE],
            'Moderator': [Permission.FOLLOW, Permission.COMMENT,
                          Permission.WRITE, Permission.MODERATE],
            'Administrator': [Permission.FOLLOW, Permission.COMMENT,
                              Permission.WRITE, Permission.MODERATE,
                              Permission.ADMIN],
        }
        default_role = 'User'
        for r in roles:
            role = Role.query.filter_by(name=r).first()
            if role is None:
                role = Role(name=r)
            role.reset_permissions()
            for perm in roles[r]:
                role.add_permission(perm)
            role.default = (role.name == default_role)
            db.session.add(role)
        db.session.commit()

    def add_permission(self, perm):
        if not self.has_permission(perm):
            self.permissions += perm

    def remove_permission(self, perm):
        if self.has_permission(perm):
            self.permissions -= perm

    def reset_permissions(self):
        self.permissions = 0

    def has_permission(self, perm):
        return self.permissions & perm == perm

    def __repr__(self):
        return '<Role %r>' % self.name


class Follow(db.Model):
    __tablename__ = 'follows'
    follower_id = db.Column(db.Integer, db.ForeignKey('users.id'),
                            primary_key=True)
    followed_id = db.Column(db.Integer, db.ForeignKey('users.id'),
                            primary_key=True)
    timestamp = db.Column(db.DateTime, default=datetime.utcnow)


class User(UserMixin, db.Model):
    __tablename__ = 'users'
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(64), unique=True, index=True)
    username = db.Column(db.String(64), unique=True, index=True)
    role_id = db.Column(db.Integer, db.ForeignKey('roles.id'))
    password_hash = db.Column(db.String(128))
    confirmed = db.Column(db.Boolean, default=False)
    name = db.Column(db.String(64))
    location = db.Column(db.String(64))
    about_me = db.Column(db.Text())
    member_since = db.Column(db.DateTime(), default=datetime.utcnow)
    last_seen = db.Column(db.DateTime(), default=datetime.utcnow)
    avatar_hash = db.Column(db.String(32))
    posts = db.relationship('Post', backref='author', lazy='dynamic')
    followed = db.relationship('Follow',
                               foreign_keys=[Follow.follower_id],
                               backref=db.backref('follower', lazy='joined'),
                               lazy='dynamic',
                               cascade='all, delete-orphan')
    followers = db.relationship('Follow',
                                foreign_keys=[Follow.followed_id],
                                backref=db.backref('followed', lazy='joined'),
                                lazy='dynamic',
                                cascade='all, delete-orphan')
    comments = db.relationship('Comment', backref='author', lazy='dynamic')

    @staticmethod
    def add_self_follows():
        for user in User.query.all():
            if not user.is_following(user):
                user.follow(user)
                db.session.add(user)
                db.session.commit()

    def __init__(self, **kwargs):
        super(User, self).__init__(**kwargs)
        if self.role is None:
            if self.email == current_app.config['FLASKY_ADMIN']:
                self.role = Role.query.filter_by(name='Administrator').first()
            if self.role is None:
                self.role = Role.query.filter_by(default=True).first()
        if self.email is not None and self.avatar_hash is None:
            self.avatar_hash = self.gravatar_hash()
        self.follow(self)

    @property
    def password(self):
        raise AttributeError('password is not a readable attribute')

    @password.setter
    def password(self, password):
        self.password_hash = generate_password_hash(password)

    def verify_password(self, password):
        return check_password_hash(self.password_hash, password)

    def generate_confirmation_token(self, expiration=3600):
        s = Serializer(current_app.config['SECRET_KEY'], expiration)
        return s.dumps({'confirm': self.id}).decode('utf-8')

    def confirm(self, token):
        s = Serializer(current_app.config['SECRET_KEY'])
        try:
            data = s.loads(token.encode('utf-8'))
        except:
            return False
        if data.get('confirm') != self.id:
            return False
        self.confirmed = True
        db.session.add(self)
        return True

    def generate_reset_token(self, expiration=3600):
        s = Serializer(current_app.config['SECRET_KEY'], expiration)
        return s.dumps({'reset': self.id}).decode('utf-8')

    @staticmethod
    def reset_password(token, new_password):
        s = Serializer(current_app.config['SECRET_KEY'])
        try:
            data = s.loads(token.encode('utf-8'))
        except:
            return False
        user = User.query.get(data.get('reset'))
        if user is None:
            return False
        user.password = new_password
        db.session.add(user)
        return True

    def generate_email_change_token(self, new_email, expiration=3600):
        s = Serializer(current_app.config['SECRET_KEY'], expiration)
        return s.dumps(
            {'change_email': self.id, 'new_email': new_email}).decode('utf-8')

    def change_email(self, token):
        s = Serializer(current_app.config['SECRET_KEY'])
        try:
            data = s.loads(token.encode('utf-8'))
        except:
            return False
        if data.get('change_email') != self.id:
            return False
        new_email = data.get('new_email')
        if new_email is None:
            return False
        if self.query.filter_by(email=new_email).first() is not None:
            return False
        self.email = new_email
        self.avatar_hash = self.gravatar_hash()
        db.session.add(self)
        return True

    def can(self, perm):
        return self.role is not None and self.role.has_permission(perm)

    def is_administrator(self):
        return self.can(Permission.ADMIN)

    def ping(self):
        self.last_seen = datetime.utcnow()
        db.session.add(self)

    def gravatar_hash(self):
        return hashlib.md5(self.email.lower().encode('utf-8')).hexdigest()

    def gravatar(self, size=100, default='identicon', rating='g'):
        url = 'https://secure.gravatar.com/avatar'
        hash = self.avatar_hash or self.gravatar_hash()
        return '{url}/{hash}?s={size}&d={default}&r={rating}'.format(
            url=url, hash=hash, size=size, default=default, rating=rating)

    def follow(self, user):
        if not self.is_following(user):
            f = Follow(follower=self, followed=user)
            db.session.add(f)

    def unfollow(self, user):
        f = self.followed.filter_by(followed_id=user.id).first()
        if f:
            db.session.delete(f)

    def is_following(self, user):
        if user.id is None:
            return False
        return self.followed.filter_by(
            followed_id=user.id).first() is not None

    def is_followed_by(self, user):
        if user.id is None:
            return False
        return self.followers.filter_by(
            follower_id=user.id).first() is not None

    @property
    def followed_posts(self):
        return Post.query.join(Follow, Follow.followed_id == Post.author_id)\\
            .filter(Follow.follower_id == self.id)

    def to_json(self):
        json_user = {
            'url': url_for('api.get_user', id=self.id),
            'username': self.username,
            'member_since': self.member_since,
            'last_seen': self.last_seen,
            'posts_url': url_for('api.get_user_posts', id=self.id),
            'followed_posts_url': url_for('api.get_user_followed_posts',
                                          id=self.id),
            'post_count': self.posts.count()
        }
        return json_user

    def generate_auth_token(self, expiration):
        s = Serializer(current_app.config['SECRET_KEY'],
                       expires_in=expiration)
        return s.dumps({'id': self.id}).decode('utf-8')

    @staticmethod
    def verify_auth_token(token):
        s = Serializer(current_app.config['SECRET_KEY'])
        try:
            data = s.loads(token)
        except:
            return None
        return User.query.get(data['id'])

    def __repr__(self):
        return '<User %r>' % self.username


class AnonymousUser(AnonymousUserMixin):
    def can(self, permissions):
        return False

    def is_administrator(self):
        return False

login_manager.anonymous_user = AnonymousUser


@login_manager.user_loader
def load_user(user_id):
    return User.query.get(int(user_id))


class Post(db.Model):
    __tablename__ = 'posts'
    id = db.Column(db.Integer, primary_key=True)
    body = db.Column(db.Text)
    body_html = db.Column(db.Text)
    timestamp = db.Column(db.DateTime, index=True, default=datetime.utcnow)
    author_id = db.Column(db.Integer, db.ForeignKey('users.id'))
    comments = db.relationship('Comment', backref='post', lazy='dynamic')

    @staticmethod
    def on_changed_body(target, value, oldvalue, initiator):
        allowed_tags = ['a', 'abbr', 'acronym', 'b', 'blockquote', 'code',
                        'em', 'i', 'li', 'ol', 'pre', 'strong', 'ul',
                        'h1', 'h2', 'h3', 'p']
        target.body_html = bleach.linkify(bleach.clean(
            markdown(value, output_format='html'),
            tags=allowed_tags, strip=True))

    def to_json(self):
        json_post = {
            'url': url_for('api.get_post', id=self.id),
            'body': self.body,
            'body_html': self.body_html,
            'timestamp': self.timestamp,
            'author_url': url_for('api.get_user', id=self.author_id),
            'comments_url': url_for('api.get_post_comments', id=self.id),
            'comment_count': self.comments.count()
        }
        return json_post

    @staticmethod
    def from_json(json_post):
        body = json_post.get('body')
        if body is None or body == '':
            raise ValidationError('post does not have a body')
        return Post(body=body)


db.event.listen(Post.body, 'set', Post.on_changed_body)


class Comment(db.Model):
    __tablename__ = 'comments'
    id = db.Column(db.Integer, primary_key=True)
    body = db.Column(db.Text)
    body_html = db.Column(db.Text)
    timestamp = db.Column(db.DateTime, index=True, default=datetime.utcnow)
    disabled = db.Column(db.Boolean)
    author_id = db.Column(db.Integer, db.ForeignKey('users.id'))
    post_id = db.Column(db.Integer, db.ForeignKey('posts.id'))

    @staticmethod
    def on_changed_body(target, value, oldvalue, initiator):
        allowed_tags = ['a', 'abbr', 'acronym', 'b', 'code', 'em', 'i',
                        'strong']
        target.body_html = bleach.linkify(bleach.clean(
            markdown(value, output_format='html'),
            tags=allowed_tags, strip=True))

    def to_json(self):
        json_comment = {
            'url': url_for('api.get_comment', id=self.id),
            'post_url': url_for('api.get_post', id=self.post_id),
            'body': self.body,
            'body_html': self.body_html,
            'timestamp': self.timestamp,
            'author_url': url_for('api.get_user', id=self.author_id),
        }
        return json_comment

    @staticmethod
    def from_json(json_comment):
        body = json_comment.get('body')
        if body is None or body == '':
            raise ValidationError('comment does not have a body')
        return Comment(body=body)


db.event.listen(Comment.body, 'set', Comment.on_changed_body)
","['Role', 'Column', 'String', 'relationship', '__init__', 'super', 'insert_roles', 'filter_by', 'first', 'reset_permissions', 'add_permission', 'add', 'commit', 'has_permission', 'remove_permission', '__repr__', 'Follow', 'ForeignKey', 'User', 'Text', 'DateTime', 'backref', 'add_self_follows', 'all', 'is_following', 'follow', 'gravatar_hash', 'password', 'AttributeError', 'generate_password_hash', 'verify_password', 'check_password_hash', 'generate_confirmation_token', 'Serializer', 'dumps', 'decode', 'confirm', 'loads', 'encode', 'get', 'generate_reset_token', 'reset_password', 'generate_email_change_token', 'change_email', 'can', 'is_administrator', 'ping', 'utcnow', 'md5', 'lower', 'hexdigest', 'gravatar', 'format', 'unfollow', 'delete', 'is_followed_by', 'followed_posts', 'join', 'filter', 'to_json', 'url_for', 'count', 'generate_auth_token', 'verify_auth_token', 'AnonymousUser', 'load_user', 'int', 'Post', 'on_changed_body', 'linkify', 'clean', 'markdown', 'from_json', 'ValidationError', 'listen', 'Comment']"
"https://github.com/plotly/dash/blob/3adb9a93f6496c7987a25d3c9c314dcfd5d57426/dash/_utils.py
","rt shlex
import sys
import uuid
import hashlib
import collections
import subprocess
import logging
import io
import json
import secrets
import string
import inspect
from html import escape
from functools import wraps
from typing import Union
from dash.types import RendererHooks

logger = logging.getLogger()


def to_json(value):
    from plotly.io.json import to_json_plotly

    return to_json_plotly(value)


def interpolate_str(template, **data):
    s = template
    for k, v in data.items():
        key = \""{%\"" + k + \""%}\""
        s = s.replace(key, v)
    return s


def format_tag(
    tag_name, attributes, inner=\""\"", closed=False, opened=False, sanitize=False
):
    attributes = \"" \"".join(
        [f'{k}=\""{escape(v) if sanitize else v}\""' for k, v in attributes.items()]
    )
    tag = f\""<{tag_name} {attributes}\""
    if closed:
        tag += \""/>\""
    elif opened:
        tag += \"">\""
    else:
        tag += \"">\"" + inner + f\""</{tag_name}>\""
    return tag


def generate_hash():
    return str(uuid.uuid4().hex).strip(\""-\"")


def patch_collections_abc(member):
    return getattr(collections.abc, member)


class AttributeDict(dict):

    def __setattr__(self, key, value):
        self[key] = value

    def __getattr__(self, key):
        try:
            return self[key]
        except KeyError:
            pass
        raise AttributeError(key)

    def set_read_only(self, names, msg=\""Attribute is read-only\""):
        new_read_only = {name: msg for name in names}
        if getattr(self, \""_read_only\"", False):
            self._read_only.update(new_read_only)
        else:
            object.__setattr__(self, \""_read_only\"", new_read_only)

    def finalize(self, msg=\""Object is final: No new keys may be added.\""):
        object.__setattr__(self, \""_final\"", msg)

    def __setitem__(self, key, val):
        if key in self.__dict__.get(\""_read_only\"", {}):
            raise AttributeError(self._read_only[key], key)

        final_msg = self.__dict__.get(\""_final\"")
        if final_msg and key not in self:
            raise AttributeError(final_msg, key)

        return super().__setitem__(key, val)

    def update(self, other):
        for k, v in other.items():
            self[k] = v

    def first(self, *names):
        for name in names:
            value = self.get(name)
            if value:
                return value
        if not names:
            return next(iter(self), {})


def create_callback_id(output, inputs):
    hashed_inputs = None

    def _concat(x):
        nonlocal hashed_inputs
        _id = x.component_id_str().replace(\"".\"", \""\\\\.\"") + \"".\"" + x.component_property
        if x.allow_duplicate:
            if not hashed_inputs:
                hashed_inputs = hashlib.md5(
                    \"".\"".join(str(x) for x in inputs).encode(\""utf-8\"")
                ).hexdigest()
            _id += f\""@{hashed_inputs}\""
        return _id

    if isinstance(output, (list, tuple)):
        return \""..\"" + \""...\"".join(_concat(x) for x in output) + \""..\""

    return _concat(output)


def split_callback_id(callback_id):
    if callback_id.startswith(\""..\""):
        return [split_callback_id(oi) for oi in callback_id[2:-2].split(\""...\"")]

    id_, prop = callback_id.rsplit(\"".\"", 1)
    return {\""id\"": id_, \""property\"": prop}


def stringify_id(id_):
    if isinstance(id_, dict):
        return json.dumps(id_, sort_keys=True, separators=(\"",\"", \"":\""))
    return id_


def inputs_to_dict(inputs_list):
    inputs = AttributeDict()
    for i in inputs_list:
        inputsi = i if isinstance(i, list) else [i]
        for ii in inputsi:
            id_str = stringify_id(ii[\""id\""])
            inputs[f'{id_str}.{ii[\""property\""]}'] = ii.get(\""value\"")
    return inputs


def convert_to_AttributeDict(nested_list):
    new_dict = []
    for i in nested_list:
        if isinstance(i, dict):
            new_dict.append(AttributeDict(i))
        else:
            new_dict.append([AttributeDict(ii) for ii in i])
    return new_dict


def inputs_to_vals(inputs):
    return [
        [ii.get(\""value\"") for ii in i] if isinstance(i, list) else i.get(\""value\"")
        for i in inputs
    ]


def run_command_with_process(cmd):
    is_win = sys.platform == \""win32\""
    with subprocess.Popen(shlex.split(cmd, posix=is_win), shell=is_win) as proc:
        proc.wait()
        if proc.poll() is None:
            logger.warning(\""🚨 trying to terminate subprocess in safe way\"")
            try:
                proc.communicate()
                logger.exception(\""🚨 first try communicate failed\"")
                proc.kill()
                proc.communicate()


def compute_md5(path):
    with io.open(path, encoding=\""utf-8\"") as fp:
        return hashlib.md5(fp.read().encode(\""utf-8\"")).hexdigest()


def job(msg=\""\""):
    def wrapper(func):
        @wraps(func)
        def _wrapper(*args, **kwargs):
            logger.info(\""🏗️  [%s] 🏗️️  - %s\"", func.__name__, msg)
            res = func(*args, **kwargs)
            logger.info(\""::: 🍻🍻🍻 [%s] job done 🍻🍻🍻 :::\"", func.__name__)
            return res

        return _wrapper

    return wrapper


def gen_salt(chars):
    return \""\"".join(
        secrets.choice(string.ascii_letters + string.digits) for _ in range(chars)
    )


class OrderedSet(collections.abc.MutableSet):
    def __init__(self, *args):
        self._data = []
        for i in args:
            self.add(i)

    def add(self, value):
        if value not in self._data:
            self._data.append(value)

    def discard(self, value):
        self._data.remove(value)

    def __contains__(self, x):
        return x in self._data

    def __len__(self):
        return len(self._data)

    def __iter__(self):
        for i in self._data:
            yield i


def coerce_to_list(obj):
    if not isinstance(obj, (list, tuple)):
        return [obj]
    return obj


def clean_property_name(name: str):
    return name.split(\""@\"")[0]


def hooks_to_js_object(hooks: Union[RendererHooks, None]) -> str:
    if hooks is None:
        return \""\""
    hook_str = \"",\"".join(f\""{key}: {val}\"" for key, val in hooks.items())

    return f\""{{{hook_str}}}\""


def parse_version(version):
    return tuple(int(s) for s in version.split(\"".\""))


def get_caller_name():
    stack = inspect.stack()
    for s in stack:
        if s.function == \""<module>\"":
            return s.frame.f_locals.get(\""__name__\"", \""__main__\"")

    return \""__main__\""
","['getLogger', 'to_json', 'to_json_plotly', 'interpolate_str', 'items', 'replace', 'format_tag', 'join', 'escape', 'generate_hash', 'str', 'uuid4', 'strip', 'patch_collections_abc', 'getattr', 'AttributeDict', '__setattr__', '__getattr__', 'AttributeError', 'set_read_only', 'update', 'finalize', '__setitem__', 'get', 'super', 'first', 'next', 'iter', 'create_callback_id', '_concat', 'component_id_str', 'md5', 'encode', 'hexdigest', 'isinstance', 'split_callback_id', 'startswith', 'split', 'rsplit', 'stringify_id', 'dumps', 'inputs_to_dict', 'convert_to_AttributeDict', 'append', 'inputs_to_vals', 'run_command_with_process', 'Popen', 'wait', 'poll', 'warning', 'communicate', 'exception', 'kill', 'compute_md5', 'open', 'read', 'job', 'wrapper', 'wraps', '_wrapper', 'info', 'func', 'gen_salt', 'choice', 'range', 'OrderedSet', '__init__', 'add', 'discard', 'remove', '__contains__', '__len__', 'len', '__iter__', 'coerce_to_list', 'clean_property_name', 'hooks_to_js_object', 'parse_version', 'tuple', 'int', 'get_caller_name', 'stack']"
"https://github.com/streamlit/streamlit/blob/836ec497c1b6c99629eace3ae2eeb745cf93c049/lib/streamlit/elements/widgets/data_editor.py
","
from __future__ import annotations

import json
from dataclasses import dataclass
from decimal import Decimal
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    List,
    Mapping,
    Optional,
    Set,
    Tuple,
    TypeVar,
    Union,
    cast,
    overload,
)

import pandas as pd
import pyarrow as pa
from typing_extensions import Literal, TypeAlias, TypedDict

from streamlit import logger as _logger
from streamlit import type_util
from streamlit.deprecation_util import deprecate_func_name
from streamlit.elements.form import current_form_id
from streamlit.elements.lib.column_config_utils import (
    INDEX_IDENTIFIER,
    ColumnConfigMapping,
    ColumnConfigMappingInput,
    ColumnDataKind,
    DataframeSchema,
    apply_data_specific_configs,
    determine_dataframe_schema,
    is_type_compatible,
    marshall_column_config,
    process_config_mapping,
    update_column_config,
)
from streamlit.elements.lib.pandas_styler_utils import marshall_styler
from streamlit.elements.utils import check_callback_rules, check_session_state_rules
from streamlit.errors import StreamlitAPIException
from streamlit.proto.Arrow_pb2 import Arrow as ArrowProto
from streamlit.runtime.metrics_util import gather_metrics
from streamlit.runtime.scriptrunner import get_script_run_ctx
from streamlit.runtime.state import (
    WidgetArgs,
    WidgetCallback,
    WidgetKwargs,
    register_widget,
)
from streamlit.runtime.state.common import compute_widget_id
from streamlit.type_util import DataFormat, DataFrameGenericAlias, Key, is_type, to_key
from streamlit.util import calc_md5

if TYPE_CHECKING:
    import numpy as np
    from pandas.io.formats.style import Styler

    from streamlit.delta_generator import DeltaGenerator

_LOGGER = _logger.get_logger(\""root\"")

EditableData = TypeVar(
    \""EditableData\
,""    bound=Union[
        Tuple[Any],
        List[Any],
        Set[Any],
        Dict[str, Any],
    ],
)


DataTypes: TypeAlias = Union[
    pd.DataFrame,
    pd.Series,
    pd.Index,
    \""Styler\
,""    pa.Table,
    \""np.ndarray[Any, np.dtype[np.float64]]\
,""    Tuple[Any],
    List[Any],
    Set[Any],
    Dict[str, Any],
]


class EditingState(TypedDict, total=False):

    edited_rows: Dict[int, Dict[str, str | int | float | bool | None]]
    added_rows: List[Dict[str, str | int | float | bool | None]]
    deleted_rows: List[int]


@dataclass
class DataEditorSerde:

    def deserialize(self, ui_value: Optional[str], widget_id: str = \""\"") -> EditingState:
        data_editor_state: EditingState = (
            {
                \""edited_rows\"": {},
                \""added_rows\"": [],
                \""deleted_rows\"": [],
            }
            if ui_value is None
            else json.loads(ui_value)
        )

        if \""edited_rows\"" not in data_editor_state:
            data_editor_state[\""edited_rows\""] = {}

        if \""deleted_rows\"" not in data_editor_state:
            data_editor_state[\""deleted_rows\""] = []

        if \""added_rows\"" not in data_editor_state:
            data_editor_state[\""added_rows\""] = []

        data_editor_state[\""edited_rows\""] = {
            int(k): v for k, v in data_editor_state[\""edited_rows\""].items()
        }
        return data_editor_state

    def serialize(self, editing_state: EditingState) -> str:
        return json.dumps(editing_state, default=str)


def _parse_value(
    value: str | int | float | bool | None,
    column_data_kind: ColumnDataKind,
) -> Any:
    if value is None:
        return None

    try:
        if column_data_kind == ColumnDataKind.STRING:
            return str(value)

        if column_data_kind == ColumnDataKind.INTEGER:
            return int(value)

        if column_data_kind == ColumnDataKind.FLOAT:
            return float(value)

        if column_data_kind == ColumnDataKind.BOOLEAN:
            return bool(value)

        if column_data_kind == ColumnDataKind.DECIMAL:
            return Decimal(str(value))

        if column_data_kind == ColumnDataKind.TIMEDELTA:
            return pd.Timedelta(value)

        if column_data_kind in [
            ColumnDataKind.DATETIME,
            ColumnDataKind.DATE,
            ColumnDataKind.TIME,
        ]:
            datetime_value = pd.Timestamp(value)

            if datetime_value is pd.NaT:
                return None

            if column_data_kind == ColumnDataKind.DATETIME:
                return datetime_value

            if column_data_kind == ColumnDataKind.DATE:
                return datetime_value.date()

            if column_data_kind == ColumnDataKind.TIME:
                return datetime_value.time()

    except (ValueError, pd.errors.ParserError) as ex:
        _LOGGER.warning(
            \""Failed to parse value %s as %s. Exception: %s\"", value, column_data_kind, ex
        )
        return None
    return value


def _apply_cell_edits(
    df: pd.DataFrame,
    edited_rows: Mapping[int, Mapping[str, str | int | float | bool | None]],
    dataframe_schema: DataframeSchema,
) -> None:
    for row_id, row_changes in edited_rows.items():
        row_pos = int(row_id)
        for col_name, value in row_changes.items():
            if col_name == INDEX_IDENTIFIER:
                df.index.values[row_pos] = _parse_value(
                    value, dataframe_schema[INDEX_IDENTIFIER]
                )
            else:
                col_pos = df.columns.get_loc(col_name)
                df.iat[row_pos, col_pos] = _parse_value(
                    value, dataframe_schema[col_name]
                )


def _apply_row_additions(
    df: pd.DataFrame,
    added_rows: List[Dict[str, Any]],
    dataframe_schema: DataframeSchema,
) -> None:
    if not added_rows:
        return

    range_index_stop = None
    range_index_step = None
    if isinstance(df.index, pd.RangeIndex):
        range_index_stop = df.index.stop
        range_index_step = df.index.step

    for added_row in added_rows:
        index_value = None
        new_row: List[Any] = [None for _ in range(df.shape[1])]
        for col_name in added_row.keys():
            value = added_row[col_name]
            if col_name == INDEX_IDENTIFIER:
                index_value = _parse_value(value, dataframe_schema[INDEX_IDENTIFIER])
            else:
                col_pos = df.columns.get_loc(col_name)
                new_row[col_pos] = _parse_value(value, dataframe_schema[col_name])
        if range_index_stop is not None:
            df.loc[range_index_stop, :] = new_row
            range_index_stop += range_index_step
        elif index_value is not None:
            df.loc[index_value, :] = new_row


def _apply_row_deletions(df: pd.DataFrame, deleted_rows: List[int]) -> None:
    df.drop(df.index[deleted_rows], inplace=True)


def _apply_dataframe_edits(
    df: pd.DataFrame,
    data_editor_state: EditingState,
    dataframe_schema: DataframeSchema,
) -> None:
    if data_editor_state.get(\""edited_rows\""):
        _apply_cell_edits(df, data_editor_state[\""edited_rows\""], dataframe_schema)

    if data_editor_state.get(\""added_rows\""):
        _apply_row_additions(df, data_editor_state[\""added_rows\""], dataframe_schema)

    if data_editor_state.get(\""deleted_rows\""):
        _apply_row_deletions(df, data_editor_state[\""deleted_rows\""])


def _is_supported_index(df_index: pd.Index) -> bool:

    return (
        type(df_index)
        in [
            pd.RangeIndex,
            pd.Index,
            pd.DatetimeIndex,
        ]
        or is_type(df_index, \""pandas.core.indexes.numeric.Int64Index\"")
        or is_type(df_index, \""pandas.core.indexes.numeric.Float64Index\"")
        or is_type(df_index, \""pandas.core.indexes.numeric.UInt64Index\"")
    )


def _fix_column_headers(data_df: pd.DataFrame) -> None:

    if isinstance(data_df.columns, pd.MultiIndex):
        data_df.columns = [
            \""_\"".join(map(str, header)) for header in data_df.columns.to_flat_index()
        ]
    elif pd.api.types.infer_dtype(data_df.columns) != \""string\"":
        data_df.rename(
            columns={column: str(column) for column in data_df.columns},
            inplace=True,
        )


def _check_column_names(data_df: pd.DataFrame):

    if data_df.columns.empty:
        return

    duplicated_columns = data_df.columns[data_df.columns.duplicated()]
    if len(duplicated_columns) > 0:
        raise StreamlitAPIException(
            f\""All column names are required to be unique for usage with data editor. \""
            f\""The following column names are duplicated: {list(duplicated_columns)}. \""
            f\""Please rename the duplicated columns in the provided data.\""
        )

    if INDEX_IDENTIFIER in data_df.columns:
        raise StreamlitAPIException(
            f\""The column name '{INDEX_IDENTIFIER}' is reserved for the index column \""
            f\""and can't be used for data columns. Please rename the column in the \""
            f\""provided data.\""
        )


def _check_type_compatibilities(
    data_df: pd.DataFrame,
    columns_config: ColumnConfigMapping,
    dataframe_schema: DataframeSchema,
):
    indices = [(INDEX_IDENTIFIER, data_df.index)]

    for column in indices + list(data_df.items()):
        column_name, _ = column
        column_data_kind = dataframe_schema[column_name]

        if column_name in columns_config:
            column_config = columns_config[column_name]
            if column_config.get(\""disabled\"") is True:
                continue

            type_config = column_config.get(\""type_config\"")

            if type_config is None:
                continue

            configured_column_type = type_config.get(\""type\"")

            if configured_column_type is None:
                continue

            if is_type_compatible(configured_column_type, column_data_kind) is False:
                raise StreamlitAPIException(
                    f\""The configured column type `{configured_column_type}` for column \""
                    f\""`{column_name}` is not compatible for editing the underlying \""
                    f\""data type `{column_data_kind}`.\\n\\nYou have following options to \""
                    f\""fix this: 1) choose a compatible type 2) disable the column \""
                    f\""3) convert the column into a compatible data type.\""
                )


class DataEditorMixin:
    @overload
    def data_editor(
        self,
        data: EditableData,
        *,
        width: int | None = None,
        height: int | None = None,
        use_container_width: bool = False,
        hide_index: bool | None = None,
        column_order: Iterable[str] | None = None,
        column_config: ColumnConfigMappingInput | None = None,
        num_rows: Literal[\""fixed\"", \""dynamic\""] = \""fixed\
,""        disabled: bool | Iterable[str] = False,
        key: Key | None = None,
        on_change: WidgetCallback | None = None,
        args: WidgetArgs | None = None,
        kwargs: WidgetKwargs | None = None,
    ) -> EditableData:
        pass

    @overload
    def data_editor(
        self,
        data: Any,
        *,
        width: int | None = None,
        height: int | None = None,
        use_container_width: bool = False,
        hide_index: bool | None = None,
        column_order: Iterable[str] | None = None,
        column_config: ColumnConfigMappingInput | None = None,
        num_rows: Literal[\""fixed\"", \""dynamic\""] = \""fixed\
,""        disabled: bool | Iterable[str] = False,
        key: Key | None = None,
        on_change: WidgetCallback | None = None,
        args: WidgetArgs | None = None,
        kwargs: WidgetKwargs | None = None,
    ) -> pd.DataFrame:
        pass

    @gather_metrics(\""data_editor\"")
    def data_editor(
        self,
        data: DataTypes,
        *,
        width: int | None = None,
        height: int | None = None,
        use_container_width: bool = False,
        hide_index: bool | None = None,
        column_order: Iterable[str] | None = None,
        column_config: ColumnConfigMappingInput | None = None,
        num_rows: Literal[\""fixed\"", \""dynamic\""] = \""fixed\
,""        disabled: bool | Iterable[str] = False,
        key: Key | None = None,
        on_change: WidgetCallback | None = None,
        args: WidgetArgs | None = None,
        kwargs: WidgetKwargs | None = None,
    ) -> DataTypes:

        key = to_key(key)
        check_callback_rules(self.dg, on_change)
        check_session_state_rules(default_value=None, key=key, writes_allowed=False)

        if column_order is not None:
            column_order = list(column_order)

        column_config_mapping: ColumnConfigMapping = {}

        data_format = type_util.determine_data_format(data)
        if data_format == DataFormat.UNKNOWN:
            raise StreamlitAPIException(
                f\""The data type ({type(data).__name__}) or format is not supported by the data editor. \""
                \""Please convert your data into a Pandas Dataframe or another supported data format.\""
            )

        data_df = type_util.convert_anything_to_df(data, ensure_copy=True)

        if not _is_supported_index(data_df.index):
            raise StreamlitAPIException(
                f\""The type of the dataframe index - {type(data_df.index).__name__} - is not \""
                \""yet supported by the data editor.\""
            )

        _check_column_names(data_df)

        column_config_mapping = process_config_mapping(column_config)
        apply_data_specific_configs(
            column_config_mapping, data_df, data_format, check_arrow_compatibility=True
        )

        _fix_column_headers(data_df)
        if isinstance(data_df.index, pd.RangeIndex) and num_rows == \""dynamic\"":
            update_column_config(
                column_config_mapping, INDEX_IDENTIFIER, {\""hidden\"": True}
            )

        if hide_index is not None:
            update_column_config(
                column_config_mapping, INDEX_IDENTIFIER, {\""hidden\"": hide_index}
            )

        if not isinstance(disabled, bool):
            for column in disabled:
                update_column_config(column_config_mapping, column, {\""disabled\"": True})

        arrow_table = pa.Table.from_pandas(data_df)

        dataframe_schema = determine_dataframe_schema(data_df, arrow_table.schema)

        _check_type_compatibilities(data_df, column_config_mapping, dataframe_schema)

        arrow_bytes = type_util.pyarrow_table_to_bytes(arrow_table)

        ctx = get_script_run_ctx()
        id = compute_widget_id(
            \""data_editor\
,""            user_key=key,
            data=arrow_bytes,
            width=width,
            height=height,
            use_container_width=use_container_width,
            column_order=column_order,
            column_config_mapping=str(column_config_mapping),
            num_rows=num_rows,
            key=key,
            form_id=current_form_id(self.dg),
            page=ctx.page_script_hash if ctx else None,
        )

        proto = ArrowProto()
        proto.id = id

        proto.use_container_width = use_container_width

        if width:
            proto.width = width
        if height:
            proto.height = height

        if column_order:
            proto.column_order[:] = column_order

        proto.disabled = disabled is True

        proto.editing_mode = (
            ArrowProto.EditingMode.DYNAMIC
            if num_rows == \""dynamic\""
            else ArrowProto.EditingMode.FIXED
        )

        proto.form_id = current_form_id(self.dg)

        if type_util.is_pandas_styler(data):
            styler_uuid = calc_md5(key or self.dg._get_delta_path_str())[:10]
            data.set_uuid(styler_uuid)
            marshall_styler(proto, data, styler_uuid)

        proto.data = arrow_bytes

        marshall_column_config(proto, column_config_mapping)

        serde = DataEditorSerde()

        widget_state = register_widget(
            \""data_editor\
,""            proto,
            user_key=key,
            on_change_handler=on_change,
            args=args,
            kwargs=kwargs,
            deserializer=serde.deserialize,
            serializer=serde.serialize,
            ctx=ctx,
        )

        _apply_dataframe_edits(data_df, widget_state.value, dataframe_schema)
        self.dg._enqueue(\""arrow_data_frame\"", proto)
        return type_util.convert_df_to_data_format(data_df, data_format)

    @property
    def dg(self) -> \""DeltaGenerator\"":
        return cast(\""DeltaGenerator\"", self)

    experimental_data_editor = deprecate_func_name(
        gather_metrics(\""experimental_data_editor\"", data_editor),
        \""experimental_data_editor\
,""        \""2023-09
","['get_logger', 'TypeVar', 'EditingState', 'deserialize', 'loads', 'int', 'items', 'serialize', 'dumps', '_parse_value', 'str', 'float', 'bool', 'Decimal', 'Timedelta', 'Timestamp', 'date', 'time', 'warning', '_apply_cell_edits', 'get_loc', '_apply_row_additions', 'isinstance', 'range', 'keys', '_apply_row_deletions', 'drop', '_apply_dataframe_edits', 'get', '_is_supported_index', 'type', 'is_type', '_fix_column_headers', 'join', 'map', 'to_flat_index', 'infer_dtype', 'rename', '_check_column_names', 'duplicated', 'len', 'StreamlitAPIException', 'list', '_check_type_compatibilities', 'is_type_compatible', 'data_editor', 'gather_metrics', 'to_key', 'check_callback_rules', 'check_session_state_rules', 'determine_data_format', 'convert_anything_to_df', 'process_config_mapping', 'apply_data_specific_configs', 'update_column_config', 'from_pandas', 'determine_dataframe_schema', 'pyarrow_table_to_bytes', 'get_script_run_ctx', 'compute_widget_id', 'current_form_id', 'ArrowProto', 'is_pandas_styler', 'calc_md5', '_get_delta_path_str', 'set_uuid', 'marshall_styler', 'marshall_column_config', 'DataEditorSerde', 'register_widget', '_enqueue', 'convert_df_to_data_format', 'dg', 'cast', 'deprecate_func_name']"
"https://github.com/deepset-ai/haystack/blob/9b11462bf886ef0c39e282d5378503d4ede1a7ba/haystack/schema.py
","from __future__ import annotations

import ast
import csv
import hashlib
import inspect
import json
import logging
import time
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional, Union
from uuid import uuid4

import numpy as np
import pandas as pd
from numpy import ndarray
from pandas import DataFrame
from pydantic import BaseConfig, Field

from pydantic.dataclasses import dataclass
from pydantic.json import pydantic_encoder

from haystack.mmh3 import hash128

logger = logging.getLogger(__name__)


BaseConfig.arbitrary_types_allowed = True


ContentTypes = Literal[\""text\"", \""table\"", \""image\"", \""audio\""]
FilterType = Dict[str, Union[Dict[str, Any], List[Any], str, int, float, bool]]

LABEL_DATETIME_FORMAT: str = \""%Y-%m-%d %H:%M:%S\""


@dataclass
class Document:
    id: str
    content: Union[str, DataFrame]
    content_type: ContentTypes = Field(default=\""text\"")
    meta: Dict[str, Any] = Field(default={})
    id_hash_keys: List[str] = Field(default=[\""content\""])
    score: Optional[float] = None
    embedding: Optional[ndarray] = None

    def __init__(
        self,
        content: Union[str, DataFrame],
        content_type: ContentTypes = \""text\
,""        id: Optional[str] = None,
        score: Optional[float] = None,
        meta: Optional[Dict[str, Any]] = None,
        embedding: Optional[ndarray] = None,
        id_hash_keys: Optional[List[str]] = None,
    ):

        if content is None:
            raise ValueError(\""Can't create 'Document': Mandatory 'content' field is None\"")

        self.content = content
        self.content_type = content_type
        self.score = score
        self.meta = meta or {}

        allowed_hash_key_attributes = [\""content\"", \""content_type\"", \""score\"", \""meta\"", \""embedding\""]

        if id_hash_keys is not None and not all(
            key in allowed_hash_key_attributes or key.startswith(\""meta.\"") for key in id_hash_keys
        ):
            raise ValueError(
                f\""You passed custom strings {id_hash_keys} to id_hash_keys which is deprecated. Supply instead a \""
                f\""list of Document's attribute names (like {', '.join(allowed_hash_key_attributes)}) or \""
                f\""a key of meta with a maximum depth of 1 (like meta.url). \""
                \""See [Custom id hashing on documentstore level](https://github.com/deepset-ai/haystack/pull/1910) and \""
                \""[Allow more flexible Document id hashing](https://github.com/deepset-ai/haystack/issues/4317) for details\""
            )
        self.id_hash_keys = id_hash_keys or [\""content\""]

        if embedding is not None:
            embedding = np.asarray(embedding)
        self.embedding = embedding

        if id is not None:
            self.id: str = str(id)
        else:
            self.id: str = self._get_id(id_hash_keys=id_hash_keys)

    def _get_id(self, id_hash_keys: Optional[List[str]] = None):

        if id_hash_keys is None:
            return \""{:02x}\"".format(hash128(str(self.content)))

        final_hash_key = \""\""
        for attr in id_hash_keys:
            if attr.startswith(\""meta.\""):
                meta_key = attr.split(\"".\"", maxsplit=1)[1]
                if meta_key in self.meta:
                    final_hash_key += \"":\"" + str(self.meta[meta_key])
            else:
                final_hash_key += \"":\"" + str(getattr(self, attr))

        if final_hash_key == \""\"":
            raise ValueError(
                \""Can't create 'Document': 'id_hash_keys' must contain at least one of ['content', 'meta'] or be set to None.\""
            )

        return \""{:02x}\"".format(hash128(final_hash_key))

    def to_dict(self, field_map: Optional[Dict[str, Any]] = None) -> Dict:
        if not field_map:
            field_map = {}

        inv_field_map = {v: k for k, v in field_map.items()}
        _doc: Dict[str, str] = {}
        for k, v in self.__dict__.items():
            if k.startswith(\""__\""):
                continue
            if k == \""content\"" and self.content_type == \""table\"" and isinstance(self.content, DataFrame):
                v = dataframe_to_list(self.content)
            k = k if k not in inv_field_map else inv_field_map[k]
            _doc[k] = v
        return _doc

    @classmethod
    def from_dict(cls, dict: Dict[str, Any], field_map: Optional[Dict[str, Any]] = None) -> Document:
        if not field_map:
            field_map = {}

        _doc = dict.copy()
        init_args = [\""content\"", \""content_type\"", \""id\"", \""score\"", \""id_hash_keys\"", \""question\"", \""meta\"", \""embedding\""]
        if \""meta\"" not in _doc.keys():
            _doc[\""meta\""] = {}
        for k, v in _doc.items():
            if k.startswith(\""__\""):
                continue
            if k not in init_args and k not in field_map:
                _doc[\""meta\""][k] = v
        _new_doc = {}
        for k, v in _doc.items():
            if k in init_args:
                _new_doc[k] = v
            elif k in field_map:
                k = field_map[k]
                _new_doc[k] = v

        if _new_doc.get(\""content_type\"", None) == \""table\"" and isinstance(_new_doc[\""content\""], list):
            _new_doc[\""content\""] = dataframe_from_list(_new_doc[\""content\""])

        return cls(**_new_doc)

    def to_json(self, field_map: Optional[Dict[str, Any]] = None) -> str:
        if not field_map:
            field_map = {}
        dictionary = self.to_dict(field_map=field_map)
        return json.dumps(dictionary, cls=NumpyEncoder)

    @classmethod
    def from_json(cls, data: Union[str, Dict[str, Any]], field_map: Optional[Dict[str, Any]] = None) -> Document:
        if not field_map:
            field_map = {}
        if isinstance(data, str):
            dict_data = json.loads(data)
        else:
            dict_data = data
        return cls.from_dict(dict_data, field_map=field_map)

    def __eq__(self, other):
        content = getattr(other, \""content\"", None)
        if isinstance(content, pd.DataFrame):
            is_content_equal = content.equals(self.content)
        else:
            is_content_equal = content == self.content
        return (
            isinstance(other, self.__class__)
            and is_content_equal
            and getattr(other, \""content_type\"", None) == self.content_type
            and getattr(other, \""id\"", None) == self.id
            and getattr(other, \""id_hash_keys\"", None) == self.id_hash_keys
            and getattr(other, \""score\"", None) == self.score
            and getattr(other, \""meta\"", None) == self.meta
            and np.array_equal(getattr(other, \""embedding\"", None), self.embedding)
        )

    def __repr__(self):
        doc_dict = self.to_dict()
        embedding = doc_dict.get(\""embedding\"", None)
        if embedding is not None:
            doc_dict[\""embedding\""] = f\""<embedding of shape {getattr(embedding, 'shape', '[no shape]')}>\""
        return f\""<Document: {str(doc_dict)}>\""

    def __str__(self):
        if self.content is None:
            return f\""<Document: id={self.id}, content=None>\""
        return f\""<Document: id={self.id}, content='{self.content[:100]}{'...' if len(self.content) > 100 else ''}'>\""

    def __lt__(self, other):
        return self.score < other.score


@dataclass
class Span:
    start: int
    end: int

    def __contains__(self, value):
        if isinstance(value, Span):
            return self.start <= value.start and self.end > value.end
        try:
            value = float(value)
            return self.start <= value < self.end
        except Exception as e:
            raise ValueError(
                f\""Cannot use 'in' with a value of type {type(value)}. Use numeric values or Span objects.\""
            ) from e


@dataclass
class TableCell:
    row: int
    col: int


@dataclass
class Answer:
    answer: str
    type: Literal[\""generative\"", \""extractive\"", \""other\""] = \""extractive\""
    score: Optional[float] = None
    context: Optional[Union[str, DataFrame]] = None
    offsets_in_document: Optional[Union[List[Span], List[TableCell]]] = None
    offsets_in_context: Optional[Union[List[Span], List[TableCell]]] = None
    document_ids: Optional[List[str]] = None
    meta: Optional[Dict[str, Any]] = None


    def __post_init__(self):
        if self.offsets_in_document is not None:
            self.offsets_in_document = self._from_dict_offsets(self.offsets_in_document)

        if self.offsets_in_context is not None:
            self.offsets_in_context = self._from_dict_offsets(self.offsets_in_context)

        if self.meta is None:
            self.meta = {}

        if isinstance(self.context, list):
            self.context = dataframe_from_list(self.context)

    def __lt__(self, other):
        return self.score < other.score

    def __str__(self):
        if self.context is None:
            return f\""<Answer: answer='{self.answer}', score={self.score}, context=None>\""
        return f\""<Answer: answer='{self.answer}', score={self.score}, context='{self.context[:50]}{'...' if len(self.context) > 50 else ''}'>\""

    def __repr__(self):
        return f\""<Answer {self.to_dict()}>\""

    def to_dict(self) -> Dict:
        return asdict(self, dict_factory=_dict_factory)

    @classmethod
    def from_dict(cls, dict: Dict) -> Answer:
        if \""document_id\"" in dict:
            dict = dict.copy()
            document_id = dict.pop(\""document_id\"")
            dict[\""document_ids\""] = [document_id] if document_id is not None else None
        return cls(**dict)

    def to_json(self):
        return json.dumps(self.to_dict(), cls=NumpyEncoder)

    @classmethod
    def from_json(cls, data: Union[str, Dict[str, Any]]):
        if isinstance(data, str):
            dict_data = json.loads(data)
        else:
            dict_data = data
        return cls.from_dict(dict_data)

    @staticmethod
    def _from_dict_offsets(offsets):
        converted_offsets = []
        for e in offsets:
            if isinstance(e, dict):
                    converted_offsets.append(TableCell(**e))
                else:
                    converted_offsets.append(Span(**e))
            else:
                converted_offsets.append(e)
        return converted_offsets

    def __eq__(self, other):
        context = getattr(other, \""context\"", None)
        if isinstance(context, pd.DataFrame):
            is_content_equal = context.equals(self.context)
        else:
            is_content_equal = context == self.context
        return (
            isinstance(other, self.__class__)
            and is_content_equal
            and getattr(other, \""answer\"", None) == self.answer
            and getattr(other, \""type\"", None) == self.type
            and getattr(other, \""score\"", None) == self.score
            and getattr(other, \""offsets_in_document\"", None) == self.offsets_in_document
            and getattr(other, \""offsets_in_context\"", None) == self.offsets_in_context
            and getattr(other, \""document_ids\"", None) == self.document_ids
            and getattr(other, \""meta\"", None) == self.meta
        )


@dataclass
class Label:
    id: str
    query: str
    document: Document
    is_correct_answer: bool
    is_correct_document: bool
    origin: Literal[\""user-feedback\"", \""gold-label\""]
    answer: Optional[Answer] = None
    pipeline_id: Optional[str] = None
    created_at: Optional[str] = None
    updated_at: Optional[str] = None
    meta: Optional[dict] = None
    filters: Optional[Dict[str, Any]] = None

    def __init__(
        self,
        query: str,
        document: Document,
        is_correct_answer: bool,
        is_correct_document: bool,
        origin: Literal[\""user-feedback\"", \""gold-label\""],
        answer: Optional[Answer],
        id: Optional[str] = None,
        pipeline_id: Optional[str] = None,
        created_at: Optional[str] = None,
        updated_at: Optional[str] = None,
        meta: Optional[dict] = None,
        filters: Optional[Dict[str, Any]] = None,
    ):

        if id:
            self.id = str(id)
        else:
            self.id = str(uuid4())

        if created_at is None:
            created_at = time.strftime(LABEL_DATETIME_FORMAT)
        self.created_at = created_at

        self.updated_at = updated_at
        self.query = query

        self.answer = answer
        self.document = document

        self.is_correct_answer = is_correct_answer
        self.is_correct_document = is_correct_document
        self.origin = origin


        self.pipeline_id = pipeline_id
        if not meta:
            self.meta = {}
        else:
            self.meta = meta
        self.filters = filters

    @property
    def no_answer(self) -> Optional[bool]:
        no_answer = None
        if self.answer is not None:
            no_answer = self.answer.answer is None or self.answer.answer.strip() == \""\""
        return no_answer

    def to_dict(self):
        return asdict(self, dict_factory=_dict_factory)

    @classmethod
    def from_dict(cls, dict: Dict):
        answer = dict.get(\""answer\"")
        if answer and isinstance(answer, Dict):
            dict[\""answer\""] = Answer.from_dict(dict[\""answer\""])
        doc = dict.get(\""document\"")
        if isinstance(doc, Dict):
            dict[\""document\""] = Document.from_dict(dict[\""document\""])
        return cls(**dict)

    def to_json(self):
        return json.dumps(self.to_dict(), cls=NumpyEncoder)

    @classmethod
    def from_json(cls, data: Union[str, Dict[str, Any]]):
        if isinstance(data, str):
            dict_data = json.loads(data)
        else:
            dict_data = data
        return cls.from_dict(dict_data)

    def __eq__(self, other):
        return (
            isinstance(other, self.__class__)
            and getattr(other, \""query\"", None) == self.query
            and getattr(other, \""answer\"", None) == self.answer
            and getattr(other, \""is_correct_answer\"", None) == self.is_correct_answer
            and getattr(other, \""is_correct_document\"", None) == self.is_correct_document
            and getattr(other, \""origin\"", None) == self.origin
            and getattr(other, \""document\"", None) == self.document
            and getattr(other, \""no_answer\"", None) == self.no_answer
            and getattr(other, \""pipeline_id\"", None) == self.pipeline_id
        )

    def __hash__(self):
        return hash(
            self.query
            + str(self.answer)
            + str(self.is_correct_answer)
            + str(self.is_correct_document)
            + str(self.origin)
            + str(self.document)
            + str(self.no_answer)
            + str(self.pipeline_id)
        )

    def __repr__(self):
        return f\""<Label: {self.to_dict()}>\""

    def __str__(self):
        return f\""<Label: {self.to_dict()}>\""


def is_positive_label(label):
    return (label.is_correct_answer and label.is_correct_document) or (
        label.answer is None and label.is_correct_document
    )


class MultiLabel:
    def __init__(self, labels: List[Label], drop_negative_labels: bool = False, drop_no_answers: bool = False):
        labels = list(dict.fromkeys(labels))
        if drop_negative_labels:
            labels = [l for l in labels if is_positive_label(l)]
        if drop_no_answers:
            labels = [l for l in labels if l.no_answer is False]

        self._labels = labels
        self._query = self._aggregate_labels(key=\""query\"", must_be_single_value=True)[0]
        self._filters = self._aggregate_labels(key=\""filters\"", must_be_single_value=True)[0]
        self.id = hashlib.md5((self.query + json.dumps(self.filters, sort_keys=True)).encode()).hexdigest()

        self._no_answer = all(l.no_answer for l in self._labels)

        if self._no_answer:
            self._answers = [\""\""]
            self._offsets_in_documents: List[dict] = []
            self._offsets_in_contexts: List[dict] = []
        else:
            answered = [l.answer for l in self._labels if not l.no_answer and l.answer is not None]
            self._answers = [answer.answer for answer in answered]
            self._offsets_in_documents = []
            self._offsets_in_contexts = []
            for answer in answered:
                if answer.offsets_in_document is not None:
                    for span in answer.offsets_in_document:
                        self._offsets_in_documents.append(self._to_dict_offsets(span))
                if answer.offsets_in_context is not None:
                    for span in answer.offsets_in_context:
                        self._offsets_in_contexts.append(self._to_dict_offsets(span))

        self._document_ids = [l.document.id for l in self._labels if not l.no_answer]
        self._contexts = [str(l.document.content) for l in self._labels if not l.no_answer]

    @staticmethod
    def _to_dict_offsets(offset: Union[Span, TableCell]) -> Dict:
        if isinstance(offset, TableCell):
            return {\""row\"": offset.row, \""col\"": offset.col}
        else:
            return {\""start\"": offset.start, \""end\"": offset.end}

    @property
    def labels(self):
        return self._labels

    @property
    def query(self):
        return self._query

    @property
    def filters(self):
        return self._filters

    @property
    def document_ids(self):
        return self._document_ids

    @property
    def contexts(self):
        return self._contexts

    @property
    def no_answer(self):
        return self._no_answer

    @property
    def answers(self):
        return self._answers

    @property
    def offsets_in_documents(self):
        return self._offsets_in_documents

    @property
    def offsets_in_contexts(self):
        return self._offsets_in_contexts

    def _aggregate_labels(self, key, must_be_single_value=True) -> List[Any]:
        if any(isinstance(getattr(l, key), dict) for l in self.labels):
            unique_values = []
            for l in self.labels:
                if l.filters not in unique_values:
                    unique_values.append(l.filters)
        else:
            unique_values = list({getattr(l, key) for l in self.labels})
        if must_be_single_value and len(unique_values) > 1:
            raise ValueError(
                f\""Tried to combine attribute '{key}' of Labels, but found multiple different values: {unique_values}\""
            )
        return unique_values

    def to_dict(self):
        result = {k[1:] if k[0] == \""_\"" else k: v for k, v in vars(self).items()}
        result[\""labels\""] = [label.to_dict() for label in result[\""labels\""]]
        return result

    @classmethod
    def from_dict(cls, dict: Dict):
        inputs = {k: v for k, v in dict.items() if k in inspect.signature(cls).parameters}
        inputs[\""labels\""] = [Label.from_dict(label) for label in inputs[\""labels\""]]
        return cls(**inputs)

    def to_json(self):
        return json.dumps(self.to_dict(), default=pydantic_encoder)

    @classmethod
    def from_json(cls, data: Union[str, Dict[str, Any]]):
        if isinstance(data, str):
            dict_data = json.loads(data)
        else:
            dict_data = data
        return cls.from_dict(dict_data)

    def __eq__(self, other):
        return isinstance(other, self.__class__) and self.labels == other.labels

    def __repr__(self):
        return f\""<MultiLabel: {self.to_dict()}>\""

    def __str__(self):
        return f\""<MultiLabel: {self.to_dict()}>\""


def _pydantic_dataclass_from_dict(dict: Dict, pydantic_dataclass_type) -> Any:
    base_model = pydantic_dataclass_type.__pydantic_model__.parse_obj(dict)
    base_mode_fields = base_model.__fields__

    values = {}
    for base_model_field_name in base_mode_fields.keys():
        value = getattr(base_model, base_model_field_name)
        values[base_model_field_name] = value

    dataclass_object = pydantic_dataclass_type(**values)
    return dataclass_object


def _dict_factory(data):

    def convert_value(v):
        if isinstance(v, pd.DataFrame):
            return dataframe_to_list(v)
        return v

    return {k: convert_value(v) for k, v in data}


class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)


def dataframe_to_list(df: pd.DataFrame) -> List[List]:
    return [df.columns.tolist()] + df.values.tolist()


def dataframe_from_list(list_df: List[List]) -> pd.DataFrame:
    return pd.DataFrame(columns=list_df[0], data=list_df[1:])


class EvaluationResult:
    def __init__(self, node_results: Optional[Dict[str, DataFrame]] = None) -> None:
        self.node_results: Dict[str, DataFrame] = {} if node_results is None else node_results

    def __getitem__(self, key: str):
        return self.node_results.__getitem__(key)

    def __delitem__(self, key: str):
        self.node_results.__delitem__(key)

    def __setitem__(self, key: str, value: DataFrame):
        self.node_results.__setitem__(key, value)

    def __contains__(self, key: str):
        return self.node_results.keys().__contains__(key)

    def __len__(self):
        return self.node_results.__len__()

    def append(self, key: str, value: DataFrame):
        if value is not None and len(value) > 0:
            if key in self.node_results:
                self.node_results[key] = pd.concat([self.node_results[key], value]).reset_index(drop=True)
            else:
                self.node_results[key] = value

    def calculate_metrics(
        self,
        simulated_top_k_reader: int = -1,
        simulated_top_k_retriever: int = -1,
        document_scope: Literal[
            \""document_id\
,""            \""context\
,""            \""document_id_and_context\
,""            \""document_id_or_context\
,""            \""answer\
,""            \""document_id_or_answer\
,""        ] = \""document_id_or_answer\
,""        eval_mode: Literal[\""integrated\"", \""isolated\""] = \""integrated\
,""        answer_scope: Literal[\""any\"", \""context\"", \""document_id\"", \""document_id_and_context\""] = \""any\
,""    ) -> Dict[str, Dict[str, float]]:
        return {
            node: self._calculate_node_metrics(
                df,
                simulated_top_k_reader=simulated_top_k_reader,
                simulated_top_k_retriever=simulated_top_k_retriever,
                document_scope=document_scope,
                answer_scope=answer_scope,
                eval_mode=eval_mode,
            )
            for node, df in self.node_results.items()
        }

    def wrong_examples(
        self,
        node: str,
        n: int = 3,
        simulated_top_k_reader: int = -1,
        simulated_top_k_retriever: int = -1,
        document_scope: Literal[
            \""document_id\
,""            \""context\
,""            \""document_id_and_context\
,""            \""document_id_or_context\
,""            \""answer\
,""            \""document_id_or_answer\
,""        ] = \""document_id_or_answer\
,""        document_metric: str = \""recall_single_hit\
,""        answer_metric: str = \""f1\
,""        document_metric_threshold: float = 0.5,
        answer_metric_threshold: float = 0.5,
        eval_mode: Literal[\""integrated\"", \""isolated\""] = \""integrated\
,""        answer_scope: Literal[\""any\"", \""context\"", \""document_id\"", \""document_id_and_context\""] = \""any\
,""    ) -> List[Dict]:
        node_df = self.node_results[node]
        node_df = self._filter_eval_mode(node_df, eval_mode)

        answers = node_df[node_df[\""type\""] == \""answer\""]
        if len(answers) > 0:
            metrics_df = self._build_answer_metrics_df(
                answers,
                simulated_top_k_reader=simulated_top_k_reader,
                simulated_top_k_retriever=simulated_top_k_retriever,
                answer_scope=answer_scope,
            )
            worst_df = metrics_df.sort_values(by=[answer_metric]).head(n)
            wrong_examples = []
            for multilabel_id, metrics in worst_df.iterrows():
                query_answers = answers[answers[\""multilabel_id\""] == multilabel_id]
                if answer_metric not in metrics:
                    logger.warning(
                        \""You specified an answer_metric=%s not available in calculated metrics=%s.\""
                        \""Skipping collection of worst performing samples.\
,""                        answer_metric,
                        metrics.keys(),
                    )
                    break
                if metrics[answer_metric] <= answer_metric_threshold:
                    query_dict = {
                        \""multilabel_id\"": query_answers[\""multilabel_id\""].iloc[0],
                        \""query\"": query_answers[\""query\""].iloc[0],
                        \""filters\"": query_answers[\""filters\""].iloc[0],
                        \""metrics\"": metrics.to_dict(),
                        \""answers\"": query_answers.drop(
                            [\""node\"", \""query\"", \""type\"", \""gold_answers\"", \""gold_offsets_in_documents\"", \""gold_document_ids\""],
                            axis=1,
                        ).to_dict(orient=\""records\""),
                        \""gold_answers\"": query_answers[\""gold_answers\""].iloc[0],
                        \""gold_document_ids\"": query_answers[\""gold_document_ids\""].iloc[0],
                    }
                    wrong_examples.append(query_dict)
            return wrong_examples

        documents = node_df[node_df[\""type\""] == \""document\""]
        if len(documents) > 0:
            document_relevance_criterion = self._get_document_relevance_criterion(
                document_scope=document_scope, answer_scope=answer_scope
            )
            metrics_df = self._build_document_metrics_df(
                documents,
                simulated_top_k_retriever=simulated_top_k_retriever,
                document_relevance_criterion=document_relevance_criterion,
            )
            worst_df = metrics_df.sort_values(by=[document_metric]).head(n)
            wrong_examples = []
            for multilabel_id, metrics in worst_df.iterrows():
                if document_metric not in metrics:
                    logger.warning(
                        \""You specified a document_metric=%s not available in calculated metrics=%s.\""
                        \""Skipping collection of worst performing samples.\
,""                        document_metric,
                        metrics.keys(),
                    )
                    break
                if metrics[document_metric] <= document_metric_threshold:
                    query_documents = documents[documents[\""multilabel_id\""] == multilabel_id]
                    query_dict = {
                        \""multilabel_id\"": query_documents[\""multilabel_id\""].iloc[0],
                        \""query\"": query_documents[\""query\""].iloc[0],
                        \""filters\"": query_documents[\""filters\""].iloc[0],
                        \""metrics\"": metrics.to_dict(),
                        \""documents\"": query_documents.drop(
                            [\""node\"", \""query\"", \""multilabel_id\"", \""filters\"", \""type\"", \""gold_document_ids\"", \""gold_contexts\""],
                            axis=1,
                        ).to_dict(orient=\""records\""),
                        \""gold_document_ids\"": query_documents[\""gold_document_ids\""].iloc[0],
                    }
                    wrong_examples.append(query_dict)
            return wrong_examples

        return []

    def _get_document_relevance_criterion(
        self,
        document_scope: Literal[
            \""document_id\
,""            \""context\
,""            \""document_id_and_context\
,""            \""document_id_or_context\
,""            \""answer\
,""            \""document_id_or_answer\
,""        ] = \""document_id_or_answer\
,""        answer_scope: Literal[\""any\"", \""context\"", \""document_id\"", \""document_id_and_context\""] = \""any\
,""    ) -> Literal[
        \""document_id\
,""        \""context\
,""        \""document_id_and_context\
,""        \""document_id_or_context\
,""        \""answer\
,""        \""context_and_answer\
,""        \""document_id_and_answer\
,""        \""document_id_and_context_and_answer\
,""        \""document_id_or_answer\
,""    ]:
        answer_scope_to_doc_relevance_crit = {
            \""context\"": \""context_and_answer\
,""            \""document_id\"": \""document_id_and_answer\
,""            \""document_id_and_context\"": \""document_id_and_context_and_answer\
,""        }

        document_relevance_criterion: str = document_scope
        if document_scope in [\""answer\"", \""document_id_or_answer\""]:
            document_relevance_criterion = answer_scope_to_doc_relevance_crit.get(answer_scope, document_scope)
        elif answer_scope in answer_scope_to_doc_relevance_crit.keys():
            logger.warning(
                \""You specified a non-answer document_scope together with a non-default answer_scope. \""
                \""This may result in inconsistencies between answer and document metrics. \""
                \""To enforce the same definition of correctness for both, document_scope must be one of 'answer', 'document_id_or_answer'.\""
            )


    def _calculate_node_metrics(
        self,
        df: DataFrame,
        simulated_top_k_reader: int = -1,
        simulated_top_k_retriever: int = -1,
        document_scope: Literal[
            \""document_id\
,""            \""context\
,""            \""document_id_and_context\
,""            \""document_id_or_context\
,""            \""answer\
,""            \""document_id_or_answer\
,""        ] = \""document_id_or_answer\
,""        eval_mode: str = \""integrated\
,""        answer_scope: Literal[\""any\"", \""context\"", \""document_id\"", \""document_id_and_context\""] = \""any\
,""    ) -> Dict[str, float]:
        df = self._filter_eval_mode(df, eval_mode)

        answer_metrics = self._calculate_answer_metrics(
            df,
            simulated_top_k_reader=simulated_top_k_reader,
            simulated_top_k_retriever=simulated_top_k_retriever,
            answer_scope=answer_scope,
        )

        document_relevance_criterion = self._get_document_relevance_criterion(
            document_scope=document_scope, answer_scope=answer_scope
        )
        document_metrics = self._calculate_document_metrics(
            df,
            simulated_top_k_retriever=simulated_top_k_retriever,
            document_relevance_criterion=document_relevance_criterion,
        )

        return {**answer_metrics, **document_metrics}

    def _filter_eval_mode(self, df: DataFrame, eval_mode: str) -> DataFrame:
        if \""eval_mode\"" in df.columns:
            df = df[df[\""eval_mode\""] == eval_mode]
        else:
            logger.warning(\""eval dataframe has no eval_mode column. eval_mode param will be ignored.\"")
        return df

    def _calculate_answer_metrics(
        self,
        df: DataFrame,
        simulated_top_k_reader: int = -1,
        simulated_top_k_retriever: int = -1,
        answer_scope: Literal[\""any\"", \""context\"", \""document_id\"", \""document_id_and_context\""] = \""any\
,""    ) -> Dict[str, float]:
        answers = df[df[\""type\""] == \""answer\""]
        if len(answers) == 0:
            return {}

        metrics_df = self._build_answer_metrics_df(
            answers,
            simulated_top_k_reader=simulated_top_k_reader,
            simulated_top_k_retriever=simulated_top_k_retriever,
            answer_scope=answer_scope,
        )
        num_examples_for_eval = len(answers[\""multilabel_id\""].unique())
        result = {metric: metrics_df[metric].mean().tolist() for metric in metrics_df.columns}
        return result

    def _build_answer_metrics_df(
        self,
        answers: DataFrame,
        simulated_top_k_reader: int = -1,
        simulated_top_k_retriever: int = -1,
        answer_scope: Literal[\""any\"", \""context\"", \""document_id\"", \""document_id_and_context\""] = \""any\
,""    ) -> DataFrame:
        multilabel_ids = answers[\""multilabel_id\""].unique()
        if simulated_top_k_retriever != -1:
            documents = self._get_documents_df()

            top_k_documents = documents[documents[\""rank\""] <= simulated_top_k_retriever]
            simulated_answers = []
            for multilabel_id in multilabel_ids:
                top_k_document_ids = top_k_documents[top_k_documents[\""multilabel_id\""] == multilabel_id][
                    \""document_id\""
                ].unique()
                query_answers = answers[answers[\""multilabel_id\""] == multilabel_id]

                simulated_query_answers = query_answers[
                    query_answers[\""document_ids\""].apply(
                        lambda document_ids, top_k_document_ids=top_k_document_ids: all(
                            document_id in top_k_document_ids for document_id in document_ids
                        )
                    )
                ]
                if simulated_top_k_reader != -1:
                    simulated_query_answers = simulated_query_answers.nsmallest(simulated_top_k_reader, \""rank\"")
                simulated_query_answers[\""rank\""] = np.arange(1, len(simulated_query_answers) + 1)
                simulated_answers.append(simulated_query_answers)
            answers = pd.concat(simulated_answers)
        elif simulated_top_k_reader != -1:
            answers = answers[answers[\""rank\""] <= simulated_top_k_reader]

        answer_metrics = [\""exact_match\"", \""f1\"", \""sas\""]
        df_records = []

        for multilabel_id in multilabel_ids:
            query_df = answers[answers[\""multilabel_id\""] == multilabel_id]
            metric_to_scoped_col = {
                metric: f\""{metric}_{answer_scope}_scope\"" if answer_scope != \""any\"" else metric
                for metric in answer_metrics
                if metric in query_df.columns
            }
            query_metrics = {
                metric: query_df[col].max() if any(query_df) else 0.0 for metric, col in metric_to_scoped_col.items()
            }
            df_records.append(query_metrics)

        metrics_df = DataFrame.from_records(df_records, index=multilabel_ids)
        return metrics_df

    def _get_documents_df(self):
        document_dfs = [
            node_df for node_df in self.node_results.values() if len(node_df[node_df[\""type\""] == \""document\""]) > 0
        ]
        if len(document_dfs) != 1:
            raise ValueError(\""cannot detect retriever dataframe\"")
        documents_df = document_dfs[0]
        documents_df = documents_df[documents_df[\""type\""] == \""document\""]
        return documents_df

    def _calculate_document_metrics(
        self,
        df: DataFrame,
        simulated_top_k_retriever: int = -1,
        document_relevance_criterion: Literal[
            \""document_id\
,""            \""context\
,""            \""document_id_and_context\
,""            \""document_id_or_context\
,""            \""answer\
,""            \""context_and_answer\
,""            \""document_id_and_answer\
,""            \""document_id_and_context_and_answer\
,""            \""document_id_or_answer\
,""        ] = \""document_id_or_answer\
,""    ) -> Dict[str, float]:
        documents = df[df[\""type\""] == \""document\""]
        if len(documents) == 0:
            return {}

        metrics_df = self._build_document_metrics_df(
            documents,
            simulated_top_k_retriever=simulated_top_k_retriever,
            document_relevance_criterion=document_relevance_criterion,
        )

        return {metric: metrics_df[metric].mean().tolist() for metric in metrics_df.columns}

    def _build_document_metrics_df(
        self,
        documents: DataFrame,
        simulated_top_k_retriever: int = -1,
        document_relevance_criterion: Literal[
            \""document_id\
,""            \""context\
,""            \""document_id_and_context\
,""            \""document_id_or_context\
,""            \""answer\
,""            \""context_and_answer\
,""            \""document_id_and_answer\
,""            \""document_id_and_context_and_answer\
,""            \""document_id_or_answer\
,""        ] = \""document_id_or_answer\
,""    ) -> DataFrame:
        if simulated_top_k_retriever != -1:
            documents = documents[documents[\""rank\""] <= simulated_top_k_retriever]

            id_matches = [idx for idx, val in enumerate(row[\""gold_documents_id_match\""]) if val == 1.0]
            context_matches = [
                idx for idx, val in enumerate(row[\""gold_contexts_similarity\""]) if val > 65.0
            answer_matches = [idx for idx, val in enumerate(row[\""gold_answers_match\""]) if val == 1.0]
            if document_relevance_criterion == \""document_id\"":
                return id_matches
            elif document_relevance_criterion == \""context\"":
                return context_matches
            elif document_relevance_criterion == \""answer\"":
                return answer_matches
            elif document_relevance_criterion == \""document_id_and_context\"":
                return list(set(id_matches) & set(context_matches))
            elif document_relevance_criterion == \""document_id_or_context\"":
                return list(set(id_matches) | set(context_matches))
            elif document_relevance_criterion == \""document_id_and_answer\"":
                return list(set(id_matches) & set(answer_matches))
            elif document_relevance_criterion == \""document_id_or_answer\"":
                return list(set(id_matches) | set(answer_matches))
            elif document_relevance_criterion == \""context_and_answer\"":
                return list(set(context_matches) & set(answer_matches))
            elif document_relevance_criterion == \""document_id_and_context_and_answer\"":
                return list(set(id_matches) & set(context_matches) & set(answer_matches))
            else:
                raise ValueError(f\""document_relevance_criterion '{document_relevance_criterion}' not supported.\"")

        documents[\""matched_label_idxs\""] = documents.apply(find_matched_label_idxs, axis=1)

        metrics = []

        for multilabel_id in documents[\""multilabel_id\""].unique():
            query_df = documents[documents[\""multilabel_id\""] == multilabel_id]

            relevance_criterion_col = f\""{document_relevance_criterion.replace('document_id', 'gold_id')}_match\""
            relevant_rows = query_df[query_df[relevance_criterion_col] == 1]

            gold_document_ids = (
                list(query_df[\""gold_custom_document_ids\""].iloc[0])
                if \""gold_custom_document_ids\"" in query_df
                else list(query_df[\""gold_document_ids\""].iloc[0])
            )
            gold_document_ids = [id for id in gold_document_ids if id != \""00\""]

            num_labels = len(gold_document_ids)
            num_matched_labels = len({idx for idxs in relevant_rows[\""matched_label_idxs\""] for idx in idxs})
            num_missing_labels = num_labels - num_matched_labels

            relevance_criterion_ids = list(relevant_rows[\""document_id\""].values)
            num_relevants = len(set(relevance_criterion_ids)) + num_missing_labels

            num_retrieved = len(query_df[\""document_id\""])
            num_retrieved_relevants = len(relevant_rows)
            rank_retrieved_relevants = relevant_rows[\""rank\""].values

            if num_labels == 0:
                rr = 1.0
                avg_precision = 1.0
                recall_multi_hit = 1.0
                recall_single_hit = 1.0
                precision = 1.0
                ndcg = 1.0
            elif num_retrieved_relevants == 0:
                rr = 0.0
                avg_precision = 0.0
                recall_multi_hit = 0.0
                recall_single_hit = 0.0
                precision = 0.0
                ndcg = 0.0
            else:
                avp_retrieved_relevants = [
                    len(relevant_rows[relevant_rows[\""rank\""] <= rank]) / rank for rank in rank_retrieved_relevants
                ]
                avg_precision = np.sum(avp_retrieved_relevants) / num_relevants
                recall_multi_hit = num_matched_labels / num_labels
                recall_single_hit = 1.0
                precision = num_retrieved_relevants / num_retrieved
                rr = 1.0 / rank_retrieved_relevants.min()
                dcg = np.sum([1.0 / np.log2(rank + 1) for rank in rank_retrieved_relevants])
                idcg = np.sum([1.0 / np.log2(rank + 1) for rank in range(1, num_relevants + 1)])
                ndcg = dcg / idcg

            metrics.append(
                {
                    \""recall_multi_hit\"": recall_multi_hit,
                    \""recall_single_hit\"": recall_single_hit,
                    \""precision\"": precision,
                    \""map\"": avg_precision,
                    \""mrr\"": rr,
                    \""ndcg\"": ndcg,
                }
            )

        metrics_df = DataFrame.from_records(metrics, index=documents[\""multilabel_id\""].unique())
        return metrics_df

    def save(self, out_dir: Union[str, Path], **to_csv_kwargs):
        out_dir = out_dir if isinstance(out_dir, Path) else Path(out_dir)
        logger.info(\""Saving evaluation results to %s\"", out_dir)
        if not out_dir.exists():
            out_dir.mkdir(parents=True)
        for node_name, df in self.node_results.items():
            target_path = out_dir / f\""{node_name}.csv\""
            default_to_csv_kwargs = {
                \""index\"": False,
            }
            to_csv_kwargs = {**default_to_csv_kwargs, **to_csv_kwargs}
            df.to_csv(target_path, **to_csv_kwargs)

    @classmethod
    def load(cls, load_dir: Union[str, Path], **read_csv_kwargs):
        load_dir = load_dir if isinstance(load_dir, Path) else Path(load_dir)
        csv_files = [file for file in load_dir.iterdir() if file.is_file() and file.suffix == \"".csv\""]
        cols_to_convert = [
            \""filters\
,""            \""gold_document_ids\
,""            \""gold_custom_document_ids\
,""            \""gold_contexts\
,""            \""gold_answers\
,""            \""gold_documents_id_match\
,""            \""gold_offsets_in_documents\
,""            \""gold_offsets_in_contexts\
,""            \""gold_answers_exact_match\
,""            \""gold_answers_f1\
,""            \""gold_answers_sas\
,""            \""gold_answers_match\
,""            \""gold_contexts_similarity\
,""            \""offsets_in_document\
,""            \""offsets_in_context\
,""            \""document_ids\
,""            \""custom_document_ids\
,""            \""gold_document_contents\
,""        ]

        def safe_literal_eval(x: str) -> Any:
            if x == \""\"":
                return None
            return ast.literal_eval(x)

        converters = dict.fromkeys(cols_to_convert, safe_literal_eval)
        default_read_csv_kwargs = {\""converters\"": converters, \""header\"": 0}
        read_csv_kwargs = {**default_read_csv_kwargs, **read_csv_kwargs}
        node_results = {file.stem: pd.read_csv(file, **read_csv_kwargs) for file in csv_files}
        for df in node_results.values():
            df.replace(to_replace=np.nan, value=None, inplace=True)
            df.rename(columns={\""gold_document_contents\"": \""gold_contexts\"", \""content\"": \""context\""}, inplace=True)
            if \""answer\"" in df.columns and \""document_id\"" in df.columns and not \""document_ids\"" in df.columns:
                df[\""document_ids\""] = df[\""document_id\""].apply(lambda x: [x] if x not in [None, \""None\""] else [])
                df.drop(columns=[\""document_id\""], inplace=True)
            if (
                \""answer\"" in df.columns
                and \""custom_document_id\"" in df.columns
                and not \""custom_document_ids\"" in df.columns
            ):
                df[\""custom_document_ids\""] = df[\""custom_document_id\""].apply(
                    lambda x: [x] if x not in [None, \""None\""] else []
                )
                df.drop(columns=[\""custom_document_id\""], inplace=True)
        result = cls(node_results)
        return result
","['getLogger', 'Field', '__init__', 'ValueError', 'all', 'startswith', 'join', 'asarray', 'str', '_get_id', 'format', 'hash128', 'split', 'getattr', 'to_dict', 'items', 'isinstance', 'dataframe_to_list', 'from_dict', 'copy', 'keys', 'get', 'dataframe_from_list', 'cls', 'to_json', 'dumps', 'from_json', 'loads', '__eq__', 'equals', 'array_equal', '__repr__', '__str__', 'len', '__lt__', '__contains__', 'float', 'type', '__post_init__', '_from_dict_offsets', 'asdict', 'pop', 'append', 'TableCell', 'Span', 'uuid4', 'strftime', 'no_answer', 'strip', '__hash__', 'hash', 'is_positive_label', 'list', 'fromkeys', '_aggregate_labels', 'md5', 'encode', 'hexdigest', '_to_dict_offsets', 'labels', 'query', 'filters', 'document_ids', 'contexts', 'answers', 'offsets_in_documents', 'offsets_in_contexts', 'any', 'vars', 'signature', '_pydantic_dataclass_from_dict', 'parse_obj', 'pydantic_dataclass_type', '_dict_factory', 'convert_value', 'NumpyEncoder', 'default', 'tolist', 'DataFrame', '__getitem__', '__delitem__', '__setitem__', '__len__', 'concat', 'reset_index', 'calculate_metrics', '_calculate_node_metrics', 'wrong_examples', '_filter_eval_mode', '_build_answer_metrics_df', 'sort_values', 'head', 'iterrows', 'warning', 'drop', '_get_document_relevance_criterion', '_build_document_metrics_df', '_calculate_answer_metrics', '_calculate_document_metrics', 'unique', 'mean', '_get_documents_df', 'apply', 'nsmallest', 'arange', 'max', 'from_records', 'values', 'enumerate', 'set', 'replace', 'sum', 'min', 'log2', 'range', 'save', 'Path', 'info', 'exists', 'mkdir', 'to_csv', 'load', 'iterdir', 'is_file', 'safe_literal_eval', 'literal_eval', 'read_csv', 'rename']"
"https://github.com/apache/airflow/blob/aaed909344b12aa4691a9e23ea9f9c98d641d853/airflow/cli/commands/webserver_command.py
"," __future__ import annotations

import logging
import os
import signal
import subprocess
import sys
import textwrap
import time
from contextlib import suppress
from pathlib import Path
from time import sleep
from typing import NoReturn

import psutil
from lockfile.pidlockfile import read_pid_from_pidfile

from airflow import settings
from airflow.cli.commands.daemon_utils import run_command_with_daemon_option
from airflow.configuration import conf
from airflow.exceptions import AirflowException, AirflowWebServerTimeout
from airflow.utils import cli as cli_utils
from airflow.utils.cli import setup_locations
from airflow.utils.hashlib_wrapper import md5
from airflow.utils.log.logging_mixin import LoggingMixin
from airflow.utils.providers_configuration_loader import providers_configuration_loaded

log = logging.getLogger(__name__)


class GunicornMonitor(LoggingMixin):

    def __init__(
        self,
        gunicorn_master_pid: int,
        num_workers_expected: int,
        master_timeout: int,
        worker_refresh_interval: int,
        worker_refresh_batch_size: int,
        reload_on_plugin_change: bool,
    ):
        super().__init__()
        self.gunicorn_master_proc = psutil.Process(gunicorn_master_pid)
        self.num_workers_expected = num_workers_expected
        self.master_timeout = master_timeout
        self.worker_refresh_interval = worker_refresh_interval
        self.worker_refresh_batch_size = worker_refresh_batch_size
        self.reload_on_plugin_change = reload_on_plugin_change

        self._num_workers_running = 0
        self._num_ready_workers_running = 0
        self._last_refresh_time = time.monotonic() if worker_refresh_interval > 0 else None
        self._last_plugin_state = self._generate_plugin_state() if reload_on_plugin_change else None
        self._restart_on_next_plugin_check = False

    def _generate_plugin_state(self) -> dict[str, float]:
        if not settings.PLUGINS_FOLDER:
            return {}

        all_filenames: list[str] = []
        for root, _, filenames in os.walk(settings.PLUGINS_FOLDER):
            all_filenames.extend(os.path.join(root, f) for f in filenames)
        plugin_state = {f: self._get_file_hash(f) for f in sorted(all_filenames)}
        return plugin_state

    @staticmethod
    def _get_file_hash(fname: str):
        hash_md5 = md5()
        with open(fname, \""rb\"") as f:
            for chunk in iter(lambda: f.read(4096), b\""\""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def _get_num_ready_workers_running(self) -> int:
        workers = psutil.Process(self.gunicorn_master_proc.pid).children()

        def ready_prefix_on_cmdline(proc):
            try:
                cmdline = proc.cmdline()
                if cmdline:
                    return settings.GUNICORN_WORKER_READY_PREFIX in cmdline[0]
            except psutil.NoSuchProcess:
                pass
            return False

        nb_ready_workers = sum(1 for proc in workers if ready_prefix_on_cmdline(proc))
        return nb_ready_workers

    def _get_num_workers_running(self) -> int:
        workers = psutil.Process(self.gunicorn_master_proc.pid).children()
        return len(workers)

    def _wait_until_true(self, fn, timeout: int = 0) -> None:
        start_time = time.monotonic()
        while not fn():
            if 0 < timeout <= time.monotonic() - start_time:
                raise AirflowWebServerTimeout(f\""No response from gunicorn master within {timeout} seconds\"")
            sleep(0.1)

    def _spawn_new_workers(self, count: int) -> None:
        excess = 0
        for _ in range(count):
            self.gunicorn_master_proc.send_signal(signal.SIGTTIN)
            excess += 1
            self._wait_until_true(
                lambda: self.num_workers_expected + excess == self._get_num_workers_running(),
                timeout=self.master_timeout,
            )

    def _kill_old_workers(self, count: int) -> None:
        for _ in range(count):
            count -= 1
            self.gunicorn_master_proc.send_signal(signal.SIGTTOU)
            self._wait_until_true(
                lambda: self.num_workers_expected + count == self._get_num_workers_running(),
                timeout=self.master_timeout,
            )

    def _reload_gunicorn(self) -> None:
        self.gunicorn_master_proc.send_signal(signal.SIGHUP)
        sleep(1)
        self._wait_until_true(
            lambda: self.num_workers_expected == self._get_num_workers_running(), timeout=self.master_timeout
        )

    def start(self) -> NoReturn:
        try:
            self._wait_until_true(
                lambda: self.num_workers_expected == self._get_num_workers_running(),
                timeout=self.master_timeout,
            )
            while True:
                if not self.gunicorn_master_proc.is_running():
                    sys.exit(1)
                self._check_workers()
                sleep(1)

        except (AirflowWebServerTimeout, OSError) as err:
            self.log.error(err)
            self.log.error(\""Shutting down webserver\"")
            try:
                self.gunicorn_master_proc.terminate()
                self.gunicorn_master_proc.wait()
            finally:
                sys.exit(1)

    def _check_workers(self) -> None:
        num_workers_running = self._get_num_workers_running()
        num_ready_workers_running = self._get_num_ready_workers_running()

        if num_ready_workers_running < num_workers_running:
            self.log.debug(
                \""[%d / %d] Some workers are starting up, waiting...\
,""                num_ready_workers_running,
                num_workers_running,
            )
            sleep(1)
            return

        if num_workers_running > self.num_workers_expected:
            excess = min(num_workers_running - self.num_workers_expected, self.worker_refresh_batch_size)
            self.log.debug(
                \""[%d / %d] Killing %s workers\"", num_ready_workers_running, num_workers_running, excess
            )
            self._kill_old_workers(excess)
            return

        if num_workers_running < self.num_workers_expected:
            self.log.error(
                \""[%d / %d] Some workers seem to have died and gunicorn did not restart them as expected\
,""                num_ready_workers_running,
                num_workers_running,
            )
            sleep(10)
            num_workers_running = self._get_num_workers_running()
            if num_workers_running < self.num_workers_expected:
                new_worker_count = min(
                    self.num_workers_expected - num_workers_running, self.worker_refresh_batch_size
                )
                self.log.info(
                    \""[%d / %d] Spawning %d workers\
,""                    num_ready_workers_running,
                    num_workers_running,
                    new_worker_count,
                )
                self._spawn_new_workers(new_worker_count)
            return


        if self.worker_refresh_interval > 0 and self._last_refresh_time:
            last_refresh_diff = time.monotonic() - self._last_refresh_time
            if self.worker_refresh_interval < last_refresh_diff:
                num_new_workers = self.worker_refresh_batch_size
                self.log.debug(
                    \""[%d / %d] Starting doing a refresh. Starting %d workers.\
,""                    num_ready_workers_running,
                    num_workers_running,
                    num_new_workers,
                )
                self._spawn_new_workers(num_new_workers)
                self._last_refresh_time = time.monotonic()
                return

        if self.reload_on_plugin_change:
            new_state = self._generate_plugin_state()
            if new_state != self._last_plugin_state:
                self.log.debug(
                    \""[%d / %d] Plugins folder changed. The gunicorn will be restarted the next time the \""
                    \""plugin directory is checked, if there is no change in it.\
,""                    num_ready_workers_running,
                    num_workers_running,
                )
                self._restart_on_next_plugin_check = True
                self._last_plugin_state = new_state
            elif self._restart_on_next_plugin_check:
                self.log.debug(
                    \""[%d / %d] Starts reloading the gunicorn configuration.\
,""                    num_ready_workers_running,
                    num_workers_running,
                )
                self._restart_on_next_plugin_check = False
                self._last_refresh_time = time.monotonic()
                self._reload_gunicorn()


@cli_utils.action_cli
@providers_configuration_loaded
def webserver(args):
    print(settings.HEADER)

    if conf.get(\""webserver\"", \""secret_key\"") == \""temporary_key\"":
        from rich import print as rich_print

        rich_print(
            \""[red][bold]ERROR:[/bold] The `secret_key` setting under the webserver config has an insecure \""
            \""value - Airflow has failed safe and refuses to start. Please change this value to a new, \""
            \""per-environment, randomly generated string, for example using this command `[cyan]openssl rand \""
            \""-hex 30[/cyan]`\
,""            file=sys.stderr,
        )
        sys.exit(1)

    access_logfile = args.access_logfile or conf.get(\""webserver\"", \""access_logfile\"")
    error_logfile = args.error_logfile or conf.get(\""webserver\"", \""error_logfile\"")
    access_logformat = args.access_logformat or conf.get(\""webserver\"", \""access_logformat\"")
    num_workers = args.workers or conf.get(\""webserver\"", \""workers\"")
    worker_timeout = args.worker_timeout or conf.get(\""webserver\"", \""web_server_worker_timeout\"")
    ssl_cert = args.ssl_cert or conf.get(\""webserver\"", \""web_server_ssl_cert\"")
    ssl_key = args.ssl_key or conf.get(\""webserver\"", \""web_server_ssl_key\"")
    if not ssl_cert and ssl_key:
        raise AirflowException(\""An SSL certificate must also be provided for use with \"" + ssl_key)
    if ssl_cert and not ssl_key:
        raise AirflowException(\""An SSL key must also be provided for use with \"" + ssl_cert)

    from airflow.www.app import create_app

    if args.debug:
        print(f\""Starting the web server on port {args.port} and host {args.hostname}.\"")
        app = create_app(testing=conf.getboolean(\""core\"", \""unit_test_mode\""))
        app.run(
            debug=True,
            use_reloader=not app.config[\""TESTING\""],
            port=args.port,
            host=args.hostname,
            ssl_context=(ssl_cert, ssl_key) if ssl_cert and ssl_key else None,
        )
    else:
        print(
            textwrap.dedent(
            )
        )

        pid_file, _, _, _ = setup_locations(\""webserver\"", pid=args.pid)
        run_args = [
            sys.executable,
            \""-m\
,""            \""gunicorn\
,""            \""--workers\
,""            str(num_workers),
            \""--worker-class\
,""            str(args.workerclass),
            \""--timeout\
,""            str(worker_timeout),
            \""--bind\
,""            args.hostname + \"":\"" + str(args.port),
            \""--name\
,""            \""airflow-webserver\
,""            \""--pid\
,""            pid_file,
            \""--config\
,""            \""python:airflow.www.gunicorn_config\
,""        ]

        if args.access_logfile:
            run_args += [\""--access-logfile\"", str(args.access_logfile)]

        if args.error_logfile:
            run_args += [\""--error-logfile\"", str(args.error_logfile)]

        if args.access_logformat and args.access_logformat.strip():
            run_args += [\""--access-logformat\"", str(args.access_logformat)]

        if args.daemon:
            run_args += [\""--daemon\""]

        if ssl_cert:
            run_args += [\""--certfile\"", ssl_cert, \""--keyfile\"", ssl_key]

        run_args += [\""airflow.www.app:cached_app()\""]

        if conf.getboolean(\""webserver\"", \""reload_on_plugin_change\"", fallback=False):
            log.warning(
                \""Setting reload_on_plugin_change = true prevents running Gunicorn with preloading. \""
                \""This means the app cannot be loaded before workers are forked, and each worker has a \""
                \""separate copy of the app. This may cause IntegrityError during webserver startup, and \""
                \""should be avoided in production.\""
            )
        else:
            run_args += [\""--preload\""]

        def kill_proc(signum: int, gunicorn_master_proc: psutil.Process | subprocess.Popen) -> NoReturn:
            log.info(\""Received signal: %s. Closing gunicorn.\"", signum)
            gunicorn_master_proc.terminate()
            with suppress(TimeoutError):
                gunicorn_master_proc.wait(timeout=30)
            if isinstance(gunicorn_master_proc, subprocess.Popen):
                still_running = gunicorn_master_proc.poll() is not None
            else:
                still_running = gunicorn_master_proc.is_running()
            if still_running:
                gunicorn_master_proc.kill()
            sys.exit(0)

        def monitor_gunicorn(gunicorn_master_proc: psutil.Process | subprocess.Popen) -> NoReturn:
            signal.signal(signal.SIGINT, lambda signum, _: kill_proc(signum, gunicorn_master_proc))
            signal.signal(signal.SIGTERM, lambda signum, _: kill_proc(signum, gunicorn_master_proc))

            GunicornMonitor(
                gunicorn_master_pid=gunicorn_master_proc.pid,
                num_workers_expected=num_workers,
                master_timeout=conf.getint(\""webserver\"", \""web_server_master_timeout\""),
                worker_refresh_interval=conf.getint(\""webserver\"", \""worker_refresh_interval\"", fallback=30),
                worker_refresh_batch_size=conf.getint(\""webserver\"", \""worker_refresh_batch_size\"", fallback=1),
                reload_on_plugin_change=conf.getboolean(
                    \""webserver\"", \""reload_on_plugin_change\"", fallback=False
                ),
            ).start()

        def start_and_monitor_gunicorn(args):
            if args.daemon:
                subprocess.Popen(run_args, close_fds=True)

                gunicorn_master_proc_pid = None
                while not gunicorn_master_proc_pid:
                    sleep(0.1)
                    gunicorn_master_proc_pid = read_pid_from_pidfile(pid_file)

                gunicorn_master_proc = psutil.Process(gunicorn_master_proc_pid)
                monitor_gunicorn(gunicorn_master_proc)
            else:
                with subprocess.Popen(run_args, close_fds=True) as gunicorn_master_proc:
                    monitor_gunicorn(gunicorn_master_proc)

        if args.daemon:
            os.environ[\""SKIP_DAGS_PARSING\""] = \""True\""
            create_app(None)
            os.environ.pop(\""SKIP_DAGS_PARSING\"")

        pid_file_path = Path(pid_file)
        monitor_pid_file = str(pid_file_path.with_name(f\""{pid_file_path.stem}-monitor{pid_file_path.suffix}\""))
        run_command_with_daemon_option(
            args=args,
            process_name=\""webserver\
,""            callback=lambda: start_and_monitor_gunicorn(args),
            should_setup_logging=True,
            pid_file=monitor_pid_file,
        )
","['getLogger', 'GunicornMonitor', '__init__', 'super', 'Process', 'monotonic', '_generate_plugin_state', 'walk', 'extend', 'join', '_get_file_hash', 'sorted', 'md5', 'open', 'iter', 'read', 'update', 'hexdigest', '_get_num_ready_workers_running', 'children', 'ready_prefix_on_cmdline', 'cmdline', 'sum', '_get_num_workers_running', 'len', '_wait_until_true', 'fn', 'AirflowWebServerTimeout', 'sleep', '_spawn_new_workers', 'range', 'send_signal', '_kill_old_workers', '_reload_gunicorn', 'start', 'is_running', 'exit', '_check_workers', 'error', 'terminate', 'wait', 'debug', 'min', 'info', 'webserver', 'get', 'rich_print', 'AirflowException', 'create_app', 'getboolean', 'run', 'dedent', 'setup_locations', 'str', 'strip', 'cached_app', 'warning', 'kill_proc', 'suppress', 'isinstance', 'poll', 'kill', 'monitor_gunicorn', 'signal', 'getint', 'start_and_monitor_gunicorn', 'Popen', 'read_pid_from_pidfile', 'pop', 'Path', 'with_name', 'run_command_with_daemon_option']"
"https://github.com/apache/airflow/blob/aaed909344b12aa4691a9e23ea9f9c98d641d853/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py
"," __future__ import annotations

import copy
import logging
import os
import secrets
import string
import warnings
from functools import reduce
from typing import TYPE_CHECKING

import re2
from dateutil import parser
from kubernetes.client import models as k8s
from kubernetes.client.api_client import ApiClient

from airflow.exceptions import (
    AirflowConfigException,
    PodMutationHookException,
    PodReconciliationError,
    RemovedInAirflow3Warning,
)
from airflow.kubernetes.pre_7_4_0_compatibility.pod_generator_deprecated import (
    PodDefaults,
    PodGenerator as PodGeneratorDeprecated,
)
from airflow.utils import yaml
from airflow.utils.hashlib_wrapper import md5
from airflow.version import version as airflow_version

if TYPE_CHECKING:
    import datetime

log = logging.getLogger(__name__)

MAX_LABEL_LEN = 63

alphanum_lower = string.ascii_lowercase + string.digits


def rand_str(num):
    return \""\"".join(secrets.choice(alphanum_lower) for _ in range(num))


def add_pod_suffix(pod_name: str, rand_len: int = 8, max_len: int = 80) -> str:
    suffix = \""-\"" + rand_str(rand_len)
    return pod_name[: max_len - len(suffix)].strip(\""-.\"") + suffix


def make_safe_label_value(string: str) -> str:
    safe_label = re2.sub(r\""^[^a-z0-9A-Z]*|[^a-zA-Z0-9_\\-\\.]|[^a-z0-9A-Z]*$\"", \""\"", string)

    if len(safe_label) > MAX_LABEL_LEN or string != safe_label:
        safe_hash = md5(string.encode()).hexdigest()[:9]
        safe_label = safe_label[: MAX_LABEL_LEN - len(safe_hash) - 1] + \""-\"" + safe_hash

    return safe_label


def datetime_to_label_safe_datestring(datetime_obj: datetime.datetime) -> str:
    return datetime_obj.isoformat().replace(\"":\"", \""_\"").replace(\""+\"", \""_plus_\"")


def label_safe_datestring_to_datetime(string: str) -> datetime.datetime:
    return parser.parse(string.replace(\""_plus_\"", \""+\"").replace(\""_\"", \"":\""))


class PodGenerator:

    def __init__(
        self,
        pod: k8s.V1Pod | None = None,
        pod_template_file: str | None = None,
        extract_xcom: bool = True,
    ):
        if not pod_template_file and not pod:
            raise AirflowConfigException(
                \""Podgenerator requires either a `pod` or a `pod_template_file` argument\""
            )
        if pod_template_file and pod:
            raise AirflowConfigException(\""Cannot pass both `pod` and `pod_template_file` arguments\"")

        if pod_template_file:
            self.ud_pod = self.deserialize_model_file(pod_template_file)
        else:
            self.ud_pod = pod

        self.extract_xcom = extract_xcom

    def gen_pod(self) -> k8s.V1Pod:
        warnings.warn(\""This function is deprecated. \"", RemovedInAirflow3Warning)
        result = self.ud_pod

        result.metadata.name = add_pod_suffix(pod_name=result.metadata.name)

        if self.extract_xcom:
            result = self.add_xcom_sidecar(result)

        return result

    @staticmethod
    def add_xcom_sidecar(pod: k8s.V1Pod) -> k8s.V1Pod:
        warnings.warn(
            \""This function is deprecated. \""
            \""Please use airflow.providers.cncf.kubernetes.utils.xcom_sidecar.add_xcom_sidecar instead\""
        )
        pod_cp = copy.deepcopy(pod)
        pod_cp.spec.volumes = pod.spec.volumes or []
        pod_cp.spec.volumes.insert(0, PodDefaults.VOLUME)
        pod_cp.spec.containers[0].volume_mounts = pod_cp.spec.containers[0].volume_mounts or []
        pod_cp.spec.containers[0].volume_mounts.insert(0, PodDefaults.VOLUME_MOUNT)
        pod_cp.spec.containers.append(PodDefaults.SIDECAR_CONTAINER)

        return pod_cp

    @staticmethod
    def from_obj(obj) -> dict | k8s.V1Pod | None:
        if obj is None:
            return None

        k8s_legacy_object = obj.get(\""KubernetesExecutor\"", None)
        k8s_object = obj.get(\""pod_override\"", None)

        if k8s_legacy_object and k8s_object:
            raise AirflowConfigException(
                \""Can not have both a legacy and new\""
                \""executor_config object. Please delete the KubernetesExecutor\""
                \""dict and only use the pod_override kubernetes.client.models.V1Pod\""
                \""object.\""
            )
        if not k8s_object and not k8s_legacy_object:
            return None

        if isinstance(k8s_object, k8s.V1Pod):
            return k8s_object
        elif isinstance(k8s_legacy_object, dict):
            warnings.warn(
                \""Using a dictionary for the executor_config is deprecated and will soon be removed.\""
                'please use a `kubernetes.client.models.V1Pod` class with a \""pod_override\"" key'
                \"" instead. \
,""                category=RemovedInAirflow3Warning,
            )
            return PodGenerator.from_legacy_obj(obj)
        else:
            raise TypeError(
                \""Cannot convert a non-kubernetes.client.models.V1Pod object into a KubernetesExecutorConfig\""
            )

    @staticmethod
    def from_legacy_obj(obj) -> k8s.V1Pod | None:
        if obj is None:
            return None

        namespaced = obj.get(\""KubernetesExecutor\"", {})

        if not namespaced:
            return None

        resources = namespaced.get(\""resources\"")

        if resources is None:
            requests = {
                \""cpu\"": namespaced.pop(\""request_cpu\"", None),
                \""memory\"": namespaced.pop(\""request_memory\"", None),
            }
            limits = {
                \""cpu\"": namespaced.pop(\""limit_cpu\"", None),
                \""memory\"": namespaced.pop(\""limit_memory\"", None),
                \""ephemeral-storage\"": namespaced.pop(\""ephemeral-storage\"", None),
            }
            all_resources = list(requests.values()) + list(limits.values())
            if all(r is None for r in all_resources):
                resources = None
            else:
                requests = {k: v for k, v in requests.items() if v is not None}
                limits = {k: v for k, v in limits.items() if v is not None}
                resources = k8s.V1ResourceRequirements(requests=requests, limits=limits)
        namespaced[\""resources\""] = resources
        return PodGeneratorDeprecated(**namespaced).gen_pod()

    @staticmethod
    def reconcile_pods(base_pod: k8s.V1Pod, client_pod: k8s.V1Pod | None) -> k8s.V1Pod:
        if client_pod is None:
            return base_pod

        client_pod_cp = copy.deepcopy(client_pod)
        client_pod_cp.spec = PodGenerator.reconcile_specs(base_pod.spec, client_pod_cp.spec)
        client_pod_cp.metadata = PodGenerator.reconcile_metadata(base_pod.metadata, client_pod_cp.metadata)
        client_pod_cp = merge_objects(base_pod, client_pod_cp)

        return client_pod_cp

    @staticmethod
    def reconcile_metadata(base_meta, client_meta):
        if base_meta and not client_meta:
            return base_meta
        if not base_meta and client_meta:
            return client_meta
        elif client_meta and base_meta:
            client_meta.labels = merge_objects(base_meta.labels, client_meta.labels)
            client_meta.annotations = merge_objects(base_meta.annotations, client_meta.annotations)
            extend_object_field(base_meta, client_meta, \""managed_fields\"")
            extend_object_field(base_meta, client_meta, \""finalizers\"")
            extend_object_field(base_meta, client_meta, \""owner_references\"")
            return merge_objects(base_meta, client_meta)

        return None

    @staticmethod
    def reconcile_specs(
        base_spec: k8s.V1PodSpec | None, client_spec: k8s.V1PodSpec | None
    ) -> k8s.V1PodSpec | None:
        if base_spec and not client_spec:
            return base_spec
        if not base_spec and client_spec:
            return client_spec
        elif client_spec and base_spec:
            client_spec.containers = PodGenerator.reconcile_containers(
                base_spec.containers, client_spec.containers
            )
            merged_spec = extend_object_field(base_spec, client_spec, \""init_containers\"")
            merged_spec = extend_object_field(base_spec, merged_spec, \""volumes\"")
            return merge_objects(base_spec, merged_spec)

        return None

    @staticmethod
    def reconcile_containers(
        base_containers: list[k8s.V1Container], client_containers: list[k8s.V1Container]
    ) -> list[k8s.V1Container]:
        if not base_containers:
            return client_containers
        if not client_containers:
            return base_containers

        client_container = client_containers[0]
        base_container = base_containers[0]
        client_container = extend_object_field(base_container, client_container, \""volume_mounts\"")
        client_container = extend_object_field(base_container, client_container, \""env\"")
        client_container = extend_object_field(base_container, client_container, \""env_from\"")
        client_container = extend_object_field(base_container, client_container, \""ports\"")
        client_container = extend_object_field(base_container, client_container, \""volume_devices\"")
        client_container = merge_objects(base_container, client_container)

        return [
            client_container,
            *PodGenerator.reconcile_containers(base_containers[1:], client_containers[1:]),
        ]

    @classmethod
    def construct_pod(
        cls,
        dag_id: str,
        task_id: str,
        pod_id: str,
        try_number: int,
        kube_image: str,
        date: datetime.datetime | None,
        args: list[str],
        pod_override_object: k8s.V1Pod | None,
        base_worker_pod: k8s.V1Pod,
        namespace: str,
        scheduler_job_id: str,
        run_id: str | None = None,
        map_index: int = -1,
        *,
        with_mutation_hook: bool = False,
    ) -> k8s.V1Pod:
        if len(pod_id) > 253:
            warnings.warn(
                \""pod_id supplied is longer than 253 characters; truncating and adding unique suffix.\""
            )
            pod_id = add_pod_suffix(pod_name=pod_id, max_len=253)
        try:
            if not image:
                image = kube_image
        except Exception:
            image = kube_image

        annotations = {
            \""dag_id\"": dag_id,
            \""task_id\"": task_id,
            \""try_number\"": str(try_number),
        }
        if map_index >= 0:
            annotations[\""map_index\""] = str(map_index)
        if date:
            annotations[\""execution_date\""] = date.isoformat()
        if run_id:
            annotations[\""run_id\""] = run_id

        dynamic_pod = k8s.V1Pod(
            metadata=k8s.V1ObjectMeta(
                namespace=namespace,
                annotations=annotations,
                name=pod_id,
                labels=cls.build_labels_for_k8s_executor_pod(
                    dag_id=dag_id,
                    task_id=task_id,
                    try_number=try_number,
                    airflow_worker=scheduler_job_id,
                    map_index=map_index,
                    execution_date=date,
                    run_id=run_id,
                ),
            ),
            spec=k8s.V1PodSpec(
                containers=[
                    k8s.V1Container(
                        name=\""base\
,""                        args=args,
                        image=image,
                        env=[k8s.V1EnvVar(name=\""AIRFLOW_IS_K8S_EXECUTOR_POD\"", value=\""True\"")],
                    )
                ]
            ),
        )

        pod_list = [base_worker_pod, pod_override_object, dynamic_pod]

        try:
            pod = reduce(PodGenerator.reconcile_pods, pod_list)
        except Exception as e:
            raise PodReconciliationError from e

        if with_mutation_hook:
            from airflow.settings import pod_mutation_hook

            try:
                pod_mutation_hook(pod)
            except Exception as e:
                raise PodMutationHookException from e

        return pod

    @classmethod
    def build_selector_for_k8s_executor_pod(
        cls,
        *,
        dag_id,
        task_id,
        try_number,
        map_index=None,
        execution_date=None,
        run_id=None,
        airflow_worker=None,
    ):
        labels = cls.build_labels_for_k8s_executor_pod(
            dag_id=dag_id,
            task_id=task_id,
            try_number=try_number,
            map_index=map_index,
            execution_date=execution_date,
            run_id=run_id,
            airflow_worker=airflow_worker,
        )
        label_strings = [f\""{label_id}={label}\"" for label_id, label in sorted(labels.items())]
            label_strings.append(\""airflow-worker\"")
        selector = \"",\"".join(label_strings)
        return selector

    @classmethod
    def build_labels_for_k8s_executor_pod(
        cls,
        *,
        dag_id,
        task_id,
        try_number,
        airflow_worker=None,
        map_index=None,
        execution_date=None,
        run_id=None,
    ):
        labels = {
            \""dag_id\"": make_safe_label_value(dag_id),
            \""task_id\"": make_safe_label_value(task_id),
            \""try_number\"": str(try_number),
            \""kubernetes_executor\"": \""True\
,""            \""airflow_version\"": airflow_version.replace(\""+\"", \""-\""),
        }
        if airflow_worker is not None:
            labels[\""airflow-worker\""] = make_safe_label_value(str(airflow_worker))
        if map_index is not None and map_index >= 0:
            labels[\""map_index\""] = str(map_index)
        if execution_date:
            labels[\""execution_date\""] = datetime_to_label_safe_datestring(execution_date)
        if run_id:
            labels[\""run_id\""] = make_safe_label_value(run_id)
        return labels

    @staticmethod
    def serialize_pod(pod: k8s.V1Pod) -> dict:
        api_client = ApiClient()
        return api_client.sanitize_for_serialization(pod)

    @staticmethod
    def deserialize_model_file(path: str) -> k8s.V1Pod:
        if os.path.exists(path):
            with open(path) as stream:
                pod = yaml.safe_load(stream)
        else:
            pod = None
            log.warning(\""Model file %s does not exist\"", path)

        return PodGenerator.deserialize_model_dict(pod)

    @staticmethod
    def deserialize_model_dict(pod_dict: dict | None) -> k8s.V1Pod:
        api_client = ApiClient()
        return api_client._ApiClient__deserialize_model(pod_dict, k8s.V1Pod)

    @staticmethod
    def make_unique_pod_id(pod_id: str) -> str | None:
        warnings.warn(
            \""This function is deprecated. Use `add_pod_suffix` in `kubernetes_helper_functions`.\
,""            RemovedInAirflow3Warning,
        )

        if not pod_id:
            return None

        trimmed_pod_id = pod_id[:base_pod_id_len].rstrip(\""-.\"")
        return f\""{trimmed_pod_id}-{suffix}\""


def merge_objects(base_obj, client_obj):
    if not base_obj:
        return client_obj
    if not client_obj:
        return base_obj

    client_obj_cp = copy.deepcopy(client_obj)

    if isinstance(base_obj, dict) and isinstance(client_obj_cp, dict):
        base_obj_cp = copy.deepcopy(base_obj)
        base_obj_cp.update(client_obj_cp)
        return base_obj_cp

    for base_key in base_obj.to_dict().keys():
        base_val = getattr(base_obj, base_key, None)
        if not getattr(client_obj, base_key, None) and base_val:
            if not isinstance(client_obj_cp, dict):
                setattr(client_obj_cp, base_key, base_val)
            else:
                client_obj_cp[base_key] = base_val
    return client_obj_cp


def extend_object_field(base_obj, client_obj, field_name):
    client_obj_cp = copy.deepcopy(client_obj)
    base_obj_field = getattr(base_obj, field_name, None)
    client_obj_field = getattr(client_obj, field_name, None)

    if (not isinstance(base_obj_field, list) and base_obj_field is not None) or (
        not isinstance(client_obj_field, list) and client_obj_field is not None
    ):
        raise ValueError(\""The chosen field must be a list.\"")

    if not base_obj_field:
        return client_obj_cp
    if not client_obj_field:
        setattr(client_obj_cp, field_name, base_obj_field)
        return client_obj_cp

    appended_fields = base_obj_field + client_obj_field
    setattr(client_obj_cp, field_name, appended_fields)
    return client_obj_cp
","['getLogger', 'rand_str', 'join', 'choice', 'range', 'add_pod_suffix', 'len', 'strip', 'make_safe_label_value', 'sub', 'md5', 'encode', 'hexdigest', 'datetime_to_label_safe_datestring', 'isoformat', 'replace', 'label_safe_datestring_to_datetime', 'parse', '__init__', 'AirflowConfigException', 'deserialize_model_file', 'gen_pod', 'warn', 'add_xcom_sidecar', 'deepcopy', 'insert', 'append', 'from_obj', 'get', 'isinstance', 'from_legacy_obj', 'TypeError', 'pop', 'list', 'values', 'all', 'items', 'V1ResourceRequirements', 'PodGeneratorDeprecated', 'reconcile_pods', 'reconcile_specs', 'reconcile_metadata', 'merge_objects', 'extend_object_field', 'reconcile_containers', 'construct_pod', 'str', 'V1Pod', 'V1ObjectMeta', 'build_labels_for_k8s_executor_pod', 'V1PodSpec', 'V1Container', 'V1EnvVar', 'reduce', 'pod_mutation_hook', 'build_selector_for_k8s_executor_pod', 'sorted', 'serialize_pod', 'ApiClient', 'sanitize_for_serialization', 'exists', 'open', 'safe_load', 'warning', 'deserialize_model_dict', '_ApiClient__deserialize_model', 'make_unique_pod_id', 'rstrip', 'update', 'to_dict', 'keys', 'getattr', 'setattr', 'ValueError']"
"https://github.com/openstack/nova/blob/a507b12df11ee743f9874812b1525c53b295c655/nova/crypto.py
","

import base64
import binascii
import io
import os
import typing as ty

from castellan.common import exception as castellan_exception
from castellan.common.objects import passphrase
from castellan import key_manager
from cryptography.hazmat import backends
from cryptography.hazmat.primitives.asymmetric import padding
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives import serialization
from cryptography import x509
from oslo_concurrency import processutils
from oslo_log import log as logging
from oslo_utils.secretutils import md5
import paramiko

import nova.conf
from nova import context as nova_context
from nova import exception
from nova.i18n import _
from nova import objects
from nova import utils


LOG = logging.getLogger(__name__)

CONF = nova.conf.CONF

_KEYMGR = None

_VTPM_SECRET_BYTE_LENGTH = 384


def _get_key_manager():
    global _KEYMGR
    if _KEYMGR is None:
        _KEYMGR = key_manager.API(configuration=CONF)
    return _KEYMGR


def generate_fingerprint(public_key: str) -> str:
    try:
        pub_bytes = public_key.encode('utf-8')
        serialization.load_ssh_public_key(
            pub_bytes, backends.default_backend())
        pub_data = base64.b64decode(public_key.split(' ')[1])
        raw_fp = md5(pub_data, usedforsecurity=False).hexdigest()
        return ':'.join(a + b for a, b in zip(raw_fp[::2], raw_fp[1::2]))
    except Exception:
        raise exception.InvalidKeypair(
            reason=_('failed to generate fingerprint'))


def generate_x509_fingerprint(pem_key: ty.Union[bytes, str]) -> str:
    try:
        if isinstance(pem_key, str):
            pem_key = pem_key.encode('utf-8')
        cert = x509.load_pem_x509_certificate(
            pem_key, backends.default_backend())
        raw_fp = binascii.hexlify(
            cert.fingerprint(hashes.SHA1())
        ).decode('ascii')
        return ':'.join(a + b for a, b in zip(raw_fp[::2], raw_fp[1::2]))
    except (ValueError, TypeError, binascii.Error) as ex:
        raise exception.InvalidKeypair(
            reason=_('failed to generate X509 fingerprint. '
                     'Error message: %s') % ex)


def generate_key_pair(bits: int = 2048) -> ty.Tuple[str, str, str]:
    key = paramiko.RSAKey.generate(bits)
    keyout = io.StringIO()
    key.write_private_key(keyout)
    private_key = keyout.getvalue()
    public_key = '%s %s Generated-by-Nova' % (key.get_name(), key.get_base64())
    fingerprint = generate_fingerprint(public_key)
    return (private_key, public_key, fingerprint)


def ssh_encrypt_text(ssh_public_key: str, text: ty.Union[str, bytes]) -> bytes:
    if isinstance(text, str):
        text = text.encode('utf-8')
    try:
        pub_bytes = ssh_public_key.encode('utf-8')
        pub_key = serialization.load_ssh_public_key(
            pub_bytes, backends.default_backend())
        return pub_key.encrypt(text, padding.PKCS1v15())
    except Exception as exc:
        raise exception.EncryptionFailure(reason=str(exc))


def generate_winrm_x509_cert(
    user_id: str,
    bits: int = 2048
) -> ty.Tuple[str, str, str]:
    subject = '/CN=%s' % user_id
    upn = '%s@localhost' % user_id

    with utils.tempdir() as tmpdir:
        keyfile = os.path.abspath(os.path.join(tmpdir, 'temp.key'))
        conffile = os.path.abspath(os.path.join(tmpdir, 'temp.conf'))

        _create_x509_openssl_config(conffile, upn)

        out, _ = processutils.execute(
            'openssl', 'req', '-x509', '-nodes', '-days', '3650',
            '-config', conffile, '-newkey', 'rsa:%s' % bits,
            '-outform', 'PEM', '-keyout', keyfile, '-subj', subject,
            '-extensions', 'v3_req_client',
            binary=True)

        certificate = out.decode('utf-8')

        out, _ = processutils.execute(
            'openssl', 'pkcs12', '-export', '-inkey', keyfile, '-password',
            'pass:', process_input=out, binary=True)

        private_key = base64.b64encode(out).decode('ascii')
        fingerprint = generate_x509_fingerprint(certificate)

    return (private_key, certificate, fingerprint)


def _create_x509_openssl_config(conffile: str, upn: str):
    content = (\""distinguished_name  = req_distinguished_name\\n\""
               \""[req_distinguished_name]\\n\""
               \""[v3_req_client]\\n\""
               \""extendedKeyUsage = clientAuth\\n\""
               \""subjectAltName = otherName:\""\""1.3.6.1.4.1.311.20.2.3;UTF8:%s\\n\"")

    with open(conffile, 'w') as file:
        file.write(content % upn)


def ensure_vtpm_secret(
    context: nova_context.RequestContext,
    instance: 'objects.Instance',
) -> ty.Tuple[str, str]:
    key_mgr = _get_key_manager()

    secret_uuid = instance.system_metadata.get('vtpm_secret_uuid')
    if secret_uuid is not None:
        try:
            secret = key_mgr.get(context, secret_uuid)
            LOG.debug(
                \""Found existing vTPM secret with UUID %s.\
,""                secret_uuid, instance=instance)
            return secret.id, secret.get_encoded()
        except castellan_exception.ManagedObjectNotFoundError:
            LOG.warning(
                \""Despite being set on the instance, failed to find a vTPM \""
                \""secret with UUID %s. This should only happen if the secret \""
                \""was manually deleted from the key manager service. Your vTPM \""
                \""is likely to be unrecoverable.\
,""                secret_uuid, instance=instance)
            raise

    secret = base64.b64encode(os.urandom(_VTPM_SECRET_BYTE_LENGTH))
    cmo = passphrase.Passphrase(
        secret, name=\""vTPM secret for instance %s\"" % instance.uuid)
    secret_uuid = key_mgr.store(context, cmo)
    LOG.debug(\""Created vTPM secret with UUID %s\
,""              secret_uuid, instance=instance)

    instance.system_metadata['vtpm_secret_uuid'] = secret_uuid
    instance.save()
    return secret_uuid, secret


def delete_vtpm_secret(
    context: nova_context.RequestContext,
    instance: 'objects.Instance',
):
    secret_uuid = instance.system_metadata.get('vtpm_secret_uuid')
    if not secret_uuid:
        return

    key_mgr = _get_key_manager()
    try:
        key_mgr.delete(context, secret_uuid)
        LOG.debug(\""Deleted vTPM secret with UUID %s\
,""                  secret_uuid, instance=instance)
    except castellan_exception.ManagedObjectNotFoundError:
        LOG.debug(\""vTPM secret with UUID %s already deleted or never existed.\
,""                  secret_uuid, instance=instance)

    del instance.system_metadata['vtpm_secret_uuid']
    instance.save()
","['getLogger', '_get_key_manager', 'API', 'generate_fingerprint', 'encode', 'load_ssh_public_key', 'default_backend', 'b64decode', 'split', 'md5', 'hexdigest', 'join', 'zip', 'InvalidKeypair', '_', 'generate_x509_fingerprint', 'isinstance', 'load_pem_x509_certificate', 'hexlify', 'fingerprint', 'SHA1', 'decode', 'generate_key_pair', 'generate', 'StringIO', 'write_private_key', 'getvalue', 'get_name', 'get_base64', 'ssh_encrypt_text', 'encrypt', 'PKCS1v15', 'EncryptionFailure', 'str', 'generate_winrm_x509_cert', 'tempdir', 'abspath', '_create_x509_openssl_config', 'execute', 'b64encode', 'open', 'write', 'ensure_vtpm_secret', 'get', 'debug', 'get_encoded', 'warning', 'urandom', 'Passphrase', 'store', 'save', 'delete_vtpm_secret', 'delete']"
"https://github.com/beetbox/beets/blob/293c3c19a1823ca042640d07e8faf4c74cf92401/beetsplug/thumbnails.py
","


import ctypes
import ctypes.util
import os
import shutil
from hashlib import md5
from pathlib import PurePosixPath

from xdg import BaseDirectory

from beets import util
from beets.plugins import BeetsPlugin
from beets.ui import Subcommand, decargs
from beets.util import bytestring_path, displayable_path, syspath
from beets.util.artresizer import ArtResizer

BASE_DIR = os.path.join(BaseDirectory.xdg_cache_home, \""thumbnails\"")
NORMAL_DIR = bytestring_path(os.path.join(BASE_DIR, \""normal\""))
LARGE_DIR = bytestring_path(os.path.join(BASE_DIR, \""large\""))


class ThumbnailsPlugin(BeetsPlugin):
    def __init__(self):
        super().__init__()
        self.config.add(
            {
                \""auto\"": True,
                \""force\"": False,
                \""dolphin\"": False,
            }
        )

        if self.config[\""auto\""] and self._check_local_ok():
            self.register_listener(\""art_set\"", self.process_album)

    def commands(self):
        thumbnails_command = Subcommand(
            \""thumbnails\"", help=\""Create album thumbnails\""
        )
        thumbnails_command.parser.add_option(
            \""-f\
,""            \""--force\
,""            dest=\""force\
,""            action=\""store_true\
,""            default=False,
            help=\""force regeneration of thumbnails deemed fine (existing & \""
            \""recent enough)\
,""        )
        thumbnails_command.parser.add_option(
            \""--dolphin\
,""            dest=\""dolphin\
,""            action=\""store_true\
,""            default=False,
            help=\""create Dolphin-compatible thumbnail information (for KDE)\
,""        )
        thumbnails_command.func = self.process_query

        return [thumbnails_command]

    def process_query(self, lib, opts, args):
        self.config.set_args(opts)
        if self._check_local_ok():
            for album in lib.albums(decargs(args)):
                self.process_album(album)

    def _check_local_ok(self):
        if not ArtResizer.shared.local:
            self._log.warning(
                \""No local image resizing capabilities, \""
                \""cannot generate thumbnails\""
            )
            return False

        for dir in (NORMAL_DIR, LARGE_DIR):
            if not os.path.exists(syspath(dir)):
                os.makedirs(syspath(dir))

        if not ArtResizer.shared.can_write_metadata:
            raise RuntimeError(
                f\""Thumbnails: ArtResizer backend {ArtResizer.shared.method}\""
                f\"" unexpectedly cannot write image metadata.\""
            )
        self._log.debug(f\""using {ArtResizer.shared.method} to write metadata\"")

        uri_getter = GioURI()
        if not uri_getter.available:
            uri_getter = PathlibURI()
        self._log.debug(\""using {0.name} to compute URIs\"", uri_getter)
        self.get_uri = uri_getter.uri

        return True

    def process_album(self, album):
        self._log.debug(\""generating thumbnail for {0}\"", album)
        if not album.artpath:
            self._log.info(\""album {0} has no art\"", album)
            return

        if self.config[\""dolphin\""]:
            self.make_dolphin_cover_thumbnail(album)

        size = ArtResizer.shared.get_size(album.artpath)
        if not size:
            self._log.warning(
                \""problem getting the picture size for {0}\"", album.artpath
            )
            return

        wrote = True
        if max(size) >= 256:
            wrote &= self.make_cover_thumbnail(album, 256, LARGE_DIR)
        wrote &= self.make_cover_thumbnail(album, 128, NORMAL_DIR)

        if wrote:
            self._log.info(\""wrote thumbnail for {0}\"", album)
        else:
            self._log.info(\""nothing to do for {0}\"", album)

    def make_cover_thumbnail(self, album, size, target_dir):
        target = os.path.join(target_dir, self.thumbnail_file_name(album.path))

        if (
            os.path.exists(syspath(target))
            and os.stat(syspath(target)).st_mtime
            > os.stat(syspath(album.artpath)).st_mtime
        ):
            if self.config[\""force\""]:
                self._log.debug(
                    \""found a suitable {1}x{1} thumbnail for {0}, \""
                    \""forcing regeneration\
,""                    album,
                    size,
                )
            else:
                self._log.debug(
                    \""{1}x{1} thumbnail for {0} exists and is \"" \""recent enough\
,""                    album,
                    size,
                )
                return False
        resized = ArtResizer.shared.resize(size, album.artpath, target)
        self.add_tags(album, resized)
        shutil.move(syspath(resized), syspath(target))
        return True

    def thumbnail_file_name(self, path):
        uri = self.get_uri(path)
        hash = md5(uri.encode(\""utf-8\"")).hexdigest()
        return bytestring_path(f\""{hash}.png\"")

    def add_tags(self, album, image_path):
        mtime = os.stat(syspath(album.artpath)).st_mtime
        metadata = {
            \""Thumb::URI\"": self.get_uri(album.artpath),
            \""Thumb::MTime\"": str(mtime),
        }
        try:
            ArtResizer.shared.write_metadata(image_path, metadata)
        except Exception:
            self._log.exception(
                \""could not write metadata to {0}\"", displayable_path(image_path)
            )

    def make_dolphin_cover_thumbnail(self, album):
        outfilename = os.path.join(album.path, b\"".directory\"")
        if os.path.exists(syspath(outfilename)):
            return
        artfile = os.path.split(album.artpath)[1]
        with open(syspath(outfilename), \""w\"") as f:
            f.write(\""[Desktop Entry]\\n\"")
            f.write(\""Icon=./{}\"".format(artfile.decode(\""utf-8\"")))
            f.close()
        self._log.debug(\""Wrote file {0}\"", displayable_path(outfilename))


class URIGetter:
    available = False
    name = \""Abstract base\""

    def uri(self, path):
        raise NotImplementedError()


class PathlibURI(URIGetter):
    available = True
    name = \""Python Pathlib\""

    def uri(self, path):
        return PurePosixPath(util.py3_path(path)).as_uri()


def copy_c_string(c_string):
    s = ctypes.cast(c_string, ctypes.c_char_p).value
    return b\""\"" + s


class GioURI(URIGetter):

    name = \""GIO\""

    def __init__(self):
        self.libgio = self.get_library()
        self.available = bool(self.libgio)
        if self.available:

            self.libgio.g_file_get_uri.argtypes = [ctypes.c_char_p]
            self.libgio.g_file_new_for_path.restype = ctypes.c_void_p

            self.libgio.g_file_get_uri.argtypes = [ctypes.c_void_p]
            self.libgio.g_file_get_uri.restype = ctypes.POINTER(ctypes.c_char)

            self.libgio.g_object_unref.argtypes = [ctypes.c_void_p]

    def get_library(self):
        lib_name = ctypes.util.find_library(\""gio-2\"")
        try:
            if not lib_name:
                return False
            return ctypes.cdll.LoadLibrary(lib_name)
        except OSError:
            return False

    def uri(self, path):
        g_file_ptr = self.libgio.g_file_new_for_path(path)
        if not g_file_ptr:
            raise RuntimeError(
                \""No gfile pointer received for {}\"".format(
                    displayable_path(path)
                )
            )

        try:
            uri_ptr = self.libgio.g_file_get_uri(g_file_ptr)
        finally:
            self.libgio.g_object_unref(g_file_ptr)
        if not uri_ptr:
            self.libgio.g_free(uri_ptr)
            raise RuntimeError(
                \""No URI received from the gfile pointer for \""
                \""{}\"".format(displayable_path(path))
            )

        try:
            uri = copy_c_string(uri_ptr)
        finally:
            self.libgio.g_free(uri_ptr)

        try:
            return uri.decode(util._fsencoding())
        except UnicodeDecodeError:
            raise RuntimeError(f\""Could not decode filename from GIO: {uri!r}\"")
","['join', 'bytestring_path', 'ThumbnailsPlugin', '__init__', 'super', 'add', '_check_local_ok', 'register_listener', 'commands', 'Subcommand', 'add_option', 'process_query', 'set_args', 'albums', 'decargs', 'process_album', 'warning', 'exists', 'syspath', 'makedirs', 'RuntimeError', 'debug', 'GioURI', 'PathlibURI', 'info', 'make_dolphin_cover_thumbnail', 'get_size', 'max', 'make_cover_thumbnail', 'thumbnail_file_name', 'stat', 'resize', 'add_tags', 'move', 'get_uri', 'md5', 'encode', 'hexdigest', 'str', 'write_metadata', 'exception', 'displayable_path', 'split', 'open', 'write', 'format', 'decode', 'close', 'uri', 'NotImplementedError', 'PurePosixPath', 'py3_path', 'as_uri', 'copy_c_string', 'cast', 'get_library', 'bool', 'POINTER', 'find_library', 'LoadLibrary', 'g_file_new_for_path', 'g_file_get_uri', 'g_object_unref', 'g_free', '_fsencoding']"
"https://github.com/apache/airflow/blob/aaed909344b12aa4691a9e23ea9f9c98d641d853/airflow/operators/python.py
"," __future__ import annotations

import fcntl
import importlib
import inspect
import json
import logging
import os
import pickle
import shutil
import subprocess
import sys
import textwrap
import types
import warnings
from abc import ABCMeta, abstractmethod
from collections.abc import Container
from pathlib import Path
from tempfile import TemporaryDirectory
from typing import TYPE_CHECKING, Any, Callable, Collection, Iterable, Mapping, Sequence, cast

import dill

from airflow.exceptions import (
    AirflowConfigException,
    AirflowException,
    AirflowSkipException,
    DeserializingResultError,
    RemovedInAirflow3Warning,
)
from airflow.models.baseoperator import BaseOperator
from airflow.models.skipmixin import SkipMixin
from airflow.models.taskinstance import _CURRENT_CONTEXT
from airflow.models.variable import Variable
from airflow.operators.branch import BranchMixIn
from airflow.utils import hashlib_wrapper
from airflow.utils.context import context_copy_partial, context_merge
from airflow.utils.operator_helpers import KeywordParameters
from airflow.utils.process_utils import execute_in_subprocess
from airflow.utils.python_virtualenv import prepare_virtualenv, write_python_script

if TYPE_CHECKING:
    from pendulum.datetime import DateTime

    from airflow.utils.context import Context


def is_venv_installed() -> bool:
    if shutil.which(\""virtualenv\"") or importlib.util.find_spec(\""virtualenv\""):
        return True
    return False


def task(python_callable: Callable | None = None, multiple_outputs: bool | None = None, **kwargs):
    from airflow.decorators.python import python_task

    warnings.warn(
    Executes a Python callable.

    .. seealso::
        For more information on how to use this operator, take a look at the guide:
        :ref:`howto/operator:PythonOperator`

    When running your callable, Airflow will pass a set of keyword arguments that can be used in your
    function. This set of kwargs correspond exactly to what you can use in your jinja templates.
    For this to work, you need to define ``**kwargs`` in your function header, or you can add directly the
    keyword arguments you would like to get - for example with the below code your callable will get
    the values of ``ti`` and ``next_ds`` context variables.

    With explicit arguments:

    .. code-block:: python

       def my_python_callable(ti, next_ds):
           pass

    With kwargs:

    .. code-block:: python

       def my_python_callable(**kwargs):
           ti = kwargs[\""ti\""]
           next_ds = kwargs[\""next_ds\""]


    :param python_callable: A reference to an object that is callable
    :param op_kwargs: a dictionary of keyword arguments that will get unpacked
        in your function
    :param op_args: a list of positional arguments that will get unpacked when
        calling your callable
    :param templates_dict: a dictionary where the values are templates that
        will get templated by the Airflow engine sometime between
        ``__init__`` and ``execute`` takes place and are made available
        in your callable's context after the template has been applied. (templated)
    :param templates_exts: a list of file extensions to resolve while
        processing templated fields, for examples ``['.sql', '.hql']``
    :param show_return_value_in_logs: a bool value whether to show return_value
        logs. Defaults to True, which allows return value log output.
        It can be set to False to prevent log output of return value when you return huge data
        such as transmission a large amount of XCom to TaskAPI.
        Call the python callable with the given arguments.

        :return: the return value of the call.
    A workflow can \""branch\"" or follow a path after the execution of this task.

    It derives the PythonOperator and expects a Python function that returns
    a single task_id or list of task_ids to follow. The task_id(s) returned
    should point to a task directly downstream from {self}. All other \""branches\""
    or directly downstream tasks are marked with a state of ``skipped`` so that
    these paths can't move forward. The ``skipped`` states are propagated
    downstream to allow for the DAG state to fill up and the DAG run's state
    to be inferred.
    Allows a pipeline to continue based on the result of a ``python_callable``.

    The ShortCircuitOperator is derived from the PythonOperator and evaluates the result of a
    ``python_callable``. If the returned result is False or a falsy value, the pipeline will be
    short-circuited. Downstream tasks will be marked with a state of \""skipped\"" based on the short-circuiting
    mode configured. If the returned result is True or a truthy value, downstream tasks proceed as normal and
    an ``XCom`` of the returned result is pushed.

    The short-circuiting can be configured to either respect or ignore the ``trigger_rule`` set for
    downstream tasks. If ``ignore_downstream_trigger_rules`` is set to True, the default setting, all
    downstream tasks are skipped without considering the ``trigger_rule`` defined for tasks. However, if this
    parameter is set to False, the direct downstream tasks are skipped but the specified ``trigger_rule`` for
    other subsequent downstream tasks are respected. In this mode, the operator assumes the direct downstream
    tasks were purposely meant to be skipped but perhaps not other subsequent tasks.

    .. seealso::
        For more information on how to use this operator, take a look at the guide:
        :ref:`howto/operator:ShortCircuitOperator`

    :param ignore_downstream_trigger_rules: If set to True, all downstream tasks from this operator task will
        be skipped. This is the default behavior. If set to False, the direct, downstream task(s) will be
        skipped but the ``trigger_rule`` defined for all other downstream tasks will be respected.
        return textwrap.dedent(inspect.getsource(self.python_callable))

    def _write_args(self, file: Path):
        if self.op_args or self.op_kwargs:
            file.write_bytes(self.pickling_library.dumps({\""args\"": self.op_args, \""kwargs\"": self.op_kwargs}))

    def _write_string_args(self, file: Path):
        file.write_text(\""\\n\"".join(map(str, self.string_args)))

    def _read_result(self, path: Path):
        if path.stat().st_size == 0:
            return None
        try:
            return self.pickling_library.loads(path.read_bytes())
        except ValueError as value_error:
            raise DeserializingResultError() from value_error

    def __deepcopy__(self, memo):
        memo[id(self.pickling_library)] = self.pickling_library
        return super().__deepcopy__(memo)

    def _execute_python_callable_in_subprocess(self, python_path: Path):
        with TemporaryDirectory(prefix=\""venv-call\"") as tmp:
            tmp_dir = Path(tmp)
            op_kwargs: dict[str, Any] = dict(self.op_kwargs)
            if self.templates_dict:
                op_kwargs[\""templates_dict\""] = self.templates_dict
            input_path = tmp_dir / \""script.in\""
            output_path = tmp_dir / \""script.out\""
            string_args_path = tmp_dir / \""string_args.txt\""
            script_path = tmp_dir / \""script.py\""
            termination_log_path = tmp_dir / \""termination.log\""

            self._write_args(input_path)
            self._write_string_args(string_args_path)
            write_python_script(
                jinja_context={
                    \""op_args\"": self.op_args,
                    \""op_kwargs\"": op_kwargs,
                    \""expect_airflow\"": self.expect_airflow,
                    \""pickling_library\"": self.pickling_library.__name__,
                    \""python_callable\"": self.python_callable.__name__,
                    \""python_callable_source\"": self.get_python_source(),
                },
                filename=os.fspath(script_path),
                render_template_as_native_obj=self.dag.render_template_as_native_obj,
            )

            try:
                execute_in_subprocess(
                    cmd=[
                        os.fspath(python_path),
                        os.fspath(script_path),
                        os.fspath(input_path),
                        os.fspath(output_path),
                        os.fspath(string_args_path),
                        os.fspath(termination_log_path),
                    ]
                )
            except subprocess.CalledProcessError as e:
                if e.returncode in self.skip_on_exit_code:
                    raise AirflowSkipException(f\""Process exited with code {e.returncode}. Skipping.\"")
                elif termination_log_path.exists() and termination_log_path.stat().st_size > 0:
                    error_msg = f\""Process returned non-zero exit status {e.returncode}.\\n\""
                    with open(termination_log_path) as file:
                        error_msg += file.read()
                    raise AirflowException(error_msg) from None
                else:
                    raise

            return self._read_result(output_path)

    def determine_kwargs(self, context: Mapping[str, Any]) -> Mapping[str, Any]:
        return KeywordParameters.determine(self.python_callable, self.op_args, context).serializing()


class PythonVirtualenvOperator(_BasePythonVirtualenvOperator):

    template_fields: Sequence[str] = tuple(
        {\""requirements\"", \""index_urls\"", \""venv_cache_path\""}.union(PythonOperator.template_fields)
    )
    template_ext: Sequence[str] = (\"".txt\"",)

    def __init__(
        self,
        *,
        python_callable: Callable,
        requirements: None | Iterable[str] | str = None,
        python_version: str | None = None,
        use_dill: bool = False,
        system_site_packages: bool = True,
        pip_install_options: list[str] | None = None,
        op_args: Collection[Any] | None = None,
        op_kwargs: Mapping[str, Any] | None = None,
        string_args: Iterable[str] | None = None,
        templates_dict: dict | None = None,
        templates_exts: list[str] | None = None,
        expect_airflow: bool = True,
        skip_on_exit_code: int | Container[int] | None = None,
        index_urls: None | Collection[str] | str = None,
        venv_cache_path: None | os.PathLike[str] = None,
        **kwargs,
    ):
        if (
            python_version
            and str(python_version)[0] != str(sys.version_info.major)
            and (op_args or op_kwargs)
        ):
            raise AirflowException(
                \""Passing op_args or op_kwargs is not supported across different Python \""
                \""major versions for PythonVirtualenvOperator. Please use string_args.\""
                f\""Sys version: {sys.version_info}. Virtual environment version: {python_version}\""
            )
        if python_version is not None and not isinstance(python_version, str):
            warnings.warn(
                \""Passing non-string types (e.g. int or float) as python_version \""
                \""is deprecated. Please use string value instead.\
,""                RemovedInAirflow3Warning,
                stacklevel=2,
            )
        if not is_venv_installed():
            raise AirflowException(\""PythonVirtualenvOperator requires virtualenv, please install it.\"")
        if not requirements:
            self.requirements: list[str] = []
        elif isinstance(requirements, str):
            self.requirements = [requirements]
        else:
            self.requirements = list(requirements)
        self.python_version = python_version
        self.system_site_packages = system_site_packages
        self.pip_install_options = pip_install_options
        if isinstance(index_urls, str):
            self.index_urls: list[str] | None = [index_urls]
        elif isinstance(index_urls, Collection):
            self.index_urls = list(index_urls)
        else:
            self.index_urls = None
        self.venv_cache_path = venv_cache_path
        super().__init__(
            python_callable=python_callable,
            use_dill=use_dill,
            op_args=op_args,
            op_kwargs=op_kwargs,
            string_args=string_args,
            templates_dict=templates_dict,
            templates_exts=templates_exts,
            expect_airflow=expect_airflow,
            skip_on_exit_code=skip_on_exit_code,
            **kwargs,
        )

    def _requirements_list(self) -> list[str]:
        requirements = [str(dependency) for dependency in self.requirements]
        if not self.system_site_packages and self.use_dill and \""dill\"" not in requirements:
            requirements.append(\""dill\"")
        return requirements

    def _prepare_venv(self, venv_path: Path) -> None:
        requirements_file = venv_path / \""requirements.txt\""
        requirements_file.write_text(\""\\n\"".join(self._requirements_list()))
        prepare_virtualenv(
            venv_directory=str(venv_path),
            python_bin=f\""python{self.python_version}\"" if self.python_version else \""python\
,""            system_site_packages=self.system_site_packages,
            requirements_file_path=str(requirements_file),
            pip_install_options=self.pip_install_options,
            index_urls=self.index_urls,
        )

    def _calculate_cache_hash(self) -> tuple[str, str]:
        hash_dict = {
            \""requirements_list\"": self._requirements_list(),
            \""pip_install_options\"": self.pip_install_options,
            \""index_urls\"": self.index_urls,
            \""cache_key\"": str(Variable.get(\""PythonVirtualenvOperator.cache_key\"", \""\"")),
            \""python_version\"": self.python_version,
            \""system_site_packages\"": self.system_site_packages,
        }
        hash_text = json.dumps(hash_dict, sort_keys=True)
        hash_object = hashlib_wrapper.md5(hash_text.encode())
        requirements_hash = hash_object.hexdigest()
        return requirements_hash[:8], hash_text

    def _ensure_venv_cache_exists(self, venv_cache_path: Path) -> Path:
        cache_hash, hash_data = self._calculate_cache_hash()
        venv_path = venv_cache_path / f\""venv-{cache_hash}\""
        self.log.info(\""Python virtual environment will be cached in %s\"", venv_path)
        venv_path.parent.mkdir(parents=True, exist_ok=True)
        with open(f\""{venv_path}.lock\"", \""w\"") as f:
            fcntl.flock(f, fcntl.LOCK_EX)

            hash_marker = venv_path / \""install_complete_marker.json\""
            try:
                if venv_path.exists():
                    if hash_marker.exists():
                        previous_hash_data = hash_marker.read_text(encoding=\""utf8\"")
                        if previous_hash_data == hash_data:
                            self.log.info(\""Re-using cached Python virtual environment in %s\"", venv_path)
                            return venv_path

                        self.log.error(
                            \""Unicorn alert: Found a previous virtual environment in %s \""
                            \""with the same hash but different parameters. Previous setup: '%s' / \""
                            \""Requested venv setup: '%s'. Please report a bug to airflow!\
,""                            venv_path,
                            previous_hash_data,
                            hash_data,
                        )
                    else:
                        self.log.warning(
                            \""Found a previous (probably partial installed) virtual environment in %s, \""
                            \""deleting and re-creating.\
,""                            venv_path,
                        )

                    shutil.rmtree(venv_path)

                venv_path.mkdir(parents=True)
                self._prepare_venv(venv_path)
                hash_marker.write_text(hash_data, encoding=\""utf8\"")
            except Exception as e:
                shutil.rmtree(venv_path)
                raise AirflowException(f\""Unable to create new virtual environment in {venv_path}\"") from e
            self.log.info(\""New Python virtual environment created in %s\"", venv_path)
            return venv_path

    def execute_callable(self):
        if self.venv_cache_path:
            venv_path = self._ensure_venv_cache_exists(Path(self.venv_cache_path))
            python_path = venv_path / \""bin\"" / \""python\""
            return self._execute_python_callable_in_subprocess(python_path)

        with TemporaryDirectory(prefix=\""venv\"") as tmp_dir:
            tmp_path = Path(tmp_dir)
            self._prepare_venv(tmp_path)
            python_path = tmp_path / \""bin\"" / \""python\""
            result = self._execute_python_callable_in_subprocess(python_path)
            return result

    def _iter_serializable_context_keys(self):
        yield from self.BASE_SERIALIZABLE_CONTEXT_KEYS
        if self.system_site_packages or \""apache-airflow\"" in self.requirements:
            yield from self.AIRFLOW_SERIALIZABLE_CONTEXT_KEYS
            yield from self.PENDULUM_SERIALIZABLE_CONTEXT_KEYS
        elif \""pendulum\"" in self.requirements:
            yield from self.PENDULUM_SERIALIZABLE_CONTEXT_KEYS


class BranchPythonVirtualenvOperator(PythonVirtualenvOperator, BranchMixIn):

    def execute(self, context: Context) -> Any:
        return self.do_branch(context, super().execute(context))


class ExternalPythonOperator(_BasePythonVirtualenvOperator):

    template_fields: Sequence[str] = tuple({\""python\""}.union(PythonOperator.template_fields))

    def __init__(
        self,
        *,
        python: str,
        python_callable: Callable,
        use_dill: bool = False,
        op_args: Collection[Any] | None = None,
        op_kwargs: Mapping[str, Any] | None = None,
        string_args: Iterable[str] | None = None,
        templates_dict: dict | None = None,
        templates_exts: list[str] | None = None,
        expect_airflow: bool = True,
        expect_pendulum: bool = False,
        skip_on_exit_code: int | Container[int] | None = None,
        **kwargs,
    ):
        if not python:
            raise ValueError(\""Python Path must be defined in ExternalPythonOperator\"")
        self.python = python
        self.expect_pendulum = expect_pendulum
        super().__init__(
            python_callable=python_callable,
            use_dill=use_dill,
            op_args=op_args,
            op_kwargs=op_kwargs,
            string_args=string_args,
            templates_dict=templates_dict,
            templates_exts=templates_exts,
            expect_airflow=expect_airflow,
            skip_on_exit_code=skip_on_exit_code,
            **kwargs,
        )

    def execute_callable(self):
        python_path = Path(self.python)
        if not python_path.exists():
            raise ValueError(f\""Python Path '{python_path}' must exists\"")
        if not python_path.is_file():
            raise ValueError(f\""Python Path '{python_path}' must be a file\"")
        if not python_path.is_absolute():
            raise ValueError(f\""Python Path '{python_path}' must be an absolute path.\"")
        python_version_as_list_of_strings = self._get_python_version_from_environment()
        if (
            python_version_as_list_of_strings
            and str(python_version_as_list_of_strings[0]) != str(sys.version_info.major)
            and (self.op_args or self.op_kwargs)
        ):
            raise AirflowException(
                \""Passing op_args or op_kwargs is not supported across different Python \""
                \""major versions for ExternalPythonOperator. Please use string_args.\""
                f\""Sys version: {sys.version_info}. \""
                f\""Virtual environment version: {python_version_as_list_of_strings}\""
            )
        return self._execute_python_callable_in_subprocess(python_path)

    def _get_python_version_from_environment(self) -> list[str]:
        try:
            result = subprocess.check_output([self.python, \""--version\""], text=True)
            return result.strip().split(\"" \"")[-1].split(\"".\"")
        except Exception as e:
            raise ValueError(f\""Error while executing {self.python}: {e}\"")

    def _iter_serializable_context_keys(self):
        yield from self.BASE_SERIALIZABLE_CONTEXT_KEYS
        if self._get_airflow_version_from_target_env():
            yield from self.AIRFLOW_SERIALIZABLE_CONTEXT_KEYS
            yield from self.PENDULUM_SERIALIZABLE_CONTEXT_KEYS
        elif self._is_pendulum_installed_in_target_env():
            yield from self.PENDULUM_SERIALIZABLE_CONTEXT_KEYS

    def _is_pendulum_installed_in_target_env(self) -> bool:
        try:
            subprocess.check_call([self.python, \""-c\"", \""import pendulum\""])
            return True
        except Exception as e:
            if self.expect_pendulum:
                self.log.warning(\""When checking for Pendulum installed in virtual environment got %s\"", e)
                self.log.warning(
                    \""Pendulum is not properly installed in the virtual environment \""
                    \""Pendulum context keys will not be available. \""
                    \""Please Install Pendulum or Airflow in your virtual environment to access them.\""
                )
            return False

    def _get_airflow_version_from_target_env(self) -> str | None:
        from airflow import __version__ as airflow_version

        try:
            result = subprocess.check_output(
                [self.python, \""-c\"", \""from airflow import __version__; print(__version__)\""],
                text=True,
                env={**os.environ, \""_AIRFLOW__AS_LIBRARY\"": \""true\""},
            )
            target_airflow_version = result.strip()
            if target_airflow_version != airflow_version:
                raise AirflowConfigException(
                    f\""The version of Airflow installed for the {self.python}(\""
                    f\""{target_airflow_version}) is different than the runtime Airflow version: \""
                    f\""{airflow_version}. Make sure your environment has the same Airflow version \""
                    f\""installed as the Airflow runtime.\""
                )
            return target_airflow_version
        except Exception as e:
            if self.expect_airflow:
                self.log.warning(\""When checking for Airflow installed in virtual environment got %s\"", e)
                self.log.warning(
                    f\""This means that Airflow is not properly installed by  \""
                    f\""{self.python}. Airflow context keys will not be available. \""
                    f\""Please Install Airflow {airflow_version} in your environment to access them.\""
                )
            return None


class BranchExternalPythonOperator(ExternalPythonOperator, BranchMixIn):

    def execute(self, context: Context) -> Any:
        return self.do_branch(context, super().execute(context))


def get_current_context() -> Context:
    if not _CURRENT_CONTEXT:
        raise AirflowException(
            \""Current context was requested but no context was found! \""
            \""Are you running within an airflow task?\""
        )
    return _CURRENT_CONTEXT[-1]
","['is_venv_installed', 'which', 'find_spec', 'task', 'warn', 'my_python_callable', 'task_id', 'dedent', 'getsource', '_write_args', 'write_bytes', 'dumps', '_write_string_args', 'write_text', 'join', 'map', '_read_result', 'stat', 'loads', 'read_bytes', 'DeserializingResultError', '__deepcopy__', 'id', 'super', '_execute_python_callable_in_subprocess', 'TemporaryDirectory', 'Path', 'dict', 'write_python_script', 'get_python_source', 'fspath', 'execute_in_subprocess', 'AirflowSkipException', 'exists', 'open', 'read', 'AirflowException', 'determine_kwargs', 'determine', 'serializing', 'PythonVirtualenvOperator', 'tuple', 'union', '__init__', 'str', 'isinstance', 'list', '_requirements_list', 'append', '_prepare_venv', 'prepare_virtualenv', '_calculate_cache_hash', 'get', 'md5', 'encode', 'hexdigest', '_ensure_venv_cache_exists', 'info', 'mkdir', 'flock', 'read_text', 'error', 'warning', 'rmtree', 'execute_callable', '_iter_serializable_context_keys', 'BranchPythonVirtualenvOperator', 'execute', 'do_branch', 'ExternalPythonOperator', 'ValueError', 'is_file', 'is_absolute', '_get_python_version_from_environment', 'check_output', 'strip', 'split', '_get_airflow_version_from_target_env', '_is_pendulum_installed_in_target_env', 'check_call', 'AirflowConfigException', 'BranchExternalPythonOperator', 'get_current_context']"
"https://github.com/jumpserver/jumpserver/blob/ed92f10208115486934613513b52954291ed2c1e/apps/common/utils/encode.py
","rt base64
import hashlib
import json
import os
import re
import time
from io import StringIO

import paramiko
import sshpubkeys
from cryptography.hazmat.primitives import serialization
from django.conf import settings
from django.core.serializers.json import DjangoJSONEncoder
from itsdangerous import (
    TimedJSONWebSignatureSerializer, JSONWebSignatureSerializer,
    BadSignature, SignatureExpired
)
from six import string_types

from .http import http_date

UUID_PATTERN = re.compile(r'[0-9a-zA-Z\\-]{36}')


class Singleton(type):
    def __init__(cls, *args, **kwargs):
        cls.__instance = None
        super().__init__(*args, **kwargs)

    def __call__(cls, *args, **kwargs):
        if cls.__instance is None:
            cls.__instance = super().__call__(*args, **kwargs)
            return cls.__instance
        else:
            return cls.__instance


class Signer(metaclass=Singleton):

    def __init__(self, secret_key=None):
        self.secret_key = secret_key

    def sign(self, value):
        s = JSONWebSignatureSerializer(self.secret_key, algorithm_name='HS256')
        return s.dumps(value).decode()

    def unsign(self, value):
        if value is None:
            return value
        s = JSONWebSignatureSerializer(self.secret_key, algorithm_name='HS256')
        try:
            return s.loads(value)
        except BadSignature:
            return None

    def sign_t(self, value, expires_in=3600):
        s = TimedJSONWebSignatureSerializer(self.secret_key, expires_in=expires_in)
        return str(s.dumps(value), encoding=\""utf8\"")

    def unsign_t(self, value):
        s = TimedJSONWebSignatureSerializer(self.secret_key)
        try:
            return s.loads(value)
        except (BadSignature, SignatureExpired):
            return None


_supported_paramiko_ssh_key_types = (
    paramiko.RSAKey,
    paramiko.DSSKey,
    paramiko.Ed25519Key,
    paramiko.ECDSAKey,)


def ssh_key_string_to_obj(text, password=None):
    key = None
    for ssh_key_type in _supported_paramiko_ssh_key_types:
        try:
            key = ssh_key_type.from_private_key(StringIO(text), password=password)
            return key
        except paramiko.SSHException:
            pass
    if key is None:
        raise ValueError('Invalid private key')
    return key


def ssh_private_key_gen(private_key, password=None):
    if isinstance(private_key, bytes):
        private_key = private_key.decode(\""utf-8\"")
    if isinstance(private_key, string_types):
        private_key = ssh_key_string_to_obj(private_key, password=password)
    return private_key


def ssh_pubkey_gen(private_key=None, username='jumpserver', hostname='localhost', password=None):
    private_key = ssh_private_key_gen(private_key, password=password)
    if not isinstance(private_key, _supported_paramiko_ssh_key_types):
        raise IOError('Invalid private key')

    public_key = \""%(key_type)s %(key_content)s %(username)s@%(hostname)s\"" % {
        'key_type': private_key.get_name(),
        'key_content': private_key.get_base64(),
        'username': username,
        'hostname': hostname,
    }
    return public_key


def ssh_key_gen(length=2048, type='rsa', password=None, username='jumpserver', hostname=None):

    if hostname is None:
        hostname = os.uname()[1]

    f = StringIO()
    try:
        if type == 'rsa':
            private_key_obj = paramiko.RSAKey.generate(length)
        elif type == 'dsa':
            private_key_obj = paramiko.DSSKey.generate(length)
        else:
            raise IOError('SSH private key must be `rsa` or `dsa`')
        private_key_obj.write_private_key(f, password=password)
        private_key = f.getvalue()
        public_key = ssh_pubkey_gen(private_key_obj, username=username, hostname=hostname)
        return private_key, public_key
    except IOError:
        raise IOError('These is error when generate ssh key.')


def validate_ssh_private_key(text, password=None):
    key = parse_ssh_private_key_str(text, password=password)
    return bool(key)


def parse_ssh_private_key_str(text: bytes, password=None) -> str:
    private_key = _parse_ssh_private_key(text, password=password)
    if private_key is None:
        return \""\""
    private_key_bytes = private_key.private_bytes(
        serialization.Encoding.PEM,
        serialization.PrivateFormat.OpenSSH,
        serialization.NoEncryption()
    )
    return private_key_bytes.decode('utf-8')


def parse_ssh_public_key_str(text: bytes = \""\"", password=None) -> str:
    private_key = _parse_ssh_private_key(text, password=password)
    if private_key is None:
        return \""\""
    public_key_bytes = private_key.public_key().public_bytes(
        serialization.Encoding.OpenSSH,
        serialization.PublicFormat.OpenSSH,
    )
    return public_key_bytes.decode('utf-8')


def _parse_ssh_private_key(text, password=None):
    if not bool(password):
        password = None
    if isinstance(text, str):
        try:
            text = text.encode(\""utf-8\"")
        except UnicodeDecodeError:
            return None
    if isinstance(password, str):
        try:
            password = password.encode(\""utf-8\"")
        except UnicodeDecodeError:
            return None

    try:
        if is_openssh_format_key(text):
            return serialization.load_ssh_private_key(text, password=password)
        return serialization.load_pem_private_key(text, password=password)
    except (ValueError, TypeError):
        return None


def is_openssh_format_key(text: bytes):
    return text.startswith(b\""-----BEGIN OPENSSH PRIVATE KEY-----\"")


def validate_ssh_public_key(text):
    ssh = sshpubkeys.SSHKey(text)
    try:
        ssh.parse()
    except (sshpubkeys.InvalidKeyException, UnicodeDecodeError):
        return False
    except NotImplementedError as e:
        return False
    return True


def content_md5(data):
    if isinstance(data, str):
        data = hashlib.md5(data.encode('utf-8'))
    value = base64.b64encode(data.hexdigest().encode('utf-8'))
    return value.decode('utf-8')


def make_signature(access_key_secret, date=None):
    if isinstance(date, bytes):
        date = bytes.decode(date)
    if isinstance(date, int):
        date_gmt = http_date(date)
    elif date is None:
        date_gmt = http_date(int(time.time()))
    else:
        date_gmt = date

    data = str(access_key_secret) + \""\\n\"" + date_gmt
    return content_md5(data)


def encrypt_password(password, salt=None, algorithm='sha512'):
    from passlib.hash import sha512_crypt, des_crypt

    def sha512():
        return sha512_crypt.using(rounds=5000).hash(password, salt=salt)

    def des():
        return des_crypt.hash(password, salt=salt[:2])

    support_algorithm = {
        'sha512': sha512,
        'des': des
    }

    if isinstance(algorithm, str):
        algorithm = algorithm.lower()

    if algorithm not in support_algorithm.keys():
        algorithm = 'sha512'

    if password and support_algorithm[algorithm]:
        return support_algorithm[algorithm]()
    return None


def get_signer():
    s = Signer(settings.SECRET_KEY)
    return s


signer = get_signer()


def ensure_last_char_is_ascii(data):
    remain = ''


def data_to_json(data, sort_keys=True, indent=2, cls=None):
    if cls is None:
        cls = DjangoJSONEncoder
    return json.dumps(data, ensure_ascii=False, sort_keys=sort_keys, indent=indent, cls=cls)
","['compile', 'Singleton', '__init__', 'super', '__call__', 'Signer', 'sign', 'JSONWebSignatureSerializer', 'dumps', 'decode', 'unsign', 'loads', 'sign_t', 'TimedJSONWebSignatureSerializer', 'str', 'unsign_t', 'ssh_key_string_to_obj', 'from_private_key', 'StringIO', 'ValueError', 'ssh_private_key_gen', 'isinstance', 'ssh_pubkey_gen', 'IOError', 'get_name', 'get_base64', 'ssh_key_gen', 'uname', 'generate', 'write_private_key', 'getvalue', 'validate_ssh_private_key', 'parse_ssh_private_key_str', 'bool', '_parse_ssh_private_key', 'private_bytes', 'NoEncryption', 'parse_ssh_public_key_str', 'public_key', 'public_bytes', 'encode', 'is_openssh_format_key', 'load_ssh_private_key', 'load_pem_private_key', 'startswith', 'validate_ssh_public_key', 'SSHKey', 'parse', 'content_md5', 'md5', 'b64encode', 'hexdigest', 'make_signature', 'http_date', 'int', 'time', 'encrypt_password', 'sha512', 'using', 'hash', 'des', 'lower', 'keys', 'get_signer', 'ensure_last_char_is_ascii', 'data_to_json']"
"https://github.com/jumpserver/jumpserver/blob/ed92f10208115486934613513b52954291ed2c1e/apps/users/models/user.py
","rt base64
import datetime
import uuid
from typing import Callable

import sshpubkeys
from django.conf import settings
from django.contrib.auth.hashers import check_password
from django.contrib.auth.models import AbstractUser
from django.core.cache import cache
from django.db import models
from django.db.models import Count
from django.shortcuts import reverse
from django.utils import timezone
from django.utils.module_loading import import_string
from django.utils.translation import gettext_lazy as _
from rest_framework.exceptions import PermissionDenied

from common.db import fields, models as jms_models
from common.utils import (
    date_expired_default, get_logger, lazyproperty,
    random_string, bulk_create_with_signal
)
from orgs.utils import current_org
from rbac.const import Scope
from ..signals import (
    post_user_change_password, post_user_leave_org, pre_user_leave_org
)

__all__ = ['User', 'UserPasswordHistory']

logger = get_logger(__file__)


class AuthMixin:
    date_password_last_updated: datetime.datetime
    history_passwords: models.Manager
    need_update_password: bool
    public_key: str
    is_local: bool
    set_password: Callable
    save: Callable
    history_passwords: models.Manager
    sect_cache_tpl = 'user_sect_{}'
    id: str

    @property
    def password_raw(self):
        raise AttributeError('Password raw is not a readable attribute')

    @password_raw.setter
    def password_raw(self, password_raw_):
        self.set_password(password_raw_)

    def set_password(self, raw_password):
        if self.can_update_password():
            self.date_password_last_updated = timezone.now()
            post_user_change_password.send(self.__class__, user=self)
            super().set_password(raw_password)

    def set_public_key(self, public_key):
        if self.can_update_ssh_key():
            self.public_key = public_key
            self.save()

    def can_update_password(self):
        return self.is_local

    def can_update_ssh_key(self):
        return self.can_use_ssh_key_login()

    @staticmethod
    def can_use_ssh_key_login():
        return settings.TERMINAL_PUBLIC_KEY_AUTH

    def is_history_password(self, password):
        allow_history_password_count = settings.OLD_PASSWORD_HISTORY_LIMIT_COUNT
        history_passwords = self.history_passwords.all() \\
                                .order_by('-date_created')[:int(allow_history_password_count)]

        for history_password in history_passwords:
            if check_password(password, history_password.password):
                return True
        else:
            return False

    def is_public_key_valid(self):
        if self.public_key:
            return True
        return False

    @property
    def public_key_obj(self):
        class PubKey(object):
            def __getattr__(self, item):
                return ''

        if self.public_key:
            try:
                return sshpubkeys.SSHKey(self.public_key)
            except (TabError, TypeError):
                pass
        return PubKey()

    def get_public_key_comment(self):
        return self.public_key_obj.comment

    def get_public_key_hash_md5(self):
        if not callable(self.public_key_obj.hash_md5):
            return ''
        try:
            return self.public_key_obj.hash_md5()
        except:
            return ''

    def reset_password(self, new_password):
        self.set_password(new_password)
        self.need_update_password = False
        self.save()

    @property
    def date_password_expired(self):
        interval = settings.SECURITY_PASSWORD_EXPIRATION_TIME
        date_expired = self.date_password_last_updated + timezone.timedelta(
            days=int(interval))
        return date_expired

    @property
    def password_expired_remain_days(self):
        date_remain = self.date_password_expired - timezone.now()
        return date_remain.days

    @property
    def password_has_expired(self):
        if self.is_local and self.password_expired_remain_days < 0:
            return True
        return False

    @property
    def password_will_expired(self):
        if self.is_local and 0 <= self.password_expired_remain_days < 5:
            return True
        return False

    @staticmethod
    def get_public_key_md5(key):
        try:
            key_obj = sshpubkeys.SSHKey(key)
            return key_obj.hash_md5()
        except Exception as e:
            return ''

    def check_public_key(self, key):
        if not self.public_key:
            return False
        key_md5 = self.get_public_key_md5(key)
        if not key_md5:
            return False
        self_key_md5 = self.get_public_key_md5(self.public_key)
        return key_md5 == self_key_md5

    def cache_login_password_if_need(self, password):
        from common.utils import signer
        if not settings.CACHE_LOGIN_PASSWORD_ENABLED:
            return
        backend = getattr(self, 'backend', '')
        if backend.lower().find('ldap') < 0:
            return
        if not password:
            return
        key = self.sect_cache_tpl.format(self.id)
        ttl = settings.CACHE_LOGIN_PASSWORD_TTL
        if not isinstance(ttl, int) or ttl <= 0:
            return
        secret = signer.sign(password)
        cache.set(key, secret, ttl)

    def get_cached_password_if_has(self):
        from common.utils import signer
        if not settings.CACHE_LOGIN_PASSWORD_ENABLED:
            return ''
        key = self.sect_cache_tpl.format(self.id)
        secret = cache.get(key)
        if not secret:
            return ''
        password = signer.unsign(secret)
        return password


class RoleManager(models.Manager):
    scope = None
    _cache = None

    def __init__(self, user, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.user = user

    @lazyproperty
    def role_binding_cls(self):
        from rbac.models import SystemRoleBinding, OrgRoleBinding
        if self.scope == Scope.org:
            return OrgRoleBinding
        else:
            return SystemRoleBinding

    @lazyproperty
    def role_cls(self):
        from rbac.models import SystemRole, OrgRole
        if self.scope == Scope.org:
            return OrgRole
        else:
            return SystemRole

    @property
    def display(self):
        roles = sorted(list(self.all()), key=lambda r: r.scope)
        roles_display = [role.display_name for role in roles]
        return ', '.join(roles_display)

    @property
    def role_bindings(self):
        queryset = self.role_binding_cls.objects.filter(user=self.user)
        if self.scope:
            queryset = queryset.filter(scope=self.scope)
        return queryset

    def _get_queryset(self):
        queryset = self.role_binding_cls.get_user_roles(self.user)
        if self.scope:
            queryset = queryset.filter(scope=self.scope)
        return queryset

    def get_queryset(self):
        if self._cache is not None:
            return self._cache
        return self._get_queryset()

    def clear(self):
        if not self.scope:
            return
        return self.role_bindings.delete()

    def _clean_roles(self, roles_or_ids):
        if not roles_or_ids:
            return
        is_model = isinstance(roles_or_ids[0], models.Model)
        if not is_model:
            roles = self.role_cls.objects.filter(id__in=roles_or_ids)
        else:
            roles = roles_or_ids
        roles = list([r for r in roles if r.scope == self.scope])
        return roles

    def add(self, *roles):
        if not roles:
            return

        roles = self._clean_roles(roles)
        old_ids = self.role_bindings.values_list('role', flat=True)
        need_adds = [r for r in roles if r.id not in old_ids]

        items = []
        for role in need_adds:
            kwargs = {'role': role, 'user': self.user, 'scope': self.scope}
            if self.scope == Scope.org:
                if current_org.is_root():
                    continue
                else:
                    kwargs['org_id'] = current_org.id
            items.append(self.role_binding_cls(**kwargs))

        try:
            result = bulk_create_with_signal(self.role_binding_cls, items, ignore_conflicts=True)
            self.user.expire_users_rbac_perms_cache()
            return result
        except Exception as e:
            logger.error('\\tCreate role binding error: {}'.format(e))

    def set(self, roles, clear=False):
        if clear:
            self.clear()
            self.add(*roles)
            return

        role_ids = set([r.id for r in roles])
        old_ids = self.role_bindings.values_list('role', flat=True)
        old_ids = set(old_ids)

        del_ids = old_ids - role_ids
        add_ids = role_ids - old_ids
        self.remove(*del_ids)
        self.add(*add_ids)

    def remove(self, *roles):
        if not roles:
            return
        roles = self._clean_roles(roles)
        deleted = self.role_bindings.filter(role__in=roles).delete()
        self.user.expire_users_rbac_perms_cache()
        return deleted

    def cache_set(self, roles):
        query = self._get_queryset()
        query._result_cache = roles
        self._cache = query

    @property
    def builtin_role(self):
        from rbac.builtin import BuiltinRole
        return BuiltinRole


class OrgRoleManager(RoleManager):
    def __init__(self, *args, **kwargs):
        from rbac.const import Scope
        self.scope = Scope.org
        super().__init__(*args, **kwargs)


class SystemRoleManager(RoleManager):
    def __init__(self, *args, **kwargs):
        from rbac.const import Scope
        self.scope = Scope.system
        super().__init__(*args, **kwargs)

    def remove_role_system_admin(self):
        role = self.builtin_role.system_admin.get_role()
        return self.remove(role)

    def add_role_system_admin(self):
        role = self.builtin_role.system_admin.get_role()
        return self.add(role)

    def add_role_system_user(self):
        role = self.builtin_role.system_user.get_role()
        return self.add(role)

    def add_role_system_component(self):
        role = self.builtin_role.system_component.get_role()
        self.add(role)


class RoleMixin:
    objects: models.Manager
    is_authenticated: bool
    is_valid: bool
    id: str
    _org_roles = None
    _system_roles = None
    PERM_CACHE_KEY = 'USER_PERMS_ROLES_{}_{}'
    PERM_ORG_KEY = 'USER_PERMS_ORG_{}'
    _is_superuser = None
    _update_superuser = False

    @lazyproperty
    def roles(self):
        return RoleManager(self)

    @lazyproperty
    def org_roles(self):
        return OrgRoleManager(self)

    @lazyproperty
    def system_roles(self):
        return SystemRoleManager(self)

    @lazyproperty
    def console_orgs(self):
        return self.cached_orgs['console_orgs']

    @lazyproperty
    def audit_orgs(self):
        return self.cached_orgs['audit_orgs']

    @lazyproperty
    def workbench_orgs(self):
        return self.cached_orgs['workbench_orgs']

    @lazyproperty
    def joined_orgs(self):
        from rbac.models import RoleBinding
        return RoleBinding.get_user_joined_orgs(self)

    @lazyproperty
    def cached_orgs(self):
        from rbac.models import RoleBinding
        key = self.PERM_ORG_KEY.format(self.id)
        data = cache.get(key)
        if data:
            return data
        console_orgs = RoleBinding.get_user_has_the_perm_orgs('rbac.view_console', self)
        audit_orgs = RoleBinding.get_user_has_the_perm_orgs('rbac.view_audit', self)
        workbench_orgs = RoleBinding.get_user_has_the_perm_orgs('rbac.view_workbench', self)

        if settings.LIMIT_SUPER_PRIV:
            audit_orgs = list(set(audit_orgs) - set(console_orgs))

        data = {
            'console_orgs': console_orgs,
            'audit_orgs': audit_orgs,
            'workbench_orgs': workbench_orgs,
        }
        cache.set(key, data, 60 * 60)
        return data

    @lazyproperty
    def cached_role_and_perms(self):
        key = self.PERM_CACHE_KEY.format(self.id, current_org.id)
        data = cache.get(key)
        if data:
            return data

        data = {
            'org_roles': self.org_roles.all(),
            'system_roles': self.system_roles.all(),
            'perms': self.get_all_permissions(),
        }
        cache.set(key, data, 60 * 60)
        return data

    def expire_rbac_perms_cache(self):
        key = self.PERM_CACHE_KEY.format(self.id, '*')
        cache.delete_pattern(key)
        key = self.PERM_ORG_KEY.format(self.id)
        cache.delete(key)

    @classmethod
    def expire_users_rbac_perms_cache(cls):
        key = cls.PERM_CACHE_KEY.format('*', '*')
        cache.delete_pattern(key)
        key = cls.PERM_ORG_KEY.format('*')
        cache.delete_pattern(key)

    @lazyproperty
    def perms(self):
        return self.cached_role_and_perms['perms']

    @property
    def is_superuser(self):
        if self._is_superuser is not None:
            return self._is_superuser

        from rbac.builtin import BuiltinRole
        ids = [str(r.id) for r in self.system_roles.all()]
        yes = BuiltinRole.system_admin.id in ids
        self._is_superuser = yes
        return yes

    @is_superuser.setter
    def is_superuser(self, value):
        self._is_superuser = value
        self._update_superuser = True
        if value:
            self.system_roles.add_role_system_admin()
        else:
            self.system_roles.remove_role_system_admin()

    @lazyproperty
    def is_org_admin(self):
        from rbac.builtin import BuiltinRole
        if self.is_superuser:
            return True
        ids = [str(r.id) for r in self.org_roles.all()]
        yes = BuiltinRole.org_admin.id in ids
        return yes

    @property
    def is_staff(self):
        return self.is_authenticated and self.is_valid

    @is_staff.setter
    def is_staff(self, value):
        pass

    service_account_email_suffix = '@local.domain'

    @classmethod
    def create_service_account(cls, name, email, comment):
        app = cls.objects.create(
            username=name, name=name, email=email,
            comment=comment, is_first_login=False,
            created_by='System', is_service_account=True,
        )
        access_key = app.create_access_key()
        return app, access_key

    def remove(self):
        if current_org.is_root():
            return
        kwargs = dict(sender=self.__class__, user=self, org=current_org)
        pre_user_leave_org.send(**kwargs)
        self.org_roles.clear()
        post_user_leave_org.send(**kwargs)

    @classmethod
    def get_super_admins(cls):
        from rbac.models import Role, RoleBinding
        system_admin = Role.BuiltinRole.system_admin.get_role()
        return RoleBinding.get_role_users(system_admin)

    @classmethod
    def get_org_admins(cls):
        from rbac.models import Role, RoleBinding
        org_admin = Role.BuiltinRole.org_admin.get_role()
        return RoleBinding.get_role_users(org_admin)

    @classmethod
    def get_super_and_org_admins(cls):
        super_admins = cls.get_super_admins()
        org_admins = cls.get_org_admins()
        admins = org_admins | super_admins
        return admins.distinct()

    @staticmethod
    def filter_not_service_account(queryset):
        return queryset.filter(is_service_account=False)

    @classmethod
    def get_nature_users(cls):
        queryset = cls.objects.all()
        return cls.filter_not_service_account(queryset)

    @classmethod
    def get_org_users(cls, org=None):
        queryset = cls.objects.all()
        if org is None:
            org = current_org
        if not org.is_root():
            queryset = org.get_members()
        queryset = cls.filter_not_service_account(queryset)
        return queryset

    def get_all_permissions(self):
        from rbac.models import RoleBinding
        perms = RoleBinding.get_user_perms(self)

        if settings.LIMIT_SUPER_PRIV and 'view_console' in perms:
            perms = [p for p in perms if p != \""view_audit\""]
        return perms


class TokenMixin:
    CACHE_KEY_USER_RESET_PASSWORD_PREFIX = \""_KEY_USER_RESET_PASSWORD_{}\""
    email = ''
    id = None

    @property
    def private_token(self):
        return self.create_private_token()

    def create_private_token(self):
        from authentication.models import PrivateToken
        token, created = PrivateToken.objects.get_or_create(user=self)
        return token

    def delete_private_token(self):
        from authentication.models import PrivateToken
        PrivateToken.objects.filter(user=self).delete()

    def refresh_private_token(self):
        self.delete_private_token()
        return self.create_private_token()

    def create_bearer_token(self, request=None):
        expiration = settings.TOKEN_EXPIRATION or 3600
        if request:
            remote_addr = request.META.get('REMOTE_ADDR', '')
        else:
            remote_addr = '0.0.0.0'
        if not isinstance(remote_addr, bytes):
            remote_addr = remote_addr.encode(\""utf-8\"")
        cache_key = '%s_%s' % (self.id, remote_addr)
        token = cache.get(cache_key)
        if not token:
            token = random_string(36)
        cache.set(token, self.id, expiration)
        cache.set('%s_%s' % (self.id, remote_addr), token, expiration)
        date_expired = timezone.now() + timezone.timedelta(seconds=expiration)
        return token, date_expired

    def refresh_bearer_token(self, token):
        pass

    def create_access_key(self):
        access_key = self.access_keys.create()
        return access_key

    @property
    def access_key(self):
        return self.access_keys.first()

    def generate_reset_token(self):
        token = random_string(50)
        key = self.CACHE_KEY_USER_RESET_PASSWORD_PREFIX.format(token)
        cache.set(key, {'id': self.id, 'email': self.email}, 3600)
        return token

    @classmethod
    def validate_reset_password_token(cls, token):
        if not token:
            return None
        key = cls.CACHE_KEY_USER_RESET_PASSWORD_PREFIX.format(token)
        value = cache.get(key)
        if not value:
            return None
        try:
            user_id = value.get('id', '')
            email = value.get('email', '')
            user = cls.objects.get(id=user_id, email=email)
            return user
        except (AttributeError, cls.DoesNotExist) as e:
            logger.error(e, exc_info=True)
            return None

    @classmethod
    def expired_reset_password_token(cls, token):
        key = cls.CACHE_KEY_USER_RESET_PASSWORD_PREFIX.format(token)
        cache.delete(key)


class MFAMixin:
    mfa_level = 0
    otp_secret_key = ''
    MFA_LEVEL_CHOICES = (
        (0, _('Disable')),
        (1, _('Enable')),
        (2, _(\""Force enable\"")),
    )
    is_org_admin: bool
    username: str
    phone: str

    @property
    def mfa_enabled(self):
        if self.mfa_force_enabled:
            return True
        return self.mfa_level > 0

    @property
    def mfa_force_enabled(self):
        force_level = settings.SECURITY_MFA_AUTH
        if force_level in [True, 1]:
            return True
        if force_level == 2 and self.is_org_admin:
            return True
        return self.mfa_level == 2

    def enable_mfa(self):
        if not self.mfa_level == 2:
            self.mfa_level = 1

    def force_enable_mfa(self):
        self.mfa_level = 2

    def disable_mfa(self):
        self.mfa_level = 0

    def no_active_mfa(self):
        return len(self.active_mfa_backends) == 0

    @lazyproperty
    def active_mfa_backends(self):
        backends = self.get_user_mfa_backends(self)
        active_backends = [b for b in backends if b.is_active()]
        return active_backends

    @property
    def active_mfa_backends_mapper(self):
        return {b.name: b for b in self.active_mfa_backends}

    @staticmethod
    def get_user_mfa_backends(user):
        backends = []
        for cls in settings.MFA_BACKENDS:
            cls = import_string(cls)
            if cls.global_enabled():
                backends.append(cls(user))
        return backends

    def get_active_mfa_backend_by_type(self, mfa_type):
        backend = self.get_mfa_backend_by_type(mfa_type)
        if not backend or not backend.is_active():
            return None
        return backend

    def get_mfa_backend_by_type(self, mfa_type):
        mfa_mapper = {b.name: b for b in self.get_user_mfa_backends(self)}
        backend = mfa_mapper.get(mfa_type)
        if not backend:
            return None
        return backend


class JSONFilterMixin:
    @staticmethod
    def get_json_filter_attr_q(name, value, match):
        from rbac.models import RoleBinding
        from orgs.utils import current_org

        kwargs = {}
        if name == 'system_roles':
            kwargs['scope'] = 'system'
        elif name == 'org_roles':
            kwargs['scope'] = 'org'
            if not current_org.is_root():
                kwargs['org_id'] = current_org.id
        else:
            return None

        bindings = RoleBinding.objects.filter(**kwargs, role__in=value)
        if match == 'm2m_all':
            user_id = bindings.values('user_id').annotate(count=Count('user_id')) \\
                .filter(count=len(value)).values_list('user_id', flat=True)
        else:
            user_id = bindings.values_list('user_id', flat=True)

        return models.Q(id__in=user_id)


class User(AuthMixin, TokenMixin, RoleMixin, MFAMixin, JSONFilterMixin, AbstractUser):
    class Source(models.TextChoices):
        local = 'local', _('Local')
        ldap = 'ldap', 'LDAP/AD'
        openid = 'openid', 'OpenID'
        radius = 'radius', 'Radius'
        cas = 'cas', 'CAS'
        saml2 = 'saml2', 'SAML2'
        oauth2 = 'oauth2', 'OAuth2'
        wecom = 'wecom', _('WeCom')
        dingtalk = 'dingtalk', _('DingTalk')
        feishu = 'feishu', _('FeiShu')
        custom = 'custom', 'Custom'

    SOURCE_BACKEND_MAPPING = {
        Source.local: [
            settings.AUTH_BACKEND_MODEL,
            settings.AUTH_BACKEND_PUBKEY,
        ],
        Source.ldap: [
            settings.AUTH_BACKEND_LDAP
        ],
        Source.openid: [
            settings.AUTH_BACKEND_OIDC_PASSWORD,
            settings.AUTH_BACKEND_OIDC_CODE
        ],
        Source.radius: [
            settings.AUTH_BACKEND_RADIUS
        ],
        Source.cas: [
            settings.AUTH_BACKEND_CAS
        ],
        Source.saml2: [
            settings.AUTH_BACKEND_SAML2
        ],
        Source.oauth2: [
            settings.AUTH_BACKEND_OAUTH2
        ],
        Source.wecom: [
            settings.AUTH_BACKEND_WECOM
        ],
        Source.feishu: [
            settings.AUTH_BACKEND_FEISHU
        ],
        Source.dingtalk: [
            settings.AUTH_BACKEND_DINGTALK
        ],
        Source.custom: [
            settings.AUTH_BACKEND_CUSTOM
        ]
    }

    id = models.UUIDField(default=uuid.uuid4, primary_key=True)
    username = models.CharField(
        max_length=128, unique=True, verbose_name=_('Username')
    )
    name = models.CharField(max_length=128, verbose_name=_('Name'))
    email = models.EmailField(
        max_length=128, unique=True, verbose_name=_('Email')
    )
    groups = models.ManyToManyField(
        'users.UserGroup', related_name='users',
        blank=True, verbose_name=_('User group')
    )
    role = models.CharField(
        default='User', max_length=10,
        blank=True, verbose_name=_('Role')
    )
    is_service_account = models.BooleanField(default=False, verbose_name=_(\""Is service account\""))
    avatar = models.ImageField(
        upload_to=\""avatar\"", null=True, verbose_name=_('Avatar')
    )
    wechat = fields.EncryptCharField(
        max_length=128, blank=True, verbose_name=_('Wechat')
    )
    phone = fields.EncryptCharField(
        max_length=128, blank=True, null=True, verbose_name=_('Phone')
    )
    mfa_level = models.SmallIntegerField(
        default=0, choices=MFAMixin.MFA_LEVEL_CHOICES, verbose_name=_('MFA')
    )
    otp_secret_key = fields.EncryptCharField(
        max_length=128, blank=True, null=True, verbose_name=_('OTP secret key')
    )
    private_key = fields.EncryptTextField(
        blank=True, null=True, verbose_name=_('Private key')
    )
    public_key = fields.EncryptTextField(
        blank=True, null=True, verbose_name=_('Public key')
    )
    comment = models.TextField(
        blank=True, null=True, verbose_name=_('Comment')
    )
    is_first_login = models.BooleanField(default=True, verbose_name=_('Is first login'))
    date_expired = models.DateTimeField(
        default=date_expired_default, blank=True, null=True,
        db_index=True, verbose_name=_('Date expired')
    )
    created_by = models.CharField(max_length=30, default='', blank=True, verbose_name=_('Created by'))
    updated_by = models.CharField(max_length=30, default='', blank=True, verbose_name=_('Updated by'))
    source = models.CharField(max_length=30, default=Source.local, choices=Source.choices, verbose_name=_('Source'))
    date_password_last_updated = models.DateTimeField(
        auto_now_add=True, blank=True, null=True,
        verbose_name=_('Date password last updated')
    )
    need_update_password = models.BooleanField(
        default=False, verbose_name=_('Need update password')
    )
    date_api_key_last_used = models.DateTimeField(null=True, blank=True, verbose_name=_('Date api key used'))
    date_updated = models.DateTimeField(auto_now=True, verbose_name=_('Date updated'))
    wecom_id = models.CharField(null=True, default=None, max_length=128, verbose_name=_('WeCom'))
    dingtalk_id = models.CharField(null=True, default=None, max_length=128, verbose_name=_('DingTalk'))
    feishu_id = models.CharField(null=True, default=None, max_length=128, verbose_name=_('FeiShu'))

    DATE_EXPIRED_WARNING_DAYS = 5

    def __str__(self):
        return '{0.name}({0.username})'.format(self)

    @property
    def secret_key(self):
        instance = self.preferences.filter(name='secret_key').first()
        if not instance:
            return
        return instance.decrypt_value

    @property
    def receive_backends(self):
        return self.user_msg_subscription.receive_backends

    @property
    def is_otp_secret_key_bound(self):
        return bool(self.otp_secret_key)

    def get_absolute_url(self):
        return reverse('users:user-detail', args=(self.id,))

    @property
    def source_display(self):
        return self.get_source_display()

    @property
    def is_expired(self):
        if self.date_expired and self.date_expired < timezone.now():
            return True
        else:
            return False

    @property
    def expired_remain_days(self):
        date_remain = self.date_expired - timezone.now()
        return date_remain.days

    @property
    def will_expired(self):
        if 0 <= self.expired_remain_days <= self.DATE_EXPIRED_WARNING_DAYS:
            return True
        else:
            return False

    @property
    def is_valid(self):
        if self.is_active and not self.is_expired:
            return True
        return False

    @property
    def is_local(self):
        return self.source == self.Source.local.value

    def is_password_authenticate(self):
        cas = self.Source.cas
        saml2 = self.Source.saml2
        oauth2 = self.Source.oauth2
        return self.source not in [cas, saml2, oauth2]

    def set_required_attr_if_need(self):
        if not self.name:
            self.name = self.username
        if not self.email or '@' not in self.email:
            email = '{}@{}'.format(self.username, settings.EMAIL_SUFFIX)
            if '@' in self.username:
                email = self.username
            self.email = email

    def save(self, *args, **kwargs):
        self.set_required_attr_if_need()
        if self.username == 'admin':
            self.role = 'Admin'
            self.is_active = True
        return super().save(*args, **kwargs)

    def is_member_of(self, user_group):
        if user_group in self.groups.all():
            return True
        return False

    def set_avatar(self, f):
        self.avatar.save(self.username, f)

    @classmethod
    def get_avatar_url(cls, username):
        user_default = settings.STATIC_URL + \""img/avatar/user.png\""
        return user_default

    def avatar_url(self):
        admin_default = settings.STATIC_URL + \""img/avatar/admin.png\""
        user_default = settings.STATIC_URL + \""img/avatar/user.png\""
        if self.avatar:
            return self.avatar.url
        if self.is_superuser:
            return admin_default
        else:
            return user_default

    def unblock_login(self):
        from users.utils import LoginBlockUtil, MFABlockUtils
        LoginBlockUtil.unblock_user(self.username)
        MFABlockUtils.unblock_user(self.username)

    @property
    def login_blocked(self):
        from users.utils import LoginBlockUtil, MFABlockUtils
        if LoginBlockUtil.is_user_block(self.username):
            return True
        if MFABlockUtils.is_user_block(self.username):
            return True
        return False

    def delete(self, using=None, keep_parents=False):
        if self.pk == 1 or self.username == 'admin':
            raise PermissionDenied(_('Can not delete admin user'))
        return super(User, self).delete(using=using, keep_parents=keep_parents)

    @classmethod
    def get_user_allowed_auth_backend_paths(cls, username):
        if not settings.ONLY_ALLOW_AUTH_FROM_SOURCE or not username:
            return None
        user = cls.objects.filter(username=username).first()
        if not user:
            return None
        return user.get_allowed_auth_backend_paths()

    def get_allowed_auth_backend_paths(self):
        if not settings.ONLY_ALLOW_AUTH_FROM_SOURCE:
            return None
        return self.SOURCE_BACKEND_MAPPING.get(self.source, [])

    class Meta:
        ordering = ['username']
        verbose_name = _(\""User\"")
        unique_together = (
            ('dingtalk_id',),
            ('wecom_id',),
            ('feishu_id',),
        )
        permissions = [
            ('invite_user', _('Can invite user')),
            ('remove_user', _('Can remove user')),
            ('match_user', _('Can match user')),
        ]

    @classmethod
    def initial(cls):
        from .group import UserGroup
        user = cls(username='admin',
                   email='admin@jumpserver.org',
                   name=_('Administrator'),
                   password_raw='admin',
                   role='Admin',
                   comment=_('Administrator is the super user of system'),
                   created_by=_('System'))
        user.save()
        user.groups.add(UserGroup.initial())

    def can_send_created_mail(self):
        if self.email and self.source == self.Source.local.value:
            return True
        return False


class UserPasswordHistory(models.Model):
    id = models.UUIDField(default=uuid.uuid4, primary_key=True)
    password = models.CharField(max_length=128)
    user = models.ForeignKey(\""users.User\"", related_name='history_passwords',
                             on_delete=jms_models.CASCADE_SIGNAL_SKIP, verbose_name=_('User'))
    date_created = models.DateTimeField(auto_now_add=True, verbose_name=_(\""Date created\""))

    def __str__(self):
        return f'{self.user} set at {self.date_created}'

    def __repr__(self):
        return self.__str__()

    class Meta:
        verbose_name = _(\""User password history\"")
","['get_logger', 'password_raw', 'AttributeError', 'set_password', 'can_update_password', 'now', 'send', 'super', 'set_public_key', 'can_update_ssh_key', 'save', 'can_use_ssh_key_login', 'is_history_password', 'all', 'order_by', 'int', 'check_password', 'is_public_key_valid', 'public_key_obj', 'PubKey', '__getattr__', 'SSHKey', 'get_public_key_comment', 'get_public_key_hash_md5', 'callable', 'hash_md5', 'reset_password', 'date_password_expired', 'timedelta', 'password_expired_remain_days', 'password_has_expired', 'password_will_expired', 'get_public_key_md5', 'check_public_key', 'cache_login_password_if_need', 'getattr', 'lower', 'find', 'format', 'isinstance', 'sign', 'set', 'get_cached_password_if_has', 'get', 'unsign', 'RoleManager', '__init__', 'role_binding_cls', 'role_cls', 'display', 'sorted', 'list', 'join', 'role_bindings', 'filter', '_get_queryset', 'get_user_roles', 'get_queryset', 'clear', 'delete', '_clean_roles', 'add', 'values_list', 'is_root', 'append', 'bulk_create_with_signal', 'expire_users_rbac_perms_cache', 'error', 'remove', 'cache_set', 'builtin_role', 'OrgRoleManager', 'SystemRoleManager', 'remove_role_system_admin', 'get_role', 'add_role_system_admin', 'add_role_system_user', 'add_role_system_component', 'roles', 'org_roles', 'system_roles', 'console_orgs', 'audit_orgs', 'workbench_orgs', 'joined_orgs', 'get_user_joined_orgs', 'cached_orgs', 'get_user_has_the_perm_orgs', 'cached_role_and_perms', 'get_all_permissions', 'expire_rbac_perms_cache', 'delete_pattern', 'perms', 'is_superuser', 'str', 'is_org_admin', 'is_staff', 'create_service_account', 'create', 'create_access_key', 'dict', 'get_super_admins', 'get_role_users', 'get_org_admins', 'get_super_and_org_admins', 'distinct', 'filter_not_service_account', 'get_nature_users', 'get_org_users', 'get_members', 'get_user_perms', 'private_token', 'create_private_token', 'get_or_create', 'delete_private_token', 'refresh_private_token', 'create_bearer_token', 'encode', 'random_string', 'refresh_bearer_token', 'access_key', 'first', 'generate_reset_token', 'validate_reset_password_token', 'expired_reset_password_token', '_', 'mfa_enabled', 'mfa_force_enabled', 'enable_mfa', 'force_enable_mfa', 'disable_mfa', 'no_active_mfa', 'len', 'active_mfa_backends', 'get_user_mfa_backends', 'is_active', 'active_mfa_backends_mapper', 'import_string', 'global_enabled', 'cls', 'get_active_mfa_backend_by_type', 'get_mfa_backend_by_type', 'get_json_filter_attr_q', 'values', 'annotate', 'Count', 'Q', 'User', 'Source', 'UUIDField', 'CharField', 'EmailField', 'ManyToManyField', 'BooleanField', 'ImageField', 'EncryptCharField', 'SmallIntegerField', 'EncryptTextField', 'TextField', 'DateTimeField', '__str__', 'secret_key', 'receive_backends', 'is_otp_secret_key_bound', 'bool', 'get_absolute_url', 'reverse', 'source_display', 'get_source_display', 'is_expired', 'expired_remain_days', 'will_expired', 'is_valid', 'is_local', 'is_password_authenticate', 'set_required_attr_if_need', 'is_member_of', 'set_avatar', 'get_avatar_url', 'avatar_url', 'unblock_login', 'unblock_user', 'login_blocked', 'is_user_block', 'PermissionDenied', 'get_user_allowed_auth_backend_paths', 'get_allowed_auth_backend_paths', 'initial', 'can_send_created_mail', 'UserPasswordHistory', 'ForeignKey', '__repr__']"
"https://github.com/apache/superset/blob/5942d8bf6831298280dc48c4be1d88ef648c3698/superset/db_engine_specs/base.py
","
from __future__ import annotations

import json
import logging
import re
from datetime import datetime
from re import Match, Pattern
from typing import (
    Any,
    Callable,
    cast,
    ContextManager,
    NamedTuple,
    TYPE_CHECKING,
    TypedDict,
    Union,
)

import pandas as pd
import sqlparse
from apispec import APISpec
from apispec.ext.marshmallow import MarshmallowPlugin
from deprecation import deprecated
from flask import current_app
from flask_appbuilder.security.sqla.models import User
from flask_babel import gettext as __, lazy_gettext as _
from marshmallow import fields, Schema
from marshmallow.validate import Range
from sqlalchemy import column, select, types
from sqlalchemy.engine.base import Engine
from sqlalchemy.engine.interfaces import Compiled, Dialect
from sqlalchemy.engine.reflection import Inspector
from sqlalchemy.engine.url import URL
from sqlalchemy.ext.compiler import compiles
from sqlalchemy.orm import Session
from sqlalchemy.sql import quoted_name, text
from sqlalchemy.sql.expression import ColumnClause, Select, TextAsFrom, TextClause
from sqlalchemy.types import TypeEngine
from sqlparse.tokens import CTE

from superset import security_manager, sql_parse
from superset.constants import TimeGrain as TimeGrainConstants
from superset.databases.utils import make_url_safe
from superset.errors import ErrorLevel, SupersetError, SupersetErrorType
from superset.sql_parse import ParsedQuery, Table
from superset.superset_typing import ResultSetColumnType, SQLAColumnType
from superset.utils import core as utils
from superset.utils.core import ColumnSpec, GenericDataType
from superset.utils.hashing import md5_sha_from_str
from superset.utils.network import is_hostname_valid, is_port_open

if TYPE_CHECKING:
    from superset.connectors.sqla.models import TableColumn
    from superset.models.core import Database
    from superset.models.sql_lab import Query

ColumnTypeMapping = tuple[
    Pattern[str],
    Union[TypeEngine, Callable[[Match[str]], TypeEngine]],
    GenericDataType,
]

logger = logging.getLogger()


def convert_inspector_columns(cols: list[SQLAColumnType]) -> list[ResultSetColumnType]:
    result_set_columns: list[ResultSetColumnType] = []
    for col in cols:
    return result_set_columns


class TimeGrain(NamedTuple):
    label: str
    function: str
    duration: str | None


builtin_time_grains: dict[str | None, str] = {
    TimeGrainConstants.SECOND: __(\""Second\""),
    TimeGrainConstants.FIVE_SECONDS: __(\""5 second\""),
    TimeGrainConstants.THIRTY_SECONDS: __(\""30 second\""),
    TimeGrainConstants.MINUTE: __(\""Minute\""),
    TimeGrainConstants.FIVE_MINUTES: __(\""5 minute\""),
    TimeGrainConstants.TEN_MINUTES: __(\""10 minute\""),
    TimeGrainConstants.FIFTEEN_MINUTES: __(\""15 minute\""),
    TimeGrainConstants.THIRTY_MINUTES: __(\""30 minute\""),
    TimeGrainConstants.HOUR: __(\""Hour\""),
    TimeGrainConstants.SIX_HOURS: __(\""6 hour\""),
    TimeGrainConstants.DAY: __(\""Day\""),
    TimeGrainConstants.WEEK: __(\""Week\""),
    TimeGrainConstants.MONTH: __(\""Month\""),
    TimeGrainConstants.QUARTER: __(\""Quarter\""),
    TimeGrainConstants.YEAR: __(\""Year\""),
    TimeGrainConstants.WEEK_STARTING_SUNDAY: __(\""Week starting Sunday\""),
    TimeGrainConstants.WEEK_STARTING_MONDAY: __(\""Week starting Monday\""),
    TimeGrainConstants.WEEK_ENDING_SATURDAY: __(\""Week ending Saturday\""),
    TimeGrainConstants.WEEK_ENDING_SUNDAY: __(\""Week ending Sunday\""),
}


class TimestampExpression(
    ColumnClause
    def __init__(self, expr: str, col: ColumnClause, **kwargs: Any) -> None:
        super().__init__(expr, **kwargs)
        self.col = col

    @property
    def _constructor(self) -> ColumnClause:
        return ColumnClause


@compiles(TimestampExpression)
def compile_timegrain_expression(
    element: TimestampExpression, compiler: Compiled, **kwargs: Any
) -> str:
    return element.name.replace(\""{col}\"", compiler.process(element.col, **kwargs))



    FETCH_MANY = \""fetch_many\""
    WRAP_SQL = \""wrap_sql\""
    FORCE_LIMIT = \""force_limit\""


class MetricType(TypedDict, total=False):

    metric_name: str
    expression: str
    verbose_name: str | None
    metric_type: str | None
    description: str | None
    d3format: str | None
    currency: str | None
    warning_text: str | None
    extra: str | None




    engine_aliases: set[str] = set()
    drivers: dict[str, str] = {}
    default_driver: str | None = None

    sqlalchemy_uri_placeholder = (
        \""engine+driver://user:password@host:port/dbname[?key=value&key=value...]\""
    )

    disable_ssh_tunneling = False

    _date_trunc_functions: dict[str, str] = {}
    _time_grain_expressions: dict[str | None, str] = {}
    _default_column_type_mappings: tuple[ColumnTypeMapping, ...] = (
        (
            re.compile(r\""^string\"", re.IGNORECASE),
            types.String(),
            GenericDataType.STRING,
        ),
        (
            re.compile(r\""^n((var)?char|text)\"", re.IGNORECASE),
            types.UnicodeText(),
            GenericDataType.STRING,
        ),
        (
            re.compile(r\""^(var)?char\"", re.IGNORECASE),
            types.String(),
            GenericDataType.STRING,
        ),
        (
            re.compile(r\""^(tiny|medium|long)?text\"", re.IGNORECASE),
            types.String(),
            GenericDataType.STRING,
        ),
        (
            re.compile(r\""^smallint\"", re.IGNORECASE),
            types.SmallInteger(),
            GenericDataType.NUMERIC,
        ),
        (
            re.compile(r\""^int(eger)?\"", re.IGNORECASE),
            types.Integer(),
            GenericDataType.NUMERIC,
        ),
        (
            re.compile(r\""^bigint\"", re.IGNORECASE),
            types.BigInteger(),
            GenericDataType.NUMERIC,
        ),
        (
            re.compile(r\""^long\"", re.IGNORECASE),
            types.Float(),
            GenericDataType.NUMERIC,
        ),
        (
            re.compile(r\""^decimal\"", re.IGNORECASE),
            types.Numeric(),
            GenericDataType.NUMERIC,
        ),
        (
            re.compile(r\""^numeric\"", re.IGNORECASE),
            types.Numeric(),
            GenericDataType.NUMERIC,
        ),
        (
            re.compile(r\""^float\"", re.IGNORECASE),
            types.Float(),
            GenericDataType.NUMERIC,
        ),
        (
            re.compile(r\""^double\"", re.IGNORECASE),
            types.Float(),
            GenericDataType.NUMERIC,
        ),
        (
            re.compile(r\""^real\"", re.IGNORECASE),
            types.REAL,
            GenericDataType.NUMERIC,
        ),
        (
            re.compile(r\""^smallserial\"", re.IGNORECASE),
            types.SmallInteger(),
            GenericDataType.NUMERIC,
        ),
        (
            re.compile(r\""^serial\"", re.IGNORECASE),
            types.Integer(),
            GenericDataType.NUMERIC,
        ),
        (
            re.compile(r\""^bigserial\"", re.IGNORECASE),
            types.BigInteger(),
            GenericDataType.NUMERIC,
        ),
        (
            re.compile(r\""^money\"", re.IGNORECASE),
            types.Numeric(),
            GenericDataType.NUMERIC,
        ),
        (
            re.compile(r\""^timestamp\"", re.IGNORECASE),
            types.TIMESTAMP(),
            GenericDataType.TEMPORAL,
        ),
        (
            re.compile(r\""^datetime\"", re.IGNORECASE),
            types.DateTime(),
            GenericDataType.TEMPORAL,
        ),
        (
            re.compile(r\""^date\"", re.IGNORECASE),
            types.Date(),
            GenericDataType.TEMPORAL,
        ),
        (
            re.compile(r\""^time\"", re.IGNORECASE),
            types.Time(),
            GenericDataType.TEMPORAL,
        ),
        (
            re.compile(r\""^interval\"", re.IGNORECASE),
            types.Interval(),
            GenericDataType.TEMPORAL,
        ),
        (
            re.compile(r\""^bool(ean)?\"", re.IGNORECASE),
            types.Boolean(),
            GenericDataType.BOOLEAN,
        ),
    )
    column_type_mappings: tuple[ColumnTypeMapping, ...] = ()

    column_type_mutators: dict[TypeEngine, Callable[[Any], Any]] = {}

    time_groupby_inline = False
    limit_method = LimitMethod.FORCE_LIMIT
    allows_joins = True
    allows_subqueries = True
    allows_alias_in_select = True
    allows_alias_in_orderby = True
    allows_sql_comments = True
    allows_escaped_colons = True

    allows_alias_to_source_column = True

    allows_hidden_orderby_agg = True

    allows_hidden_cc_in_orderby = False

    allows_cte_in_subquery = True
    cte_alias = \""__cte\""
    allow_limit_clause = True
    select_keywords: set[str] = {\""SELECT\""}
    top_keywords: set[str] = {\""TOP\""}
    disallow_uri_query_params: dict[str, set[str]] = {}
    enforce_uri_query_params: dict[str, dict[str, Any]] = {}

    force_column_alias_quotes = False
    arraysize = 0
    max_column_name_length: int | None = None
    run_multiple_statements_as_one = False
    custom_errors: dict[
        Pattern[str], tuple[str, SupersetErrorType, dict[str, Any]]
    ] = {}

    supports_file_upload = True

    supports_dynamic_schema = False

    supports_catalog = False

    supports_dynamic_catalog = False

    @classmethod
    def get_allows_alias_in_select(
    ) -> bool:
        return cls.allows_alias_in_select

    @classmethod
    def supports_url(cls, url: URL) -> bool:
        backend = url.get_backend_name()
        driver = url.get_driver_name()
        return cls.supports_backend(backend, driver)

    @classmethod
    def supports_backend(cls, backend: str, driver: str | None = None) -> bool:
        if backend != cls.engine and backend not in cls.engine_aliases:
            return False

        if not cls.drivers or driver is None:
            return True

        return driver in cls.drivers

    @classmethod
    def get_default_schema(cls, database: Database) -> str | None:
        with database.get_inspector_with_context() as inspector:
            return inspector.default_schema_name

    @classmethod
        cls,
        sqlalchemy_uri: URL,
        connect_args: dict[str, Any],
    ) -> str | None:
        return None

    @classmethod
    def get_default_schema_for_query(
        cls,
        database: Database,
        query: Query,
    ) -> str | None:
        if cls.supports_dynamic_schema:
            return query.schema

        try:
            connect_args = database.get_extra()[\""engine_params\""][\""connect_args\""]
        except KeyError:
            connect_args = {}
        sqlalchemy_uri = make_url_safe(database.sqlalchemy_uri)
        if schema := cls.get_schema_from_engine_params(sqlalchemy_uri, connect_args):
            return schema

        return cls.get_default_schema(database)

    @classmethod
    def get_dbapi_exception_mapping(cls) -> dict[type[Exception], type[Exception]]:
        return {}

    @classmethod
    def parse_error_exception(cls, exception: Exception) -> Exception:
        return exception

    @classmethod
    def get_dbapi_mapped_exception(cls, exception: Exception) -> Exception:
        new_exception = cls.get_dbapi_exception_mapping().get(type(exception))
        if not new_exception:
            return cls.parse_error_exception(exception)
        return new_exception(str(exception))

    @classmethod
        cls,
        extra: dict[str, Any],
    ) -> bool:
        return False

    @classmethod
    def get_text_clause(cls, clause: str) -> TextClause:
        if cls.allows_escaped_colons:
            clause = clause.replace(\"":\"", \""\\\\:\"")
        return text(clause)

    @classmethod
    def get_engine(
        cls,
        database: Database,
        schema: str | None = None,
        source: utils.QuerySource | None = None,
    ) -> ContextManager[Engine]:
        return database.get_sqla_engine_with_context(schema=schema, source=source)

    @classmethod
    def get_timestamp_expr(
        cls,
        col: ColumnClause,
        pdf: str | None,
        time_grain: str | None,
    ) -> TimestampExpression:
        if time_grain:
            type_ = str(getattr(col, \""type\"", \""\""))
            time_expr = cls.get_time_grain_expressions().get(time_grain)
            if not time_expr:
                raise NotImplementedError(
                    f\""No grain spec for {time_grain} for database {cls.engine}\""
                )
            if type_ and \""{func}\"" in time_expr:
                date_trunc_function = cls._date_trunc_functions.get(type_)
                if date_trunc_function:
                    time_expr = time_expr.replace(\""{func}\"", date_trunc_function)
            if type_ and \""{type}\"" in time_expr:
                date_trunc_function = cls._date_trunc_functions.get(type_)
                if date_trunc_function:
                    time_expr = time_expr.replace(\""{type}\"", type_)
        else:
            time_expr = \""{col}\""

        if pdf == \""epoch_s\"":
            time_expr = time_expr.replace(\""{col}\"", cls.epoch_to_dttm())
        elif pdf == \""epoch_ms\"":
            time_expr = time_expr.replace(\""{col}\"", cls.epoch_ms_to_dttm())

        return TimestampExpression(time_expr, col, type_=col.type)

    @classmethod
    def get_time_grains(cls) -> tuple[TimeGrain, ...]:

        ret_list = []
        time_grains = builtin_time_grains.copy()
        time_grains.update(current_app.config[\""TIME_GRAIN_ADDONS\""])
        for duration, func in cls.get_time_grain_expressions().items():
            if duration in time_grains:
                name = time_grains[duration]
                ret_list.append(TimeGrain(name, _(name), func, duration))
        return tuple(ret_list)

    @classmethod
    def _sort_time_grains(
        cls, val: tuple[str | None, str], index: int
    ) -> float | int | str:
        pos = {
            \""FIRST\"": 0,
            \""SECOND\"": 1,
            \""THIRD\"": 2,
            \""LAST\"": 3,
        }

        if val[0] is None:
            return pos[\""FIRST\""]

        prog = re.compile(r\""(.*\\/)?(P|PT)([0-9\\.]+)(S|M|H|D|W|M|Y)(\\/.*)?\"")
        result = prog.match(val[0])

        if result is None:
            return pos[\""LAST\""]

        second_minute_hour = [\""S\"", \""M\"", \""H\""]
        day_week_month_year = [\""D\"", \""W\"", \""M\"", \""Y\""]
        is_less_than_day = result.group(2) == \""PT\""
        interval = result.group(4)
        epoch_time_start_string = result.group(1) or result.group(5)
        has_starting_or_ending = bool(len(epoch_time_start_string or \""\""))

        def sort_day_week() -> int:
            if has_starting_or_ending:
                return pos[\""LAST\""]
            if is_less_than_day:
                return pos[\""SECOND\""]
            return pos[\""THIRD\""]

        def sort_interval() -> float:
            if is_less_than_day:
                return second_minute_hour.index(interval)
            return day_week_month_year.index(interval)

        plist = {
            0: sort_day_week(),
            1: pos[\""SECOND\""] if is_less_than_day else pos[\""THIRD\""],
            2: sort_interval(),
            3: float(result.group(3)),
        }

        return plist.get(index, 0)

    @classmethod
    def get_time_grain_expressions(cls) -> dict[str | None, str]:
        time_grain_expressions = cls._time_grain_expressions.copy()
        grain_addon_expressions = current_app.config[\""TIME_GRAIN_ADDON_EXPRESSIONS\""]
        time_grain_expressions.update(grain_addon_expressions.get(cls.engine, {}))
        denylist: list[str] = current_app.config[\""TIME_GRAIN_DENYLIST\""]
        for key in denylist:
            time_grain_expressions.pop(key, None)

        return dict(
            sorted(
                time_grain_expressions.items(),
                key=lambda x: (
                    cls._sort_time_grains(x, 0),
                    cls._sort_time_grains(x, 1),
                    cls._sort_time_grains(x, 2),
                    cls._sort_time_grains(x, 3),
                ),
            )
        )

    @classmethod
    def fetch_data(cls, cursor: Any, limit: int | None = None) -> list[tuple[Any, ...]]:
        if cls.arraysize:
            cursor.arraysize = cls.arraysize
        try:
            if cls.limit_method == LimitMethod.FETCH_MANY and limit:
                return cursor.fetchmany(limit)
            data = cursor.fetchall()
            description = cursor.description or []
            column_mutators = {
                row[0]: func
                for row in description
                if (
                    func := cls.column_type_mutators.get(
                        type(cls.get_sqla_column_type(cls.get_datatype(row[1])))
                    )
                )
            }
            if column_mutators:
                indexes = {row[0]: idx for idx, row in enumerate(description)}
                for row_idx, row in enumerate(data):
                    new_row = list(row)
                    for col, func in column_mutators.items():
                        col_idx = indexes[col]
                        new_row[col_idx] = func(row[col_idx])
                    data[row_idx] = tuple(new_row)

            return data
        except Exception as ex:
            raise cls.get_dbapi_mapped_exception(ex) from ex

    @classmethod
    def expand_data(
        cls, columns: list[ResultSetColumnType], data: list[dict[Any, Any]]
    ) -> tuple[
        list[ResultSetColumnType], list[dict[Any, Any]], list[ResultSetColumnType]
    ]:
        return columns, data, []

    @classmethod
    def alter_new_orm_column(cls, orm_col: TableColumn) -> None:

    @classmethod
    def epoch_to_dttm(cls) -> str:
        raise NotImplementedError()

    @classmethod
    def epoch_ms_to_dttm(cls) -> str:
        return cls.epoch_to_dttm().replace(\""{col}\"", \""({col}/1000)\"")

    @classmethod
    def get_datatype(cls, type_code: Any) -> str | None:
        if isinstance(type_code, str) and type_code != \""\"":
            return type_code.upper()
        return None

    @classmethod
    @deprecated(deprecated_in=\""3.0\"")
    def normalize_indexes(cls, indexes: list[dict[str, Any]]) -> list[dict[str, Any]]:
        return indexes

    @classmethod
        cls,
        database: Database,
        table_name: str,
        schema_name: str | None,
    ) -> dict[str, Any]:
        return {}

    @classmethod
    def apply_limit_to_sql(
        cls, sql: str, limit: int, database: Database, force: bool = False
    ) -> str:
        if cls.limit_method == LimitMethod.WRAP_SQL:
            sql = sql.strip(\""\\t\\n ;\"")
            qry = (
                select(\""*\"")
                .select_from(TextAsFrom(text(sql), [\""*\""]).alias(\""inner_qry\""))
                .limit(limit)
            )
            return database.compile_sqla_query(qry)

        if cls.limit_method == LimitMethod.FORCE_LIMIT:
            parsed_query = sql_parse.ParsedQuery(sql)
            sql = parsed_query.set_or_update_query_limit(limit, force=force)

        return sql

    @classmethod
    def apply_top_to_sql(cls, sql: str, limit: int) -> str:

        cte = None
        sql_remainder = None
        sql = sql.strip(\"" \\t\\n;\"")
        sql_statement = sqlparse.format(sql, strip_comments=True)
        query_limit: int | None = sql_parse.extract_top_from_query(
            sql_statement, cls.top_keywords
        )
        if not limit:
            final_limit = query_limit
        elif int(query_limit or 0) < limit and query_limit is not None:
            final_limit = query_limit
        else:
            final_limit = limit
        if not cls.allows_cte_in_subquery:
            cte, sql_remainder = sql_parse.get_cte_remainder_query(sql_statement)
        if cte:
            str_statement = str(sql_remainder)
            cte = cte + \""\\n\""
        else:
            cte = \""\""
            str_statement = str(sql)
        str_statement = str_statement.replace(\""\\n\"", \"" \"").replace(\""\\r\"", \""\"")

        tokens = str_statement.rstrip().split(\"" \"")
        tokens = [token for token in tokens if token]
        if cls.top_not_in_sql(str_statement):
            selects = [
                i
                for i, word in enumerate(tokens)
                if word.upper() in cls.select_keywords
            ]
            first_select = selects[0]
            if tokens[first_select + 1].upper() == \""DISTINCT\"":
                first_select += 1

            tokens.insert(first_select + 1, \""TOP\"")
            tokens.insert(first_select + 2, str(final_limit))

        next_is_limit_token = False
        new_tokens = []

        for token in tokens:
            if token in cls.top_keywords:
                next_is_limit_token = True
            elif next_is_limit_token:
                if token.isdigit():
                    token = str(final_limit)
                    next_is_limit_token = False
            new_tokens.append(token)
        sql = \"" \"".join(new_tokens)
        return cte + sql

    @classmethod
    def top_not_in_sql(cls, sql: str) -> bool:
        for top_word in cls.top_keywords:
            if top_word.upper() in sql.upper():
                return False
        return True

    @classmethod
    def get_limit_from_sql(cls, sql: str) -> int | None:
        parsed_query = sql_parse.ParsedQuery(sql)
        return parsed_query.limit

    @classmethod
    def set_or_update_query_limit(cls, sql: str, limit: int) -> str:
        parsed_query = sql_parse.ParsedQuery(sql)
        return parsed_query.set_or_update_query_limit(limit)

    @classmethod
    def get_cte_query(cls, sql: str) -> str | None:
        if not cls.allows_cte_in_subquery:
            stmt = sqlparse.parse(sql)[0]

            idx, token = stmt.token_next(-1, skip_ws=True, skip_cm=True)
            if not (token and token.ttype == CTE):
                return None
            idx, token = stmt.token_next(idx)
            idx = stmt.token_index(token) + 1

            remainder = \""\"".join(str(token) for token in stmt.tokens[idx:]).strip()
            return f\""WITH {token.value},\\n{cls.cte_alias} AS (\\n{remainder}\\n)\""

        return None

    @classmethod
    def df_to_sql(
        cls,
        database: Database,
        table: Table,
        df: pd.DataFrame,
        to_sql_kwargs: dict[str, Any],
    ) -> None:

        to_sql_kwargs[\""name\""] = table.table

        if table.schema:
            to_sql_kwargs[\""schema\""] = table.schema

        with cls.get_engine(database) as engine:
            if engine.dialect.supports_multivalues_insert:
                to_sql_kwargs[\""method\""] = \""multi\""

            df.to_sql(con=engine, **to_sql_kwargs)

    @classmethod
        cls, target_type: str, dttm: datetime, db_extra: dict[str, Any] | None = None
    ) -> str | None:
        return None

    @classmethod
    def handle_cursor(cls, cursor: Any, query: Query, session: Session) -> None:

    @classmethod
    def execute_with_cursor(
        cls, cursor: Any, sql: str, query: Query, session: Session
    ) -> None:
        logger.debug(\""Query %d: Running query: %s\"", query.id, sql)
        cls.execute(cursor, sql, async_=True)
        logger.debug(\""Query %d: Handling cursor\"", query.id)
        cls.handle_cursor(cursor, query, session)

    @classmethod
    def extract_error_message(cls, ex: Exception) -> str:
        return f\""{cls.engine} error: {cls._extract_error_message(ex)}\""

    @classmethod
    def _extract_error_message(cls, ex: Exception) -> str:
        return utils.error_msg_from_exception(ex)

    @classmethod
    def extract_errors(
        cls, ex: Exception, context: dict[str, Any] | None = None
    ) -> list[SupersetError]:
        raw_message = cls._extract_error_message(ex)

        context = context or {}
        for regex, (message, error_type, extra) in cls.custom_errors.items():
            if match := regex.search(raw_message):
                params = {**context, **match.groupdict()}
                extra[\""engine_name\""] = cls.engine_name
                return [
                    SupersetError(
                        error_type=error_type,
                        message=message % params,
                        level=ErrorLevel.ERROR,
                        extra=extra,
                    )
                ]

        return [
            SupersetError(
                error_type=SupersetErrorType.GENERIC_DB_ENGINE_ERROR,
                message=cls._extract_error_message(ex),
                level=ErrorLevel.ERROR,
                extra={\""engine_name\"": cls.engine_name},
            )
        ]

    @classmethod
        cls,
        uri: URL,
        connect_args: dict[str, Any],
        catalog: str | None = None,
        schema: str | None = None,
    ) -> tuple[URL, dict[str, Any]]:
        return uri, {
            **connect_args,
            **cls.enforce_uri_query_params.get(uri.get_driver_name(), {}),
        }

    @classmethod
    def get_prequeries(
        cls,
    ) -> list[str]:
        return []

    @classmethod
    def patch(cls) -> None:

    @classmethod
        cls,
        database: Database,
        inspector: Inspector,
    ) -> list[str]:
        return []

    @classmethod
    def get_schema_names(cls, inspector: Inspector) -> list[str]:
        return sorted(inspector.get_schema_names())

    @classmethod
        cls,
        database: Database,
        inspector: Inspector,
        schema: str | None,
    ) -> set[str]:

        try:
            tables = set(inspector.get_table_names(schema))
        except Exception as ex:
            raise cls.get_dbapi_mapped_exception(ex) from ex

        if schema and cls.try_remove_schema_from_table_name:
            tables = {re.sub(f\""^{schema}\\\\.\"", \""\"", table) for table in tables}
        return tables

    @classmethod
        cls,
        database: Database,
        inspector: Inspector,
        schema: str | None,
    ) -> set[str]:

        try:
            views = set(inspector.get_view_names(schema))
        except Exception as ex:
            raise cls.get_dbapi_mapped_exception(ex) from ex

        if schema and cls.try_remove_schema_from_table_name:
            views = {re.sub(f\""^{schema}\\\\.\"", \""\"", view) for view in views}
        return views

    @classmethod
    def get_indexes(
        cls,
        inspector: Inspector,
        table_name: str,
        schema: str | None,
    ) -> list[dict[str, Any]]:

        return inspector.get_indexes(table_name, schema)

    @classmethod
    def get_table_comment(
        cls, inspector: Inspector, table_name: str, schema: str | None
    ) -> str | None:
        comment = None
        try:
            comment = inspector.get_table_comment(table_name, schema)
            comment = comment.get(\""text\"") if isinstance(comment, dict) else None
        except NotImplementedError:
            pass
            logger.error(\""Unexpected error while fetching table comment\"", exc_info=True)
            logger.exception(ex)
        return comment

    @classmethod
    def get_columns(
        cls, inspector: Inspector, table_name: str, schema: str | None
    ) -> list[ResultSetColumnType]:
        return convert_inspector_columns(
            cast(list[SQLAColumnType], inspector.get_columns(table_name, schema))
        )

    @classmethod
        cls,
        database: Database,
        inspector: Inspector,
        table_name: str,
        schema: str | None,
    ) -> list[MetricType]:
        return [
            {
                \""metric_name\"": \""count\
,""                \""verbose_name\"": \""COUNT(*)\
,""                \""metric_type\"": \""count\
,""                \""expression\"": \""COUNT(*)\
,""            }
        ]

    @classmethod
        cls,
        table_name: str,
        schema: str | None,
        database: Database,
        query: Select,
        columns: list[ResultSetColumnType] | None = None,
    ) -> Select | None:
        return None

    @classmethod
    def _get_fields(cls, cols: list[ResultSetColumnType]) -> list[Any]:
        return [column(c[\""column_name\""]) for c in cols]

    @classmethod
        cls,
        database: Database,
        table_name: str,
        engine: Engine,
        schema: str | None = None,
        limit: int = 100,
        show_cols: bool = False,
        indent: bool = True,
        latest_partition: bool = True,
        cols: list[ResultSetColumnType] | None = None,
    ) -> str:
        fields: str | list[Any] = \""*\""
        cols = cols or []
        if (show_cols or latest_partition) and not cols:
            cols = database.get_columns(table_name, schema)

        if show_cols:
            fields = cls._get_fields(cols)
        quote = engine.dialect.identifier_preparer.quote
        if schema:
            full_table_name = quote(schema) + \"".\"" + quote(table_name)
        else:
            full_table_name = quote(table_name)

        qry = select(fields).select_from(text(full_table_name))

        if limit:
            qry = qry.limit(limit)
        if latest_partition:
            partition_query = cls.where_latest_partition(
                table_name, schema, database, qry, columns=cols
            )
            if partition_query is not None:
                qry = partition_query
        sql = database.compile_sqla_query(qry)
        if indent:
            sql = sqlparse.format(sql, reindent=True)
        return sql

    @classmethod
    def estimate_statement_cost(cls, statement: str, cursor: Any) -> dict[str, Any]:
            \""Database does not support cost estimation\""
        )

    @classmethod
    def query_cost_formatter(
        cls, raw_cost: list[dict[str, Any]]
    ) -> list[dict[str, str]]:
            \""Database does not support cost estimation\""
        )

    @classmethod
    def process_statement(cls, statement: str, database: Database) -> str:
        parsed_query = ParsedQuery(statement)
        sql = parsed_query.stripped()
        sql_query_mutator = current_app.config[\""SQL_QUERY_MUTATOR\""]
        mutate_after_split = current_app.config[\""MUTATE_AFTER_SPLIT\""]
        if sql_query_mutator and not mutate_after_split:
            sql = sql_query_mutator(
                sql,
                security_manager=security_manager,
                database=database,
            )

        return sql

    @classmethod
    def estimate_query_cost(
        cls,
        database: Database,
        schema: str,
        sql: str,
        source: utils.QuerySource | None = None,
    ) -> list[dict[str, Any]]:
        extra = database.get_extra() or {}
        if not cls.get_allow_cost_estimate(extra):
                \""Database does not support cost estimation\""
            )

        parsed_query = sql_parse.ParsedQuery(sql)
        statements = parsed_query.get_statements()

        costs = []
        with database.get_raw_connection(schema=schema, source=source) as conn:
            cursor = conn.cursor()
            for statement in statements:
                processed_statement = cls.process_statement(statement, database)
                costs.append(cls.estimate_statement_cost(processed_statement, cursor))

        return costs

    @classmethod
    def get_url_for_impersonation(
        cls, url: URL, impersonate_user: bool, username: str | None
    ) -> URL:
        if impersonate_user and username is not None:
            url = url.set(username=username)

        return url

    @classmethod
    def update_impersonation_config(
        cls,
        connect_args: dict[str, Any],
        uri: str,
        username: str | None,
    ) -> None:

    @classmethod
        cls,
        cursor: Any,
        query: str,
        **kwargs: Any,
    ) -> None:
        if not cls.allows_sql_comments:
            query = sql_parse.strip_comments_from_sql(query)

        if cls.arraysize:
            cursor.arraysize = cls.arraysize
        try:
            cursor.execute(query)
        except Exception as ex:
            raise cls.get_dbapi_mapped_exception(ex) from ex

    @classmethod
    def make_label_compatible(cls, label: str) -> str | quoted_name:
        label_mutated = cls._mutate_label(label)
        if (
            cls.max_column_name_length
            and len(label_mutated) > cls.max_column_name_length
        ):
            label_mutated = cls._truncate_label(label)
        if cls.force_column_alias_quotes:
            label_mutated = quoted_name(label_mutated, True)
        return label_mutated

    @classmethod
    def get_column_types(
        cls,
        column_type: str | None,
    ) -> tuple[TypeEngine, GenericDataType] | None:
        if not column_type:
            return None

        for regex, sqla_type, generic_type in (
            cls.column_type_mappings + cls._default_column_type_mappings
        ):
            match = regex.match(column_type)
            if not match:
                continue
            if callable(sqla_type):
                return sqla_type(match), generic_type
            return sqla_type, generic_type
        return None

    @staticmethod
    def _mutate_label(label: str) -> str:
        return label

    @classmethod
    def _truncate_label(cls, label: str) -> str:
        label = md5_sha_from_str(label)
        if cls.max_column_name_length and len(label) > cls.max_column_name_length:
            label = label[: cls.max_column_name_length]
        return label

    @classmethod
    def column_datatype_to_string(
        cls, sqla_column_type: TypeEngine, dialect: Dialect
    ) -> str:
        sqla_column_type = sqla_column_type.copy()
        if hasattr(sqla_column_type, \""collation\""):
            sqla_column_type.collation = None
        if hasattr(sqla_column_type, \""charset\""):
            sqla_column_type.charset = None
        return sqla_column_type.compile(dialect=dialect).upper()

    @classmethod
        cls,
        database: Database,
    ) -> list[str]:
        return []

    @staticmethod
    def pyodbc_rows_to_tuples(data: list[Any]) -> list[tuple[Any, ...]]:
        if data and type(data[0]).__name__ == \""Row\"":
            data = [tuple(row) for row in data]
        return data

    @staticmethod
        database: Database,
    ) -> None:
        return None

    @staticmethod
    def get_extra_params(database: Database) -> dict[str, Any]:
        extra: dict[str, Any] = {}
        if database.extra:
            try:
                extra = json.loads(database.extra)
            except json.JSONDecodeError as ex:
                logger.error(ex, exc_info=True)
                raise ex
        return extra

    @staticmethod
        database: Database, params: dict[str, Any]
    ) -> None:
        if not database.encrypted_extra:
            return
        try:
            encrypted_extra = json.loads(database.encrypted_extra)
            params.update(encrypted_extra)
        except json.JSONDecodeError as ex:
            logger.error(ex, exc_info=True)
            raise ex

    @classmethod
    def is_readonly_query(cls, parsed_query: ParsedQuery) -> bool:
        return (
            parsed_query.is_select()
            or parsed_query.is_explain()
            or parsed_query.is_show()
        )

    @classmethod
    def is_select_query(cls, parsed_query: ParsedQuery) -> bool:
        return parsed_query.is_select()

    @classmethod
        cls,
        native_type: str | None,
        db_extra: dict[str, Any] | None = None,
        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,
    ) -> ColumnSpec | None:
        if col_types := cls.get_column_types(native_type):
            column_type, generic_type = col_types
            is_dttm = generic_type == GenericDataType.TEMPORAL
            return ColumnSpec(
                sqla_type=column_type, generic_type=generic_type, is_dttm=is_dttm
            )
        return None

    @classmethod
    def get_sqla_column_type(
        cls,
        native_type: str | None,
        db_extra: dict[str, Any] | None = None,
        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,
    ) -> TypeEngine | None:
        column_spec = cls.get_column_spec(
            native_type=native_type,
            db_extra=db_extra,
            source=source,
        )
        return column_spec.sqla_type if column_spec else None

    @classmethod
    def prepare_cancel_query(cls, query: Query, session: Session) -> None:
        return None

    @classmethod
    def has_implicit_cancel(cls) -> bool:

        return False

    @classmethod
        cls,
        cursor: Any,
        query: Query,
    ) -> str | None:

        return None

    @classmethod
        cls,
        cursor: Any,
        query: Query,
        cancel_query_id: str,
    ) -> bool:

        return False

    @classmethod
    def parse_sql(cls, sql: str) -> list[str]:
        return [str(s).strip(\"" ;\"") for s in sqlparse.parse(sql)]

    @classmethod
    def get_impersonation_key(cls, user: User | None) -> Any:
        return user.username if user else None

    @classmethod
    def mask_encrypted_extra(cls, encrypted_extra: str | None) -> str | None:
        return encrypted_extra

    @classmethod
    def unmask_encrypted_extra(cls, old: str | None, new: str | None) -> str | None:
        return new

    @classmethod
    def get_public_information(cls) -> dict[str, Any]:
        return {
            \""supports_file_upload\"": cls.supports_file_upload,
            \""disable_ssh_tunneling\"": cls.disable_ssh_tunneling,
        }

    @classmethod
    def validate_database_uri(cls, sqlalchemy_uri: URL) -> None:
        if existing_disallowed := cls.disallow_uri_query_params.get(
            sqlalchemy_uri.get_driver_name(), set()
        ).intersection(sqlalchemy_uri.query):
            raise ValueError(f\""Forbidden query parameter(s): {existing_disallowed}\"")

    @classmethod
    def denormalize_name(cls, dialect: Dialect, name: str) -> str:
        if (
            hasattr(dialect, \""requires_name_normalize\"")
            and dialect.requires_name_normalize
        ):
            return dialect.denormalize_name(name)

        return name


class BasicParametersSchema(Schema):
    username = fields.String(
        required=True, allow_none=True, metadata={\""description\"": __(\""Username\"")}
    )
    password = fields.String(allow_none=True, metadata={\""description\"": __(\""Password\"")})
    host = fields.String(
        required=True, metadata={\""description\"": __(\""Hostname or IP address\"")}
    )
    port = fields.Integer(
        required=True,
        metadata={\""description\"": __(\""Database port\"")},
        validate=Range(min=0, max=2**16, max_inclusive=False),
    )
    database = fields.String(
        required=True, metadata={\""description\"": __(\""Database name\"")}
    )
    query = fields.Dict(
        keys=fields.Str(),
        values=fields.Raw(),
        metadata={\""description\"": __(\""Additional parameters\"")},
    )
    encryption = fields.Boolean(
        required=False,
        metadata={\""description\"": __(\""Use an encrypted connection to the database\"")},
    )
    ssh = fields.Boolean(
        required=False,
        metadata={\""description\"": __(\""Use an ssh tunnel connection to the database\"")},
    )


class BasicParametersType(TypedDict, total=False):
    username: str | None
    password: str | None
    host: str
    port: int
    database: str
    query: dict[str, Any]
    encryption: bool


class BasicPropertiesType(TypedDict):
    parameters: BasicParametersType


class BasicParametersMixin:

    parameters_schema = BasicParametersSchema()

    default_driver = \""\""

    encryption_parameters: dict[str, str] = {}

    @classmethod
        cls,
        parameters: BasicParametersType,
        encrypted_extra: dict[str, str] | None = None,
    ) -> str:
        query = parameters.get(\""query\"", {}).copy()
        if parameters.get(\""encryption\""):
            if not cls.encryption_parameters:
                    \""Unable to build a URL with encryption enabled\""
                )
            query.update(cls.encryption_parameters)

        return str(
            URL.create(
                username=parameters.get(\""username\""),
                password=parameters.get(\""password\""),
                host=parameters[\""host\""],
                port=parameters[\""port\""],
                database=parameters[\""database\""],
                query=query,
            )
        )

    @classmethod
        cls, uri: str, encrypted_extra: dict[str, Any] | None = None
    ) -> BasicParametersType:
        url = make_url_safe(uri)
        query = {
            key: value
            for (key, value) in url.query.items()
            if (key, value) not in cls.encryption_parameters.items()
        }
        encryption = all(
            item in url.query.items() for item in cls.encryption_parameters.items()
        )
        return {
            \""username\"": url.username,
            \""password\"": url.password,
            \""host\"": url.host,
            \""port\"": url.port,
            \""database\"": url.database,
            \""query\"": query,
            \""encryption\"": encryption,
        }

    @classmethod
    def validate_parameters(
        cls, properties: BasicPropertiesType
    ) -> list[SupersetError]:
        errors: list[SupersetError] = []

        required = {\""host\"", \""port\"", \""username\"", \""database\""}
        parameters = properties.get(\""parameters\"", {})
        present = {key for key in parameters if parameters.get(key, ())}

        if missing := sorted(required - present):
            errors.append(
                SupersetError(
                    message=f'One or more parameters are missing: {\"", \"".join(missing)}',
                    error_type=SupersetErrorType.CONNECTION_MISSING_PARAMETERS_ERROR,
                    level=ErrorLevel.WARNING,
                    extra={\""missing\"": missing},
                ),
            )

        host = parameters.get(\""host\"", None)
        if not host:
            return errors
        if not is_hostname_valid(host):
            errors.append(
                SupersetError(
                    message=\""The hostname provided can't be resolved.\
,""                    error_type=SupersetErrorType.CONNECTION_INVALID_HOSTNAME_ERROR,
                    level=ErrorLevel.ERROR,
                    extra={\""invalid\"": [\""host\""]},
                ),
            )
            return errors

        port = parameters.get(\""port\"", None)
        if not port:
            return errors
        try:
            port = int(port)
        except (ValueError, TypeError):
            errors.append(
                SupersetError(
                    message=\""Port must be a valid integer.\
,""                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,
                    level=ErrorLevel.ERROR,
                    extra={\""invalid\"": [\""port\""]},
                ),
            )
        if not (isinstance(port, int) and 0 <= port < 2**16):
            errors.append(
                SupersetError(
                    message=(
                        \""The port must be an integer between 0 and 65535 \""
                        \""(inclusive).\""
                    ),
                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,
                    level=ErrorLevel.ERROR,
                    extra={\""invalid\"": [\""port\""]},
                ),
            )
        elif not is_port_open(host, port):
            errors.append(
                SupersetError(
                    message=\""The port is closed.\
,""                    error_type=SupersetErrorType.CONNECTION_PORT_CLOSED_ERROR,
                    level=ErrorLevel.ERROR,
                    extra={\""invalid\"": [\""port\""]},
                ),
            )

        return errors

    @classmethod
    def parameters_json_schema(cls) -> Any:
        if not cls.parameters_schema:
            return None

        spec = APISpec(
            title=\""Database Parameters\
,""            version=\""1.0.0\
,""            openapi_version=\""3.0.2\
,""            plugins=[MarshmallowPlugin()],
        )
        spec.components.schema(cls.__name__, schema=cls.parameters_schema)
        return spec.to_dict()[\""components\""][\""schemas\""][cls.__name__]
","['getLogger', 'convert_inspector_columns', 'TimeGrain', '__', 'TimestampExpression', '__init__', 'super', '_constructor', 'compiles', 'compile_timegrain_expression', 'replace', 'process', 'MetricType', 'set', 'compile', 'String', 'n', 'UnicodeText', 'SmallInteger', 'int', 'Integer', 'BigInteger', 'Float', 'Numeric', 'TIMESTAMP', 'DateTime', 'Date', 'Time', 'Interval', 'bool', 'Boolean', 'get_allows_alias_in_select', 'supports_url', 'get_backend_name', 'get_driver_name', 'supports_backend', 'get_default_schema', 'get_inspector_with_context', 'get_default_schema_for_query', 'get_extra', 'make_url_safe', 'get_schema_from_engine_params', 'get_dbapi_exception_mapping', 'parse_error_exception', 'get_dbapi_mapped_exception', 'get', 'type', 'new_exception', 'str', 'get_text_clause', 'text', 'get_engine', 'get_sqla_engine_with_context', 'get_timestamp_expr', 'getattr', 'get_time_grain_expressions', 'NotImplementedError', 'epoch_to_dttm', 'epoch_ms_to_dttm', 'get_time_grains', 'copy', 'update', 'items', 'append', '_', 'tuple', '_sort_time_grains', 'match', 'group', 'len', 'sort_day_week', 'sort_interval', 'index', 'float', 'pop', 'dict', 'sorted', 'fetch_data', 'fetchmany', 'fetchall', 'get_sqla_column_type', 'get_datatype', 'enumerate', 'list', 'func', 'expand_data', 'alter_new_orm_column', 'isinstance', 'upper', 'deprecated', 'normalize_indexes', 'apply_limit_to_sql', 'strip', 'select', 'select_from', 'TextAsFrom', 'alias', 'limit', 'compile_sqla_query', 'ParsedQuery', 'set_or_update_query_limit', 'apply_top_to_sql', 'format', 'extract_top_from_query', 'get_cte_remainder_query', 'rstrip', 'split', 'top_not_in_sql', 'insert', 'isdigit', 'join', 'get_limit_from_sql', 'get_cte_query', 'parse', 'token_next', 'token_index', 'df_to_sql', 'to_sql', 'handle_cursor', 'execute_with_cursor', 'debug', 'execute', 'extract_error_message', '_extract_error_message', 'error_msg_from_exception', 'extract_errors', 'search', 'groupdict', 'SupersetError', 'get_prequeries', 'patch', 'get_schema_names', 'get_table_names', 'sub', 'get_view_names', 'get_indexes', 'get_table_comment', 'error', 'exception', 'get_columns', 'cast', 'COUNT', '_get_fields', 'column', 'quote', 'where_latest_partition', 'estimate_statement_cost', 'query_cost_formatter', 'process_statement', 'stripped', 'sql_query_mutator', 'estimate_query_cost', 'get_allow_cost_estimate', 'get_statements', 'get_raw_connection', 'cursor', 'get_url_for_impersonation', 'update_impersonation_config', 'strip_comments_from_sql', 'make_label_compatible', '_mutate_label', '_truncate_label', 'quoted_name', 'get_column_types', 'callable', 'sqla_type', 'md5_sha_from_str', 'column_datatype_to_string', 'hasattr', 'pyodbc_rows_to_tuples', 'get_extra_params', 'loads', 'is_readonly_query', 'is_select', 'is_explain', 'is_show', 'is_select_query', 'ColumnSpec', 'get_column_spec', 'prepare_cancel_query', 'has_implicit_cancel', 'parse_sql', 'get_impersonation_key', 'mask_encrypted_extra', 'unmask_encrypted_extra', 'get_public_information', 'validate_database_uri', 'intersection', 'ValueError', 'parameter', 'denormalize_name', 'BasicParametersSchema', 'Range', 'Dict', 'Str', 'Raw', 'BasicParametersType', 'BasicPropertiesType', 'create', 'all', 'validate_parameters', 'is_hostname_valid', 'is_port_open', 'parameters_json_schema', 'APISpec', 'MarshmallowPlugin', 'schema', 'to_dict']"
